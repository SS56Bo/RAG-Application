{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e745b4",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd75b8a",
   "metadata": {},
   "source": [
    "#### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa25c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eaa159",
   "metadata": {},
   "source": [
    "#### Formatting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1741ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_format(text: str)->str :\n",
    "    cleaner_text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    return cleaner_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c48ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_source(path: str)->list[dict]:\n",
    "    doc = fitz.open(path)\n",
    "    pages_text = []\n",
    "\n",
    "    for pageno, pagecontent in tqdm(enumerate(doc)):\n",
    "        text = pagecontent.get_text()\n",
    "        text = text_format(text=text)\n",
    "        pages_text.append({\"Page No.\": pageno, \"page_char_count\": len(text), \"page_word_count\": len(text.split(\" \")), \"page_sentence_count (Not accurate)\": len(text.split(\".\")), \"page_token_count\": len(text)/4, \"text\": text})\n",
    "        \n",
    "    return pages_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1ecf6",
   "metadata": {},
   "source": [
    "#### Using our custom function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78bcf739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00, 262.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Page No.': 7,\n",
       "  'page_char_count': 608,\n",
       "  'page_word_count': 108,\n",
       "  'page_sentence_count (Not accurate)': 8,\n",
       "  'page_token_count': 152.0,\n",
       "  'text': '(the Box) is a pointer to that value on the heap. When the Box is eventually dropped, that memory is freed. If you forget to deallocate heap memory, it will stick around forever, and your application will eventually eat up all the memory on your machine. This is called leaking memory and is usually something you want to avoid. However, there are some cases where you explicitly want to leak memory. For example, say you have a read-only configuration that the entire program should be able to access. You can allocate that on the heap and explicitly leak it with Box::leak to get a ‘static reference to it.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "text_info = get_text_from_source(path=\"test.pdf\")\n",
    "random.sample(text_info, k=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c6f78",
   "metadata": {},
   "source": [
    "#### Converting to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71177ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page No.</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count (Not accurate)</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.50</td>\n",
       "      <td>3045.88</td>\n",
       "      <td>493.50</td>\n",
       "      <td>26.75</td>\n",
       "      <td>761.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.45</td>\n",
       "      <td>1152.18</td>\n",
       "      <td>207.38</td>\n",
       "      <td>8.92</td>\n",
       "      <td>288.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>608.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>152.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.75</td>\n",
       "      <td>2737.75</td>\n",
       "      <td>408.75</td>\n",
       "      <td>25.25</td>\n",
       "      <td>684.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.50</td>\n",
       "      <td>3102.00</td>\n",
       "      <td>455.50</td>\n",
       "      <td>26.50</td>\n",
       "      <td>775.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.25</td>\n",
       "      <td>4014.25</td>\n",
       "      <td>695.50</td>\n",
       "      <td>34.00</td>\n",
       "      <td>1003.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.00</td>\n",
       "      <td>4063.00</td>\n",
       "      <td>716.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>1015.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Page No.  page_char_count  page_word_count  \\\n",
       "count      8.00             8.00             8.00   \n",
       "mean       3.50          3045.88           493.50   \n",
       "std        2.45          1152.18           207.38   \n",
       "min        0.00           608.00           108.00   \n",
       "25%        1.75          2737.75           408.75   \n",
       "50%        3.50          3102.00           455.50   \n",
       "75%        5.25          4014.25           695.50   \n",
       "max        7.00          4063.00           716.00   \n",
       "\n",
       "       page_sentence_count (Not accurate)  page_token_count  \n",
       "count                                8.00              8.00  \n",
       "mean                                26.75            761.47  \n",
       "std                                  8.92            288.04  \n",
       "min                                  8.00            152.00  \n",
       "25%                                 25.25            684.44  \n",
       "50%                                 26.50            775.50  \n",
       "75%                                 34.00           1003.56  \n",
       "max                                 36.00           1015.75  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(text_info)\n",
    "data.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b8468",
   "metadata": {},
   "source": [
    "#### Splitting text. Conversion of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae6336a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x21468f0c550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instance of English\n",
    "obj = English()\n",
    "\n",
    "#adding pipeling\n",
    "obj.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b744a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 190.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for items in tqdm(text_info):\n",
    "    items[\"sentences\"] = list(obj(items[\"text\"]).sents)\n",
    "    items[\"sentences\"]= [str(sentence) for sentence in items[\"sentences\"]]\n",
    "    items[\"senetences_count_spacy\"] = len(items[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14471b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Page No.': 0,\n",
       "  'page_char_count': 3397,\n",
       "  'page_word_count': 474,\n",
       "  'page_sentence_count (Not accurate)': 27,\n",
       "  'page_token_count': 849.25,\n",
       "  'text': 'Introduction With the increasing demand for high-performance computing in domains such as deep learning, scientific simulations, and real-time rendering, Graphics Processing Units (GPUs) have become essential for accelerating large-scale parallel workloads. Unlike Central Processing Units (CPUs), GPUs are designed with a large number of lightweight threads and high memory bandwidth, making them highly suitable for data-parallel computation. To exploit this capability, various GPU programming frameworks have emerged, targeting different hardware architectures and levels of portability. This project presents a comparative study of four prominent GPU programming models: CUDA, HIP on NVIDIA (CUDA-supported), HIP on AMD (ROCm-supported), and OpenCL. The report begins with an overview of GPU and CPU architectures, the fundamentals of parallel programming, and the conceptual mapping of memory and execution models to the underlying hardware. It further explores architectural differences between NVIDIA and AMD GPUs and discusses how each framework adapts to these variations. Performance evaluation is carried out using vector addition and matrix multiplication programs. Benchmarks were conducted on an NVIDIA T1000 8GB GPU and an AMD GPU (MI50/MI60) configured with the ROCm stack. Execution times were recorded and plotted to compare behavior across platforms. The study demonstrates the tradeoffs between performance and portability, offering practical insights for heterogeneous computing. Future work will extend this analysis to include OpenCL implementations and additional workloads for broader framework comparison. Parallel Computing Parallel programming is a way of solving problems by dividing a task into smaller subtasks and executing them simultaneously across multiple processors or cores. This significantly reduces execution time and is especially useful for large, compute-intensive problems. Instead of doing one thing at a time (sequential execution), parallel programming lets us do many things at once, taking advantage of modern multi-core processors and GPUs. Types of Parallelism There are two main types of parallelism: • Data Parallelism: The same operation is performed on different pieces of data at the same time. Example: Adding two large arrays element-wise. • Task Parallelism: Different tasks (functions or operations) are performed in parallel, possibly on the same or different data. Example: While one thread reads data from a file, another processes a different part of it. Applications of Parallel Programming Parallel programming is used in various real-world domains: • Artificial Intelligence and Machine Learning • Scientific Simulations (e.g., weather forecasting) • Computer Graphics and Gaming • Image and Signal Processing • Cryptography and Blockchain • Financial Modeling CPU vs GPU CPUs are designed for general-purpose tasks. They have a few powerful cores optimized for sequential execution and complex decision-making logic. This makes them suitable for running operating systems and everyday applications. GPUs, on the other hand, are made for high-speed parallel processing. They contain thousands of smaller, simpler cores that can execute many threads simultaneously. This architecture is ideal for graphics rendering, scientific computing, and machine learning tasks that involve repetitive numerical operations on large datasets.',\n",
       "  'sentences': ['Introduction With the increasing demand for high-performance computing in domains such as deep learning, scientific simulations, and real-time rendering, Graphics Processing Units (GPUs) have become essential for accelerating large-scale parallel workloads.',\n",
       "   'Unlike Central Processing Units (CPUs), GPUs are designed with a large number of lightweight threads and high memory bandwidth, making them highly suitable for data-parallel computation.',\n",
       "   'To exploit this capability, various GPU programming frameworks have emerged, targeting different hardware architectures and levels of portability.',\n",
       "   'This project presents a comparative study of four prominent GPU programming models: CUDA, HIP on NVIDIA (CUDA-supported), HIP on AMD (ROCm-supported), and OpenCL.',\n",
       "   'The report begins with an overview of GPU and CPU architectures, the fundamentals of parallel programming, and the conceptual mapping of memory and execution models to the underlying hardware.',\n",
       "   'It further explores architectural differences between NVIDIA and AMD GPUs and discusses how each framework adapts to these variations.',\n",
       "   'Performance evaluation is carried out using vector addition and matrix multiplication programs.',\n",
       "   'Benchmarks were conducted on an NVIDIA T1000 8GB GPU and an AMD GPU (MI50/MI60) configured with the ROCm stack.',\n",
       "   'Execution times were recorded and plotted to compare behavior across platforms.',\n",
       "   'The study demonstrates the tradeoffs between performance and portability, offering practical insights for heterogeneous computing.',\n",
       "   'Future work will extend this analysis to include OpenCL implementations and additional workloads for broader framework comparison.',\n",
       "   'Parallel Computing Parallel programming is a way of solving problems by dividing a task into smaller subtasks and executing them simultaneously across multiple processors or cores.',\n",
       "   'This significantly reduces execution time and is especially useful for large, compute-intensive problems.',\n",
       "   'Instead of doing one thing at a time (sequential execution), parallel programming lets us do many things at once, taking advantage of modern multi-core processors and GPUs.',\n",
       "   'Types of Parallelism There are two main types of parallelism: • Data Parallelism: The same operation is performed on different pieces of data at the same time.',\n",
       "   'Example: Adding two large arrays element-wise. •',\n",
       "   'Task Parallelism: Different tasks (functions or operations) are performed in parallel, possibly on the same or different data.',\n",
       "   'Example: While one thread reads data from a file, another processes a different part of it.',\n",
       "   'Applications of Parallel Programming Parallel programming is used in various real-world domains: • Artificial Intelligence and Machine Learning • Scientific Simulations (e.g., weather forecasting) • Computer Graphics and Gaming • Image and Signal Processing • Cryptography and Blockchain • Financial Modeling CPU vs GPU CPUs are designed for general-purpose tasks.',\n",
       "   'They have a few powerful cores optimized for sequential execution and complex decision-making logic.',\n",
       "   'This makes them suitable for running operating systems and everyday applications.',\n",
       "   'GPUs, on the other hand, are made for high-speed parallel processing.',\n",
       "   'They contain thousands of smaller, simpler cores that can execute many threads simultaneously.',\n",
       "   'This architecture is ideal for graphics rendering, scientific computing, and machine learning tasks that involve repetitive numerical operations on large datasets.'],\n",
       "  'senetences_count_spacy': 24}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(text_info, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b87b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page No.</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count (Not accurate)</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>senetences_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.50</td>\n",
       "      <td>3045.88</td>\n",
       "      <td>493.50</td>\n",
       "      <td>26.75</td>\n",
       "      <td>761.47</td>\n",
       "      <td>24.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.45</td>\n",
       "      <td>1152.18</td>\n",
       "      <td>207.38</td>\n",
       "      <td>8.92</td>\n",
       "      <td>288.04</td>\n",
       "      <td>7.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>608.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>152.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.75</td>\n",
       "      <td>2737.75</td>\n",
       "      <td>408.75</td>\n",
       "      <td>25.25</td>\n",
       "      <td>684.44</td>\n",
       "      <td>23.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.50</td>\n",
       "      <td>3102.00</td>\n",
       "      <td>455.50</td>\n",
       "      <td>26.50</td>\n",
       "      <td>775.50</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.25</td>\n",
       "      <td>4014.25</td>\n",
       "      <td>695.50</td>\n",
       "      <td>34.00</td>\n",
       "      <td>1003.56</td>\n",
       "      <td>28.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.00</td>\n",
       "      <td>4063.00</td>\n",
       "      <td>716.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>1015.75</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Page No.  page_char_count  page_word_count  \\\n",
       "count      8.00             8.00             8.00   \n",
       "mean       3.50          3045.88           493.50   \n",
       "std        2.45          1152.18           207.38   \n",
       "min        0.00           608.00           108.00   \n",
       "25%        1.75          2737.75           408.75   \n",
       "50%        3.50          3102.00           455.50   \n",
       "75%        5.25          4014.25           695.50   \n",
       "max        7.00          4063.00           716.00   \n",
       "\n",
       "       page_sentence_count (Not accurate)  page_token_count  \\\n",
       "count                                8.00              8.00   \n",
       "mean                                26.75            761.47   \n",
       "std                                  8.92            288.04   \n",
       "min                                  8.00            152.00   \n",
       "25%                                 25.25            684.44   \n",
       "50%                                 26.50            775.50   \n",
       "75%                                 34.00           1003.56   \n",
       "max                                 36.00           1015.75   \n",
       "\n",
       "       senetences_count_spacy  \n",
       "count                    8.00  \n",
       "mean                    24.25  \n",
       "std                      7.63  \n",
       "min                      7.00  \n",
       "25%                     23.75  \n",
       "50%                     25.00  \n",
       "75%                     28.50  \n",
       "max                     32.00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(text_info)\n",
    "data.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0af18",
   "metadata": {},
   "source": [
    "#### Chunking sentences into group of 10 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "546875e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10\n",
    "\n",
    "\n",
    "def create_chunk(big_list: list[str], split_size: int=chunk_size)->list[list[str]]:\n",
    "    return [big_list[i:i+split_size] for i in range(0, len(big_list), split_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d52f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 45160.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#Chunk size\n",
    "for items in tqdm(text_info):\n",
    "    items[\"text_chunks\"] = create_chunk(big_list=items[\"sentences\"], split_size=chunk_size)\n",
    "    items[\"chunk_size\"] = len(items[\"text_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "621fa34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Page No.': 0,\n",
       "  'page_char_count': 3397,\n",
       "  'page_word_count': 474,\n",
       "  'page_sentence_count (Not accurate)': 27,\n",
       "  'page_token_count': 849.25,\n",
       "  'text': 'Introduction With the increasing demand for high-performance computing in domains such as deep learning, scientific simulations, and real-time rendering, Graphics Processing Units (GPUs) have become essential for accelerating large-scale parallel workloads. Unlike Central Processing Units (CPUs), GPUs are designed with a large number of lightweight threads and high memory bandwidth, making them highly suitable for data-parallel computation. To exploit this capability, various GPU programming frameworks have emerged, targeting different hardware architectures and levels of portability. This project presents a comparative study of four prominent GPU programming models: CUDA, HIP on NVIDIA (CUDA-supported), HIP on AMD (ROCm-supported), and OpenCL. The report begins with an overview of GPU and CPU architectures, the fundamentals of parallel programming, and the conceptual mapping of memory and execution models to the underlying hardware. It further explores architectural differences between NVIDIA and AMD GPUs and discusses how each framework adapts to these variations. Performance evaluation is carried out using vector addition and matrix multiplication programs. Benchmarks were conducted on an NVIDIA T1000 8GB GPU and an AMD GPU (MI50/MI60) configured with the ROCm stack. Execution times were recorded and plotted to compare behavior across platforms. The study demonstrates the tradeoffs between performance and portability, offering practical insights for heterogeneous computing. Future work will extend this analysis to include OpenCL implementations and additional workloads for broader framework comparison. Parallel Computing Parallel programming is a way of solving problems by dividing a task into smaller subtasks and executing them simultaneously across multiple processors or cores. This significantly reduces execution time and is especially useful for large, compute-intensive problems. Instead of doing one thing at a time (sequential execution), parallel programming lets us do many things at once, taking advantage of modern multi-core processors and GPUs. Types of Parallelism There are two main types of parallelism: • Data Parallelism: The same operation is performed on different pieces of data at the same time. Example: Adding two large arrays element-wise. • Task Parallelism: Different tasks (functions or operations) are performed in parallel, possibly on the same or different data. Example: While one thread reads data from a file, another processes a different part of it. Applications of Parallel Programming Parallel programming is used in various real-world domains: • Artificial Intelligence and Machine Learning • Scientific Simulations (e.g., weather forecasting) • Computer Graphics and Gaming • Image and Signal Processing • Cryptography and Blockchain • Financial Modeling CPU vs GPU CPUs are designed for general-purpose tasks. They have a few powerful cores optimized for sequential execution and complex decision-making logic. This makes them suitable for running operating systems and everyday applications. GPUs, on the other hand, are made for high-speed parallel processing. They contain thousands of smaller, simpler cores that can execute many threads simultaneously. This architecture is ideal for graphics rendering, scientific computing, and machine learning tasks that involve repetitive numerical operations on large datasets.',\n",
       "  'sentences': ['Introduction With the increasing demand for high-performance computing in domains such as deep learning, scientific simulations, and real-time rendering, Graphics Processing Units (GPUs) have become essential for accelerating large-scale parallel workloads.',\n",
       "   'Unlike Central Processing Units (CPUs), GPUs are designed with a large number of lightweight threads and high memory bandwidth, making them highly suitable for data-parallel computation.',\n",
       "   'To exploit this capability, various GPU programming frameworks have emerged, targeting different hardware architectures and levels of portability.',\n",
       "   'This project presents a comparative study of four prominent GPU programming models: CUDA, HIP on NVIDIA (CUDA-supported), HIP on AMD (ROCm-supported), and OpenCL.',\n",
       "   'The report begins with an overview of GPU and CPU architectures, the fundamentals of parallel programming, and the conceptual mapping of memory and execution models to the underlying hardware.',\n",
       "   'It further explores architectural differences between NVIDIA and AMD GPUs and discusses how each framework adapts to these variations.',\n",
       "   'Performance evaluation is carried out using vector addition and matrix multiplication programs.',\n",
       "   'Benchmarks were conducted on an NVIDIA T1000 8GB GPU and an AMD GPU (MI50/MI60) configured with the ROCm stack.',\n",
       "   'Execution times were recorded and plotted to compare behavior across platforms.',\n",
       "   'The study demonstrates the tradeoffs between performance and portability, offering practical insights for heterogeneous computing.',\n",
       "   'Future work will extend this analysis to include OpenCL implementations and additional workloads for broader framework comparison.',\n",
       "   'Parallel Computing Parallel programming is a way of solving problems by dividing a task into smaller subtasks and executing them simultaneously across multiple processors or cores.',\n",
       "   'This significantly reduces execution time and is especially useful for large, compute-intensive problems.',\n",
       "   'Instead of doing one thing at a time (sequential execution), parallel programming lets us do many things at once, taking advantage of modern multi-core processors and GPUs.',\n",
       "   'Types of Parallelism There are two main types of parallelism: • Data Parallelism: The same operation is performed on different pieces of data at the same time.',\n",
       "   'Example: Adding two large arrays element-wise. •',\n",
       "   'Task Parallelism: Different tasks (functions or operations) are performed in parallel, possibly on the same or different data.',\n",
       "   'Example: While one thread reads data from a file, another processes a different part of it.',\n",
       "   'Applications of Parallel Programming Parallel programming is used in various real-world domains: • Artificial Intelligence and Machine Learning • Scientific Simulations (e.g., weather forecasting) • Computer Graphics and Gaming • Image and Signal Processing • Cryptography and Blockchain • Financial Modeling CPU vs GPU CPUs are designed for general-purpose tasks.',\n",
       "   'They have a few powerful cores optimized for sequential execution and complex decision-making logic.',\n",
       "   'This makes them suitable for running operating systems and everyday applications.',\n",
       "   'GPUs, on the other hand, are made for high-speed parallel processing.',\n",
       "   'They contain thousands of smaller, simpler cores that can execute many threads simultaneously.',\n",
       "   'This architecture is ideal for graphics rendering, scientific computing, and machine learning tasks that involve repetitive numerical operations on large datasets.'],\n",
       "  'senetences_count_spacy': 24,\n",
       "  'text_chunks': [['Introduction With the increasing demand for high-performance computing in domains such as deep learning, scientific simulations, and real-time rendering, Graphics Processing Units (GPUs) have become essential for accelerating large-scale parallel workloads.',\n",
       "    'Unlike Central Processing Units (CPUs), GPUs are designed with a large number of lightweight threads and high memory bandwidth, making them highly suitable for data-parallel computation.',\n",
       "    'To exploit this capability, various GPU programming frameworks have emerged, targeting different hardware architectures and levels of portability.',\n",
       "    'This project presents a comparative study of four prominent GPU programming models: CUDA, HIP on NVIDIA (CUDA-supported), HIP on AMD (ROCm-supported), and OpenCL.',\n",
       "    'The report begins with an overview of GPU and CPU architectures, the fundamentals of parallel programming, and the conceptual mapping of memory and execution models to the underlying hardware.',\n",
       "    'It further explores architectural differences between NVIDIA and AMD GPUs and discusses how each framework adapts to these variations.',\n",
       "    'Performance evaluation is carried out using vector addition and matrix multiplication programs.',\n",
       "    'Benchmarks were conducted on an NVIDIA T1000 8GB GPU and an AMD GPU (MI50/MI60) configured with the ROCm stack.',\n",
       "    'Execution times were recorded and plotted to compare behavior across platforms.',\n",
       "    'The study demonstrates the tradeoffs between performance and portability, offering practical insights for heterogeneous computing.'],\n",
       "   ['Future work will extend this analysis to include OpenCL implementations and additional workloads for broader framework comparison.',\n",
       "    'Parallel Computing Parallel programming is a way of solving problems by dividing a task into smaller subtasks and executing them simultaneously across multiple processors or cores.',\n",
       "    'This significantly reduces execution time and is especially useful for large, compute-intensive problems.',\n",
       "    'Instead of doing one thing at a time (sequential execution), parallel programming lets us do many things at once, taking advantage of modern multi-core processors and GPUs.',\n",
       "    'Types of Parallelism There are two main types of parallelism: • Data Parallelism: The same operation is performed on different pieces of data at the same time.',\n",
       "    'Example: Adding two large arrays element-wise. •',\n",
       "    'Task Parallelism: Different tasks (functions or operations) are performed in parallel, possibly on the same or different data.',\n",
       "    'Example: While one thread reads data from a file, another processes a different part of it.',\n",
       "    'Applications of Parallel Programming Parallel programming is used in various real-world domains: • Artificial Intelligence and Machine Learning • Scientific Simulations (e.g., weather forecasting) • Computer Graphics and Gaming • Image and Signal Processing • Cryptography and Blockchain • Financial Modeling CPU vs GPU CPUs are designed for general-purpose tasks.',\n",
       "    'They have a few powerful cores optimized for sequential execution and complex decision-making logic.'],\n",
       "   ['This makes them suitable for running operating systems and everyday applications.',\n",
       "    'GPUs, on the other hand, are made for high-speed parallel processing.',\n",
       "    'They contain thousands of smaller, simpler cores that can execute many threads simultaneously.',\n",
       "    'This architecture is ideal for graphics rendering, scientific computing, and machine learning tasks that involve repetitive numerical operations on large datasets.']],\n",
       "  'chunk_size': 3},\n",
       " {'Page No.': 3,\n",
       "  'page_char_count': 2807,\n",
       "  'page_word_count': 437,\n",
       "  'page_sentence_count (Not accurate)': 23,\n",
       "  'page_token_count': 701.75,\n",
       "  'text': '‣ SIMD Engines execute parallel tasks by applying the same instruction to multiple data elements simultaneously, similar to how CUDA cores execute parallel threads in NVIDIA GPUs. • ROCm Stack Overview: ‣ The ROCm (Radeon Open Compute) stack is AMD’s software platform for GPU computing, including libraries, tools, and compilers for GPU programming. It supports hardware (CUs and SIMD engines) and parallel programming models such as HIP. ‣ ROCm offers tools for debugging, profiling, and managing AMD GPUs, optimized for high- performance computing (HPC), AI, and machine learning workloads. • HIP Support: ‣ HIP (Heterogeneous Interface for Portability) enables the development of portable applications that can run on both AMD and NVIDIA GPUs. HIP allows developers to write code that can be compiled for CUDA-enabled NVIDIA GPUs or ROCm-enabled AMD GPUs. GPU Programming Models Overview of GPU Programming Models: In GPU computing, parallelism is exploited to perform computations more efficiently. GPUs are designed to execute thousands of threads simultaneously, making them ideal for data-parallel tasks. These programming models are designed to work on different hardware, like NVIDIA, AMD, or even different vendors (OpenCL), and they organize computations into small tasks that can run concurrently. CUDA Programming Model (NVIDIA) • Thread Hierarchy: -The CUDA model organizes parallelism into a grid of blocks, where each block contains multiple threads. ‣ Threads within a block can synchronize with each other and communicate through shared memory. • Memory Model: ‣ Shared Memory: Fast, on-chip memory shared among threads within a block. ‣ Global Memory: Large memory that is accessible by all threads but has higher latency. ‣ Constant & Texture Memory: Specialized memory for read-only data that can be accessed by all threads. ‣ Registers: Fast, temporary storage for each individual thread. • Synchronization: ‣ CUDA can synchronize threads within a block. ‣ Synchronization across blocks is not supported in CUDA directly. • Execution Model: ‣ CUDA kernels are executed across the grid of blocks, where each thread in a block executes independently. Operating System Abstraction: The Process In this note, we discuss one of the most fundamental abstractions that the OS provides to users: the process. The definition of a process, informally, is quite simple: it is a running program. The program itself is a lifeless thing: it just sits there on the disk, a bunch of instructions (and maybe some static data), waiting to spring into action. It is the operating system that takes these bytes and gets them running, transforming the program into something useful. It turns out that one often wants to run more than one program at once; for example, consider your desktop or laptop where',\n",
       "  'sentences': ['‣ SIMD Engines execute parallel tasks by applying the same instruction to multiple data elements simultaneously, similar to how CUDA cores execute parallel threads in NVIDIA GPUs. •',\n",
       "   'ROCm Stack Overview: ‣ The ROCm (Radeon Open Compute) stack is AMD’s software platform for GPU computing, including libraries, tools, and compilers for GPU programming.',\n",
       "   'It supports hardware (CUs and SIMD engines) and parallel programming models such as HIP. ‣',\n",
       "   'ROCm offers tools for debugging, profiling, and managing AMD GPUs, optimized for high- performance computing (HPC), AI, and machine learning workloads. •',\n",
       "   'HIP Support: ‣ HIP (Heterogeneous Interface for Portability) enables the development of portable applications that can run on both AMD and NVIDIA GPUs.',\n",
       "   'HIP allows developers to write code that can be compiled for CUDA-enabled NVIDIA GPUs or ROCm-enabled AMD GPUs.',\n",
       "   'GPU Programming Models Overview of GPU Programming Models: In GPU computing, parallelism is exploited to perform computations more efficiently.',\n",
       "   'GPUs are designed to execute thousands of threads simultaneously, making them ideal for data-parallel tasks.',\n",
       "   'These programming models are designed to work on different hardware, like NVIDIA, AMD, or even different vendors (OpenCL), and they organize computations into small tasks that can run concurrently.',\n",
       "   'CUDA Programming Model (NVIDIA) • Thread Hierarchy: -The CUDA model organizes parallelism into a grid of blocks, where each block contains multiple threads. ‣',\n",
       "   'Threads within a block can synchronize with each other and communicate through shared memory. •',\n",
       "   'Memory Model: ‣ Shared Memory: Fast, on-chip memory shared among threads within a block. ‣',\n",
       "   'Global Memory: Large memory that is accessible by all threads but has higher latency. ‣',\n",
       "   'Constant & Texture Memory: Specialized memory for read-only data that can be accessed by all threads. ‣',\n",
       "   'Registers: Fast, temporary storage for each individual thread. •',\n",
       "   'Synchronization: ‣ CUDA can synchronize threads within a block. ‣',\n",
       "   'Synchronization across blocks is not supported in CUDA directly. •',\n",
       "   'Execution Model: ‣ CUDA kernels are executed across the grid of blocks, where each thread in a block executes independently.',\n",
       "   'Operating System Abstraction: The Process In this note, we discuss one of the most fundamental abstractions that the OS provides to users: the process.',\n",
       "   'The definition of a process, informally, is quite simple: it is a running program.',\n",
       "   'The program itself is a lifeless thing: it just sits there on the disk, a bunch of instructions (and maybe some static data), waiting to spring into action.',\n",
       "   'It is the operating system that takes these bytes and gets them running, transforming the program into something useful.',\n",
       "   'It turns out that one often wants to run more than one program at once; for example, consider your desktop or laptop where'],\n",
       "  'senetences_count_spacy': 23,\n",
       "  'text_chunks': [['‣ SIMD Engines execute parallel tasks by applying the same instruction to multiple data elements simultaneously, similar to how CUDA cores execute parallel threads in NVIDIA GPUs. •',\n",
       "    'ROCm Stack Overview: ‣ The ROCm (Radeon Open Compute) stack is AMD’s software platform for GPU computing, including libraries, tools, and compilers for GPU programming.',\n",
       "    'It supports hardware (CUs and SIMD engines) and parallel programming models such as HIP. ‣',\n",
       "    'ROCm offers tools for debugging, profiling, and managing AMD GPUs, optimized for high- performance computing (HPC), AI, and machine learning workloads. •',\n",
       "    'HIP Support: ‣ HIP (Heterogeneous Interface for Portability) enables the development of portable applications that can run on both AMD and NVIDIA GPUs.',\n",
       "    'HIP allows developers to write code that can be compiled for CUDA-enabled NVIDIA GPUs or ROCm-enabled AMD GPUs.',\n",
       "    'GPU Programming Models Overview of GPU Programming Models: In GPU computing, parallelism is exploited to perform computations more efficiently.',\n",
       "    'GPUs are designed to execute thousands of threads simultaneously, making them ideal for data-parallel tasks.',\n",
       "    'These programming models are designed to work on different hardware, like NVIDIA, AMD, or even different vendors (OpenCL), and they organize computations into small tasks that can run concurrently.',\n",
       "    'CUDA Programming Model (NVIDIA) • Thread Hierarchy: -The CUDA model organizes parallelism into a grid of blocks, where each block contains multiple threads. ‣'],\n",
       "   ['Threads within a block can synchronize with each other and communicate through shared memory. •',\n",
       "    'Memory Model: ‣ Shared Memory: Fast, on-chip memory shared among threads within a block. ‣',\n",
       "    'Global Memory: Large memory that is accessible by all threads but has higher latency. ‣',\n",
       "    'Constant & Texture Memory: Specialized memory for read-only data that can be accessed by all threads. ‣',\n",
       "    'Registers: Fast, temporary storage for each individual thread. •',\n",
       "    'Synchronization: ‣ CUDA can synchronize threads within a block. ‣',\n",
       "    'Synchronization across blocks is not supported in CUDA directly. •',\n",
       "    'Execution Model: ‣ CUDA kernels are executed across the grid of blocks, where each thread in a block executes independently.',\n",
       "    'Operating System Abstraction: The Process In this note, we discuss one of the most fundamental abstractions that the OS provides to users: the process.',\n",
       "    'The definition of a process, informally, is quite simple: it is a running program.'],\n",
       "   ['The program itself is a lifeless thing: it just sits there on the disk, a bunch of instructions (and maybe some static data), waiting to spring into action.',\n",
       "    'It is the operating system that takes these bytes and gets them running, transforming the program into something useful.',\n",
       "    'It turns out that one often wants to run more than one program at once; for example, consider your desktop or laptop where']],\n",
       "  'chunk_size': 3}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(text_info, k=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
