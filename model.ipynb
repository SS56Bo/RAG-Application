{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e745b4",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd75b8a",
   "metadata": {},
   "source": [
    "#### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa25c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eaa159",
   "metadata": {},
   "source": [
    "#### Formatting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1741ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_format(text: str)->str :\n",
    "    cleaner_text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    return cleaner_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c48ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_source(path: str)->list[dict]:\n",
    "    doc = fitz.open(path)\n",
    "    pages_text = []\n",
    "\n",
    "    for pageno, pagecontent in tqdm(enumerate(doc)):\n",
    "        text = pagecontent.get_text()\n",
    "        text = text_format(text=text)\n",
    "        pages_text.append({\"Page No.\": pageno, \"page_char_count\": len(text), \"page_word_count\": len(text.split(\" \")), \"page_sentence_count (Not accurate)\": len(text.split(\".\")), \"page_token_count\": len(text)/4, \"text\": text})\n",
    "        \n",
    "    return pages_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1ecf6",
   "metadata": {},
   "source": [
    "#### Using our custom function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78bcf739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00, 225.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Page No.': 0,\n",
       "  'page_char_count': 4082,\n",
       "  'page_word_count': 609,\n",
       "  'page_sentence_count (Not accurate)': 24,\n",
       "  'page_token_count': 1020.5,\n",
       "  'text': 'Building LLMs Understanding Large Langauge Models Large language models (LLMs), such as those offered in OpenAI’s ChatGPT, are deep neural network models that have been developed over the past few years. They ushered in a new era for natural language processing (NLP). Before the advent of LLMs, traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern recognition that could be captured with handcrafted rules or simpler models. However, they typically underperformed in language tasks that demanded complex understanding and generation abilities, such as parsing detailed instructions, conducting contextual analysis, and creating coherent and contextually appropriate original text. For example, previous generations of language models could not write an email from a list of keywords—a task that is trivial for contemporary LLMs. LLMs have remarkable capabilities to understand, generate, and interpret human language. However, it’s important to clarify that when we say language models “understand,” we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension. Enabled by advancements in deep learning, which is a subset of machine learning and artificial intelligence (AI) focused on neural networks, LLMs are trained on vast quantities of text data. This large-scale training allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches. As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more. Another important distinction between contemporary LLMs and earlier NLP models is that earlier NLP models were typically designed for specific tasks, such as text categorization, language translation, etc. While those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks. The success behind LLMs can be attributed to the transformer architecture that underpins many LLMs and the vast amounts of data on which LLMs are trained, allowing them to capture a wide variety of linguistic nuances, contexts, and patterns that would be challenging to encode manually. This shift toward implementing models based on the transformer architecture and using large training datasets to train LLMs has fundamentally transformed NLP, providing more capable tools for understanding and interacting with human language. The following discussion sets a foundation to accomplish the primary objective of this book: understanding LLMs by implementing a ChatGPT-like LLM based on the transformer architecture step by step in code. What is an LLM ? An LLM is a neural network designed to understand, generate, and respond to human-like text. These models are deep neural networks trained on massive amounts of text data, sometimes encompassing large portions of the entire publicly available text on the internet. The “large” in “large language model” refers to both the model’s size in terms of parameters and the immense dataset on which it’s trained. Models like this often have tens or even hundreds of billions of parameters, which are the adjustable weights in the network that are optimized during training to predict the next word in a sequence. Next-word prediction is sensible because it harnesses the inherent sequential nature of language to train models on understanding context, structure, and relationships within text. Yet, it is a very simple task, and so it is surprising to many researchers that it can produce such capable models. In later chapters, we will discuss and implement the nextword training procedure step by step. LLMs utilize an architecture called the transformer, which allows them to pay selective attention to different parts of the input when making predictions, making them especially adept at handling the nuances and complexities of human language.'},\n",
       " {'Page No.': 1,\n",
       "  'page_char_count': 4270,\n",
       "  'page_word_count': 649,\n",
       "  'page_sentence_count (Not accurate)': 28,\n",
       "  'page_token_count': 1067.5,\n",
       "  'text': 'Since LLMs are capable of generating text, LLMs are also often referred to as a form of generative artificial intelligence, often abbreviated as generative AI or GenAI. As illustrated in figure 1.1, AI encompasses the broader field of creating machines that can perform tasks requiring humanlike intelligence, including understanding language, recognizing patterns, and making decisions, and includes subfields like machine learning and deep learning. The algorithms used to implement AI are the focus of the field of machine learning. Specifically, machine learning involves the development of algorithms that can learn from and make predictions or decisions based on data without being explicitly programmed. To illustrate this, imagine a spam filter as a practical application of machine learning. Instead of manually writing rules to identify spam emails, a machine learning algorithm is fed examples of emails labeled as spam and legitimate emails. By minimizing the error in its predictions on a training dataset, the model then learns to recognize patterns and characteristics indicative of spam, enabling it to classify new emails as either spam or not spam. deep learning is a subset of machine learning that focuses on utilizing neural networks with three or more layers (also called deep neural networks) to model complex patterns and abstractions in data. In contrast to deep learning, traditional machine learning requires manual feature extraction. This means that human experts need to identify and select the most relevant features for the model. While the field of AI is now dominated by machine learning and deep learning, it also includes other approaches—for example, using rule-based systems, genetic algorithms, expert systems, fuzzy logic, or symbolic reasoning. Returning to the spam classification example, in traditional machine learning, human experts might manually extract features from email text such as the frequency of certain trigger words (for example, “prize,” “win,” “free”), the number of exclamation marks, use of all uppercase words, or the presence of suspicious links. This dataset, created based on these expert- defined features, would then be used to train the model. In contrast to traditional machine learning, deep learning does not require manual feature extraction. This means that human experts do not need to identify and select the most relevant features for a deep learning model. (However, both traditional machine learning and deep learning for spam classification still require the collection of labels, such as spam or non-spam, which need to be gathered either by an expert or users.) Let’s look at some of the problems LLMs can solve today, the challenges that LLMs address, and the general LLM architecture we will implement later. Applications of LLMs Owing to their advanced capabilities to parse and understand unstructured text data, LLMs have a broad range of applications across various domains. Today, LLMs are employed for machine translation, generation of novel texts, sentiment analysis, text summarization, and many other tasks. LLMs have recently been used for content creation, such as writing fiction, articles, and even computer code. LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s ChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries and augment traditional search engines such as Google Search or Microsoft Bing. Moreover, LLMs may be used for effective knowledge retrieval from vast volumes of text in specialized areas such as medicine or law. This includes sifting through documents, summarizing lengthy passages, and answering technical questions. In short, LLMs are invaluable for automating almost any task that involves parsing and generating text. Their applications are virtually endless, and as we continue to innovate and explore new ways to use these models, it’s clear that LLMs have the potential to redefine our relationship with technology, making it more conversational, intuitive, and accessible. We will focus on understanding how LLMs work from the ground up, coding an LLM that can generate texts. You will also learn about techniques that allow LLMs to carry out queries, ranging from answering questions'},\n",
       " {'Page No.': 2,\n",
       "  'page_char_count': 4204,\n",
       "  'page_word_count': 639,\n",
       "  'page_sentence_count (Not accurate)': 32,\n",
       "  'page_token_count': 1051.0,\n",
       "  'text': 'to summarizing text, translating text into different languages, and more. In other words, you will learn how complex LLM assistants such as ChatGPT work by building one step by step. Stages of building and using LLMs Why should we build our own LLMs? Coding an LLM from the ground up is an excellent exercise to understand its mechanics and limitations. Also, it equips us with the required knowledge for pretraining or fine-tuning existing open source LLM architectures to our own domain-specific datasets or tasks. Research has shown that when it comes to modeling performance, custom-built LLMs—those tailored for specific tasks or domains—can outperform general-purpose LLMs, such as those provided by ChatGPT, which are designed for a wide array of applications. Examples of these include BloombergGPT (specialized for finance) and LLMs tailored for medical question answering. Using custom-built LLMs offers several advantages, particularly regarding data privacy. For instance, companies may prefer not to share sensitive data with third-party LLM providers like OpenAI due to confidentiality concerns. Additionally, developing smaller custom LLMs enables deployment directly on customer devices, such as laptops and smartphones, which is something companies like Apple are currently exploring. This local implementation can significantly decrease latency and reduce server-related costs. Furthermore, custom LLMs grant developers complete autonomy, allowing them to control updates and modifications to the model as needed. The general process of creating an LLM includes pretraining and fine-tuning. The “pre” in “pretraining” refers to the initial phase where a model like an LLM is trained on a large, diverse dataset to develop a broad understanding of language. This pretrained model then serves as a foundational resource that can be further refined through fine-tuning, a process where the model is specifically trained on a narrower dataset that is more specific to particular tasks or domains. The first step in creating an LLM is to train it on a large corpus of text data, sometimes referred to as raw text. Here, “raw” refers to the fact that this data is just regular text without any labeling information. (Filtering may be applied, such as removing formatting characters or documents in unknown languages.) This first training stage of an LLM is also known as pretraining, creating an initial pretrained LLM, often called a base or foundation model. A typical example of such a model is the GPT-3 model (the precursor of the original model offered in ChatGPT). This model is capable of text completion—that is, finishing a half-written sentence provided by a user. It also has limited few-shot capabilities, which means it can learn to perform new tasks based on only a few examples instead of needing extensive training data. After obtaining a pretrained LLM from training on large text datasets, where the LLM is trained to predict the next word in the text, we can further train the LLM on labeled data, also known as fine-tuning. The two most popular categories of fine-tuning LLMs are instruction fine-tuning and classification fine-tuning. In instruction fine-tuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text. In classification fine-tuning, the labeled dataset consists of texts and associated class labels—for example, emails associated with “spam” and “not spam” labels. Introduction to Transformers Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper “Attention Is All You Need” (https://arxiv.org/abs/1706. 03762). To understand LLMs, we must understand the original transformer, which was developed for machine translation, translating English texts to German and French. The transformer architecture consists of two submodules: an encoder and a decoder. The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input. Then, the decoder module takes these encoded vectors and generates the'},\n",
       " {'Page No.': 3,\n",
       "  'page_char_count': 3729,\n",
       "  'page_word_count': 561,\n",
       "  'page_sentence_count (Not accurate)': 30,\n",
       "  'page_token_count': 932.25,\n",
       "  'text': 'output text. In a translation task, for example, the encoder would encode the text from the source language into vectors, and the decoder would decode these vectors to generate text in the target language. Both the encoder and decoder consist of many layers connected by a so-called self- attention mechanism. You may have many questions regarding how the inputs are preprocessed and encoded. These will be addressed in a step-by-step implementation in subsequent chapters. A key component of transformers and LLMs is the selfattention mechanism (not shown), which allows the model to weigh the importance of different words or tokens in a sequence relative to each other. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output. Later variants of the transformer architecture, such as BERT (short for bidirectional encoder representations from transformers) and the various GPT models (short for generative pretrained transformers), built on this concept to adapt this architecture for different tasks. BERT, which is built upon the original transformer’s encoder submodule, differs in its training approach from GPT. While GPT is designed for generative tasks, BERT and its variants specialize in masked word prediction, where the model predicts masked or hidden words in a given sentence. This unique training strategy equips BERT with strengths in text classification tasks, including sentiment prediction and document categorization. As an application of its capabilities, as of this writing, X (formerly Twitter) uses BERT to detect toxic content. GPT, on the other hand, focuses on the decoder portion of the original transformer architecture and is designed for tasks that require generating texts. This includes machine translation, text summarization, fiction writing, writing computer code, and more. GPT models, primarily designed and trained to perform text completion tasks, also show remarkable versatility in their capabilities. These models are adept at executing both zeroshot and few-shot learning tasks. Zero-shot learning refers to the ability to generalize to completely unseen tasks without any prior specific examples. On the other hand, fewshot learning involves learning from a minimal number of examples the user provides as input. Transformers vs. LLM Today’s LLMs are based on the transformer architecture. Hence, transformers and LLMs are terms that are often used synonymously in the literature. However, note that not all transformers are LLMs since transformers can also be used for computer vision. Also, not all LLMs are transformers, as there are LLMs based on recurrent and convolutional architectures. The main motivation behind these alternative approaches is to improve the computational efficiency of LLMs. Whether these alternative LLM architectures can compete with the capabilities of transformer-based LLMs and whether they are going to be adopted in practice remains to be seen. For simplicity, I use the term “LLM” to refer to transformer-based LLMs similar to GPT. Utilizing large datasets The large training datasets for popular GPT- and BERT-like models represent diverse and comprehensive text corpora encompassing billions of words, which include a vast array of topics and natural and computer languages. To provide a concrete example, table 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base model for the first version of ChatGPT. Dataset Name Dataset Description Number of tokens Proportion in training data Common Crawl    (filtered) Web crawl data 410 billion 60% WebText2 Web crawl data 19 billion 22%'},\n",
       " {'Page No.': 4,\n",
       "  'page_char_count': 3720,\n",
       "  'page_word_count': 582,\n",
       "  'page_sentence_count (Not accurate)': 33,\n",
       "  'page_token_count': 930.0,\n",
       "  'text': 'Dataset Name Dataset Description Number of tokens Proportion in training data Books1 Internet-based book corpus 12 billion 8% Books2 Internet-based book corpus 55 billion 8% Wikipedia High-quality text 3 billion 3% The table above, reports the number of tokens, where a token is a unit of text that a model reads and the number of tokens in a dataset is roughly equivalent to the number of words and punctuation characters in the text. The main takeaway is that the scale and diversity of this training dataset allow these models to perform well on diverse tasks, including language syntax, semantics, and context—even some requiring general knowledge. The pretrained nature of these models makes them incredibly versatile for further fine-tuning on downstream tasks, which is why they are also known as base or foundation models. Pretraining LLMs requires access to significant resources and is very expensive. For example, the GPT-3 pretraining cost is estimated to be 4.6 million USD in terms of cloud computing credits (https://mng.bz/VxEW). The good news is that many pretrained LLMs, available as open source models, can be used as general-purpose tools to write, extract, and edit texts that were not part of the training data. Also, LLMs can be fine-tuned on specific tasks with relatively smaller datasets, reducing the computational resources needed and improving performance. We will implement the code for pretraining and use it to pretrain an LLM for educational purposes. All computations are executable on consumer hardware. After implementing the pretraining code, we will learn how to reuse openly available model weights and load them into the architecture we will implement, allowing us to skip the expensive pretraining stage when we fine-tune our LLM. GPT-3 Dataset Details The Table above displays the dataset used for GPT-3. The proportions column in the table sums up to 100% of the sampled data, adjusted for rounding errors. Although the subsets in the Number of Tokens column total 499 billion, the model was trained on only 300 billion tokens. The authors of the GPT-3 paper did not specify why the model was not trained on all 499 billion tokens. For context, consider the size of the CommonCrawl dataset, which alone consists of 410 billion tokens and requires about 570 GB of storage. In comparison, later iterations of models like GPT-3, such as Meta’s LLaMA, have expanded their training scope to include additional data sources like Arxiv research papers (92 GB) and StackExchange’s code-related Q&As (78 GB). The authors of the GPT-3 paper did not share the training dataset, but a comparable dataset that is publicly available is Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research by Soldaini et al. 2024 (https://arxiv.org/abs/2402.00159). However, the collection may contain copyrighted works, and the exact usage terms may depend on the intended use case and country. A closer look at the GPT architecture GPT was originally introduced in the paper “Improving Language Understanding by Generative Pre- Training” (https://mng.bz/x2qg) by Radford et al. from OpenAI. GPT-3 is a scaled-up version of this model that has more parameters and was trained on a larger dataset. In addition, the original model offered in ChatGPT was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI’s InstructGPT paper (https://arxiv.org/abs/2203.02155). As figure 1.6 shows, these models are competent text completion models and can carry out other tasks such as spelling correction, classification, or language translation. This is actually very remarkable given that GPT models are pretrained on a relatively simple next-word prediction task'},\n",
       " {'Page No.': 5,\n",
       "  'page_char_count': 4350,\n",
       "  'page_word_count': 655,\n",
       "  'page_sentence_count (Not accurate)': 34,\n",
       "  'page_token_count': 1087.5,\n",
       "  'text': 'The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling. This means that we don’t need to collect labels for the training data explicitly but can use the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict. Since this next-word prediction task allows us to create labels “on the fly,” it is possible to use massive unlabeled text datasets to train LLMs. Compared to the original transformer architecture, the general GPT architecture is relatively simple. Essentially, it’s just the decoder part without the encoder. Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of autoregressive model. Autoregressive models incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves the coherence of the resulting text. Architectures such as GPT-3 are also significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion parameters in total. GPT-3 was introduced in 2020, which, by the standards of deep learning and large language model development, is considered a long time ago. However, more recent architectures, such as Meta’s Llama models, are still based on the same underlying concepts, introducing only minor modifications. Hence, understanding GPT remains as relevant as ever, so I focus on implementing the prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs. Although the original transformer model, consisting of encoder and decoder blocks, was explicitly designed for language translation, GPT models—despite their larger yet simpler decoder-only architecture aimed at next-word prediction—are also capable of performing translation tasks. This capability was initially unexpected to researchers, as it emerged from a model primarily trained on a next-word prediction task, which is a task that did not specifically target translation. The ability to perform tasks that the model wasn’t explicitly trained to perform is called an emergent behavior. This capability isn’t explicitly taught during training but emerges as a natural consequence of the model’s exposure to vast quantities of multilingual data in diverse contexts. The fact that GPT models can “learn” the translation patterns between languages and perform translation tasks even though they weren’t specifically trained for it demonstrates the benefits and capabilities of these large-scale, generative language models. We can perform diverse tasks without using diverse models for each. Working with text data During the pretraining stage, LLMs process text one word at a time. Training LLMs with millions to billions of parameters using a next-word prediction task yields models with impressive capabilities. These models can then be further finetuned to follow general instructions or perform specific target tasks. But before we can implement and train LLMs, we need to prepare the training dataset. This involves splitting text into individual word and subword tokens, which can then be encoded into vector representations for the LLM. You’ll also learn about advanced tokenization schemes like byte pair encoding, which is utilized in popular LLMs like GPT. Lastly, we’ll implement a sampling and data-loading strategy to produce the input-output pairs necessary for training LLMs. Understanding word embeddings Deep neural network models, including LLMs, cannot process raw text directly. Since text is categorical, it isn’t compatible with the mathematical operations used to implement and train neural networks. Therefore, we need a way to represent words as continuous-valued vectors. The concept of converting data into a vector format is often referred to as embedding. Using a specific neural network layer or another pretrained neural network model, we can embed different data types—for example, video, audio, and text. However, it’s important to note that different data formats require distinct embedding models. For example, an embedding model designed for text would not be'},\n",
       " {'Page No.': 6,\n",
       "  'page_char_count': 4223,\n",
       "  'page_word_count': 653,\n",
       "  'page_sentence_count (Not accurate)': 37,\n",
       "  'page_token_count': 1055.75,\n",
       "  'text': 'suitable for embedding audio or video data. At its core, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space— the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process. While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for retrieval-augmented generation. Retrieval-augmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text. Since our goal is to train GPT- like LLMs, which learn to generate text one word at a time, we will focus on word embeddings. Several algorithms and frameworks have been developed to generate word embeddings. One of the earlier and most popular examples is the Word2Vec approach. Word2Vec trained neural network architecture to generate word embeddings by predicting the context of a word given the target word or vice versa. The main idea behind Word2Vec is that words that appear in similar contexts tend to have similar meanings. Consequently, when projected into twodimensional word embeddings for visualization purposes, similar terms are clustered together. Word embeddings can have varying dimensions, from one to thousands. A higher dimensionality might capture more nuanced relationships but at the cost of computational efficiency. While we can use pretrained models such as Word2Vec to generate embeddings for machine learning models, LLMs commonly produce their own embeddings that are part of the input layer and are updated during training. The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand. Unfortunately, high-dimensional embeddings present a challenge for visualization because our sensory perception and common graphical representations are inherently limited to three dimensions or fewer, which is why figure 2.3 shows two-dimensional embeddings in a two-dimensional scatterplot. However, when working with LLMs, we typically use embeddings with a much higher dimensionality. For both GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality of the model’s hidden states) varies based on the specific model variant and size. It is a tradeoff between performance and efficiency. The smallest GPT-2 models (117M and 125M parameters) use an embedding size of 768 dimensions to provide concrete examples. The largest GPT-3 model (175B parameters) uses an embedding size of 12,288 dimensions. Tokenizing text Tokenization is a fundamental step in Natural Language Processing (NLP). It involves dividing a Textual input into smaller units known as tokens. These tokens can be in the form of words, characters, sub-words, or sentences. It helps in improving interpretability of text by different models. Let’s understand How Tokenization Works. Depending on the LLM, some researchers also consider additional special tokens such as the following: • [BOS] (beginning of sequence) —This token marks the start of a text. It signifies to the LLM where a piece of content begins. • [EOS] (end of sequence) —This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one ends and the next begins. • [PAD] (padding) —When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or “padded” using the [PAD] token, up to the length of the longest text in the batch. The tokenizer used for GPT models does not need any of these tokens; it only uses an <|endoftext|> token for simplicity. <|endoftext|> is analogous to the [EOS] token. <|endoftext|> is also used for padding. When training on batched inputs, we typically use a mask, meaning we don’t attend to'},\n",
       " {'Page No.': 7,\n",
       "  'page_char_count': 3745,\n",
       "  'page_word_count': 559,\n",
       "  'page_sentence_count (Not accurate)': 26,\n",
       "  'page_token_count': 936.25,\n",
       "  'text': 'padded tokens. Thus, the specific token chosen for padding becomes inconsequential. Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token for out-of-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks words down into subword units. Attention in LLMs In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by “soft” weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size. Unlike “hard” weights, which are computed during the backwards training pass, “soft” weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme. Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state. Transformers In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets. The modern version of the transformer was proposed in the 2017 paper “Attention Is All You Need” by researchers at Google.Transformers were first developed as an improvement over previous architectures for machine translation,but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,audio,multimodal learning, robotics,[9] and even playing chess.It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers). Attention Mechanisms Before we dive into the self-attention mechanism at the heart of LLMs, let’s consider the problem with pre-LLM architectures that do not include attention mechanisms. Suppose we want to develop a language translation model that translates text from one language into another. We can’t simply translate a text word by word due to the grammatical structures in the source and target language. To address this problem, it is common to use a deep neural network with two submodules, an encoder and a decoder. The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text.'},\n",
       " {'Page No.': 8,\n",
       "  'page_char_count': 2651,\n",
       "  'page_word_count': 420,\n",
       "  'page_sentence_count (Not accurate)': 18,\n",
       "  'page_token_count': 662.75,\n",
       "  'text': 'Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder–decoder architecture for language translation. An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text. If you are unfamiliar with RNNs, don’t worry—you don’t need to know the detailed workings of RNNs to follow this discussion; our focus here is more on the general concept of the encoder–decoder setup. In an encoder–decoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence in the final hidden state. The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction. While we don’t need to know the inner workings of these encoder–decoder RNNs, the key idea here is that the encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in this hidden state to produce the output. You can think of this hidden state as an embedding vector. The big limitation of encoder–decoder RNNs is that the RNN can’t directly access earlier hidden states from the encoder during the decoding phase. Consequently, it relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances. Capturing data dependencies with attention mechanisms Although RNNs work fine for translating short sentences, they don’t work well for longer texts as they don’t have direct access to previous words in the input. One major shortcoming in this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder. Hence, researchers developed the Bahdanau attention mechanism for RNNs in 2014 (named after the first author of the respective paper), which modifies the encoder– decoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step. Interestingly, only three years later, researchers found that RNN architectures are not required for building deep neural networks for natural language processing and proposed the original transformer architecture including a self-attention mechanism inspired by the Bahdanau attention mechanism.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "text_info = get_text_from_source(path=\"Test.pdf\")\n",
    "text_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c6f78",
   "metadata": {},
   "source": [
    "#### Converting to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71177ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page No.</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count (Not accurate)</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.00</td>\n",
       "      <td>3886.00</td>\n",
       "      <td>591.89</td>\n",
       "      <td>29.11</td>\n",
       "      <td>971.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.74</td>\n",
       "      <td>525.97</td>\n",
       "      <td>75.14</td>\n",
       "      <td>5.82</td>\n",
       "      <td>131.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2651.00</td>\n",
       "      <td>420.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>662.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.00</td>\n",
       "      <td>3729.00</td>\n",
       "      <td>561.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>932.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4082.00</td>\n",
       "      <td>609.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1020.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.00</td>\n",
       "      <td>4223.00</td>\n",
       "      <td>649.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>1055.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.00</td>\n",
       "      <td>4350.00</td>\n",
       "      <td>655.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>1087.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Page No.  page_char_count  page_word_count  \\\n",
       "count      9.00             9.00             9.00   \n",
       "mean       4.00          3886.00           591.89   \n",
       "std        2.74           525.97            75.14   \n",
       "min        0.00          2651.00           420.00   \n",
       "25%        2.00          3729.00           561.00   \n",
       "50%        4.00          4082.00           609.00   \n",
       "75%        6.00          4223.00           649.00   \n",
       "max        8.00          4350.00           655.00   \n",
       "\n",
       "       page_sentence_count (Not accurate)  page_token_count  \n",
       "count                                9.00              9.00  \n",
       "mean                                29.11            971.50  \n",
       "std                                  5.82            131.49  \n",
       "min                                 18.00            662.75  \n",
       "25%                                 26.00            932.25  \n",
       "50%                                 30.00           1020.50  \n",
       "75%                                 33.00           1055.75  \n",
       "max                                 37.00           1087.50  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(text_info)\n",
    "data.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b8468",
   "metadata": {},
   "source": [
    "#### Splitting text. Conversion of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae6336a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1d2b18e42d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instance of English\n",
    "obj = English()\n",
    "\n",
    "#adding pipeling\n",
    "obj.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b744a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 155.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for items in tqdm(text_info):\n",
    "    items[\"sentences\"] = list(obj(items[\"text\"]).sents)\n",
    "    items[\"sentences\"]= [str(sentence) for sentence in items[\"sentences\"]]\n",
    "    items[\"sentences_count_spacy\"] = len(items[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14471b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Page No.': 5,\n",
       "  'page_char_count': 4350,\n",
       "  'page_word_count': 655,\n",
       "  'page_sentence_count (Not accurate)': 34,\n",
       "  'page_token_count': 1087.5,\n",
       "  'text': 'The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling. This means that we don’t need to collect labels for the training data explicitly but can use the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict. Since this next-word prediction task allows us to create labels “on the fly,” it is possible to use massive unlabeled text datasets to train LLMs. Compared to the original transformer architecture, the general GPT architecture is relatively simple. Essentially, it’s just the decoder part without the encoder. Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of autoregressive model. Autoregressive models incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves the coherence of the resulting text. Architectures such as GPT-3 are also significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion parameters in total. GPT-3 was introduced in 2020, which, by the standards of deep learning and large language model development, is considered a long time ago. However, more recent architectures, such as Meta’s Llama models, are still based on the same underlying concepts, introducing only minor modifications. Hence, understanding GPT remains as relevant as ever, so I focus on implementing the prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs. Although the original transformer model, consisting of encoder and decoder blocks, was explicitly designed for language translation, GPT models—despite their larger yet simpler decoder-only architecture aimed at next-word prediction—are also capable of performing translation tasks. This capability was initially unexpected to researchers, as it emerged from a model primarily trained on a next-word prediction task, which is a task that did not specifically target translation. The ability to perform tasks that the model wasn’t explicitly trained to perform is called an emergent behavior. This capability isn’t explicitly taught during training but emerges as a natural consequence of the model’s exposure to vast quantities of multilingual data in diverse contexts. The fact that GPT models can “learn” the translation patterns between languages and perform translation tasks even though they weren’t specifically trained for it demonstrates the benefits and capabilities of these large-scale, generative language models. We can perform diverse tasks without using diverse models for each. Working with text data During the pretraining stage, LLMs process text one word at a time. Training LLMs with millions to billions of parameters using a next-word prediction task yields models with impressive capabilities. These models can then be further finetuned to follow general instructions or perform specific target tasks. But before we can implement and train LLMs, we need to prepare the training dataset. This involves splitting text into individual word and subword tokens, which can then be encoded into vector representations for the LLM. You’ll also learn about advanced tokenization schemes like byte pair encoding, which is utilized in popular LLMs like GPT. Lastly, we’ll implement a sampling and data-loading strategy to produce the input-output pairs necessary for training LLMs. Understanding word embeddings Deep neural network models, including LLMs, cannot process raw text directly. Since text is categorical, it isn’t compatible with the mathematical operations used to implement and train neural networks. Therefore, we need a way to represent words as continuous-valued vectors. The concept of converting data into a vector format is often referred to as embedding. Using a specific neural network layer or another pretrained neural network model, we can embed different data types—for example, video, audio, and text. However, it’s important to note that different data formats require distinct embedding models. For example, an embedding model designed for text would not be',\n",
       "  'sentences': ['The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling.',\n",
       "   'This means that we don’t need to collect labels for the training data explicitly but can use the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict.',\n",
       "   'Since this next-word prediction task allows us to create labels “on the fly,” it is possible to use massive unlabeled text datasets to train LLMs.',\n",
       "   'Compared to the original transformer architecture, the general GPT architecture is relatively simple.',\n",
       "   'Essentially, it’s just the decoder part without the encoder.',\n",
       "   'Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of autoregressive model.',\n",
       "   'Autoregressive models incorporate their previous outputs as inputs for future predictions.',\n",
       "   'Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves the coherence of the resulting text.',\n",
       "   'Architectures such as GPT-3 are also significantly larger than the original transformer model.',\n",
       "   'For instance, the original transformer repeated the encoder and decoder blocks six times.',\n",
       "   'GPT-3 has 96 transformer layers and 175 billion parameters in total.',\n",
       "   'GPT-3 was introduced in 2020, which, by the standards of deep learning and large language model development, is considered a long time ago.',\n",
       "   'However, more recent architectures, such as Meta’s Llama models, are still based on the same underlying concepts, introducing only minor modifications.',\n",
       "   'Hence, understanding GPT remains as relevant as ever, so I focus on implementing the prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs.',\n",
       "   'Although the original transformer model, consisting of encoder and decoder blocks, was explicitly designed for language translation, GPT models—despite their larger yet simpler decoder-only architecture aimed at next-word prediction—are also capable of performing translation tasks.',\n",
       "   'This capability was initially unexpected to researchers, as it emerged from a model primarily trained on a next-word prediction task, which is a task that did not specifically target translation.',\n",
       "   'The ability to perform tasks that the model wasn’t explicitly trained to perform is called an emergent behavior.',\n",
       "   'This capability isn’t explicitly taught during training but emerges as a natural consequence of the model’s exposure to vast quantities of multilingual data in diverse contexts.',\n",
       "   'The fact that GPT models can “learn” the translation patterns between languages and perform translation tasks even though they weren’t specifically trained for it demonstrates the benefits and capabilities of these large-scale, generative language models.',\n",
       "   'We can perform diverse tasks without using diverse models for each.',\n",
       "   'Working with text data During the pretraining stage, LLMs process text one word at a time.',\n",
       "   'Training LLMs with millions to billions of parameters using a next-word prediction task yields models with impressive capabilities.',\n",
       "   'These models can then be further finetuned to follow general instructions or perform specific target tasks.',\n",
       "   'But before we can implement and train LLMs, we need to prepare the training dataset.',\n",
       "   'This involves splitting text into individual word and subword tokens, which can then be encoded into vector representations for the LLM.',\n",
       "   'You’ll also learn about advanced tokenization schemes like byte pair encoding, which is utilized in popular LLMs like GPT.',\n",
       "   'Lastly, we’ll implement a sampling and data-loading strategy to produce the input-output pairs necessary for training LLMs.',\n",
       "   'Understanding word embeddings Deep neural network models, including LLMs, cannot process raw text directly.',\n",
       "   'Since text is categorical, it isn’t compatible with the mathematical operations used to implement and train neural networks.',\n",
       "   'Therefore, we need a way to represent words as continuous-valued vectors.',\n",
       "   'The concept of converting data into a vector format is often referred to as embedding.',\n",
       "   'Using a specific neural network layer or another pretrained neural network model, we can embed different data types—for example, video, audio, and text.',\n",
       "   'However, it’s important to note that different data formats require distinct embedding models.',\n",
       "   'For example, an embedding model designed for text would not be'],\n",
       "  'sentences_count_spacy': 34}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(text_info, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b87b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page No.</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count (Not accurate)</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>sentences_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.00</td>\n",
       "      <td>3886.00</td>\n",
       "      <td>591.89</td>\n",
       "      <td>29.11</td>\n",
       "      <td>971.50</td>\n",
       "      <td>27.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.74</td>\n",
       "      <td>525.97</td>\n",
       "      <td>75.14</td>\n",
       "      <td>5.82</td>\n",
       "      <td>131.49</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2651.00</td>\n",
       "      <td>420.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>662.75</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.00</td>\n",
       "      <td>3729.00</td>\n",
       "      <td>561.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>932.25</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4082.00</td>\n",
       "      <td>609.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1020.50</td>\n",
       "      <td>27.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.00</td>\n",
       "      <td>4223.00</td>\n",
       "      <td>649.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>1055.75</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.00</td>\n",
       "      <td>4350.00</td>\n",
       "      <td>655.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>1087.50</td>\n",
       "      <td>36.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Page No.  page_char_count  page_word_count  \\\n",
       "count      9.00             9.00             9.00   \n",
       "mean       4.00          3886.00           591.89   \n",
       "std        2.74           525.97            75.14   \n",
       "min        0.00          2651.00           420.00   \n",
       "25%        2.00          3729.00           561.00   \n",
       "50%        4.00          4082.00           609.00   \n",
       "75%        6.00          4223.00           649.00   \n",
       "max        8.00          4350.00           655.00   \n",
       "\n",
       "       page_sentence_count (Not accurate)  page_token_count  \\\n",
       "count                                9.00              9.00   \n",
       "mean                                29.11            971.50   \n",
       "std                                  5.82            131.49   \n",
       "min                                 18.00            662.75   \n",
       "25%                                 26.00            932.25   \n",
       "50%                                 30.00           1020.50   \n",
       "75%                                 33.00           1055.75   \n",
       "max                                 37.00           1087.50   \n",
       "\n",
       "       sentences_count_spacy  \n",
       "count                   9.00  \n",
       "mean                   27.44  \n",
       "std                     5.88  \n",
       "min                    17.00  \n",
       "25%                    24.00  \n",
       "50%                    27.00  \n",
       "75%                    32.00  \n",
       "max                    36.00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(text_info)\n",
    "data.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0af18",
   "metadata": {},
   "source": [
    "#### Chunking sentences into group of 10 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546875e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10\n",
    "def create_chunk(big_list: list[str], split_size: int=chunk_size)->list[list[str]]:\n",
    "    return [big_list[i:i+split_size] for i in range(0, len(big_list), split_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d52f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 28793.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#Chunk size\n",
    "for items in tqdm(text_info):\n",
    "    items[\"text_chunks\"] = create_chunk(big_list=items[\"sentences\"], split_size=chunk_size)\n",
    "    items[\"chunk_size\"] = len(items[\"text_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e30dee33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Page No.': 0,\n",
       "  'page_char_count': 4082,\n",
       "  'page_word_count': 609,\n",
       "  'page_sentence_count (Not accurate)': 24,\n",
       "  'page_token_count': 1020.5,\n",
       "  'text': 'Building LLMs Understanding Large Langauge Models Large language models (LLMs), such as those offered in OpenAI’s ChatGPT, are deep neural network models that have been developed over the past few years. They ushered in a new era for natural language processing (NLP). Before the advent of LLMs, traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern recognition that could be captured with handcrafted rules or simpler models. However, they typically underperformed in language tasks that demanded complex understanding and generation abilities, such as parsing detailed instructions, conducting contextual analysis, and creating coherent and contextually appropriate original text. For example, previous generations of language models could not write an email from a list of keywords—a task that is trivial for contemporary LLMs. LLMs have remarkable capabilities to understand, generate, and interpret human language. However, it’s important to clarify that when we say language models “understand,” we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension. Enabled by advancements in deep learning, which is a subset of machine learning and artificial intelligence (AI) focused on neural networks, LLMs are trained on vast quantities of text data. This large-scale training allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches. As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more. Another important distinction between contemporary LLMs and earlier NLP models is that earlier NLP models were typically designed for specific tasks, such as text categorization, language translation, etc. While those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks. The success behind LLMs can be attributed to the transformer architecture that underpins many LLMs and the vast amounts of data on which LLMs are trained, allowing them to capture a wide variety of linguistic nuances, contexts, and patterns that would be challenging to encode manually. This shift toward implementing models based on the transformer architecture and using large training datasets to train LLMs has fundamentally transformed NLP, providing more capable tools for understanding and interacting with human language. The following discussion sets a foundation to accomplish the primary objective of this book: understanding LLMs by implementing a ChatGPT-like LLM based on the transformer architecture step by step in code. What is an LLM ? An LLM is a neural network designed to understand, generate, and respond to human-like text. These models are deep neural networks trained on massive amounts of text data, sometimes encompassing large portions of the entire publicly available text on the internet. The “large” in “large language model” refers to both the model’s size in terms of parameters and the immense dataset on which it’s trained. Models like this often have tens or even hundreds of billions of parameters, which are the adjustable weights in the network that are optimized during training to predict the next word in a sequence. Next-word prediction is sensible because it harnesses the inherent sequential nature of language to train models on understanding context, structure, and relationships within text. Yet, it is a very simple task, and so it is surprising to many researchers that it can produce such capable models. In later chapters, we will discuss and implement the nextword training procedure step by step. LLMs utilize an architecture called the transformer, which allows them to pay selective attention to different parts of the input when making predictions, making them especially adept at handling the nuances and complexities of human language.',\n",
       "  'sentences': ['Building LLMs Understanding Large Langauge Models Large language models (LLMs), such as those offered in OpenAI’s ChatGPT, are deep neural network models that have been developed over the past few years.',\n",
       "   'They ushered in a new era for natural language processing (NLP).',\n",
       "   'Before the advent of LLMs, traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern recognition that could be captured with handcrafted rules or simpler models.',\n",
       "   'However, they typically underperformed in language tasks that demanded complex understanding and generation abilities, such as parsing detailed instructions, conducting contextual analysis, and creating coherent and contextually appropriate original text.',\n",
       "   'For example, previous generations of language models could not write an email from a list of keywords—a task that is trivial for contemporary LLMs.',\n",
       "   'LLMs have remarkable capabilities to understand, generate, and interpret human language.',\n",
       "   'However, it’s important to clarify that when we say language models “understand,” we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension.',\n",
       "   'Enabled by advancements in deep learning, which is a subset of machine learning and artificial intelligence (AI) focused on neural networks, LLMs are trained on vast quantities of text data.',\n",
       "   'This large-scale training allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches.',\n",
       "   'As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more.',\n",
       "   'Another important distinction between contemporary LLMs and earlier NLP models is that earlier NLP models were typically designed for specific tasks, such as text categorization, language translation, etc.',\n",
       "   'While those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks.',\n",
       "   'The success behind LLMs can be attributed to the transformer architecture that underpins many LLMs and the vast amounts of data on which LLMs are trained, allowing them to capture a wide variety of linguistic nuances, contexts, and patterns that would be challenging to encode manually.',\n",
       "   'This shift toward implementing models based on the transformer architecture and using large training datasets to train LLMs has fundamentally transformed NLP, providing more capable tools for understanding and interacting with human language.',\n",
       "   'The following discussion sets a foundation to accomplish the primary objective of this book: understanding LLMs by implementing a ChatGPT-like LLM based on the transformer architecture step by step in code.',\n",
       "   'What is an LLM ?',\n",
       "   'An LLM is a neural network designed to understand, generate, and respond to human-like text.',\n",
       "   'These models are deep neural networks trained on massive amounts of text data, sometimes encompassing large portions of the entire publicly available text on the internet.',\n",
       "   'The “large” in “large language model” refers to both the model’s size in terms of parameters and the immense dataset on which it’s trained.',\n",
       "   'Models like this often have tens or even hundreds of billions of parameters, which are the adjustable weights in the network that are optimized during training to predict the next word in a sequence.',\n",
       "   'Next-word prediction is sensible because it harnesses the inherent sequential nature of language to train models on understanding context, structure, and relationships within text.',\n",
       "   'Yet, it is a very simple task, and so it is surprising to many researchers that it can produce such capable models.',\n",
       "   'In later chapters, we will discuss and implement the nextword training procedure step by step.',\n",
       "   'LLMs utilize an architecture called the transformer, which allows them to pay selective attention to different parts of the input when making predictions, making them especially adept at handling the nuances and complexities of human language.'],\n",
       "  'sentences_count_spacy': 24,\n",
       "  'text_chunks': [['Building LLMs Understanding Large Langauge Models Large language models (LLMs), such as those offered in OpenAI’s ChatGPT, are deep neural network models that have been developed over the past few years.',\n",
       "    'They ushered in a new era for natural language processing (NLP).',\n",
       "    'Before the advent of LLMs, traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern recognition that could be captured with handcrafted rules or simpler models.',\n",
       "    'However, they typically underperformed in language tasks that demanded complex understanding and generation abilities, such as parsing detailed instructions, conducting contextual analysis, and creating coherent and contextually appropriate original text.',\n",
       "    'For example, previous generations of language models could not write an email from a list of keywords—a task that is trivial for contemporary LLMs.',\n",
       "    'LLMs have remarkable capabilities to understand, generate, and interpret human language.',\n",
       "    'However, it’s important to clarify that when we say language models “understand,” we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension.',\n",
       "    'Enabled by advancements in deep learning, which is a subset of machine learning and artificial intelligence (AI) focused on neural networks, LLMs are trained on vast quantities of text data.',\n",
       "    'This large-scale training allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches.',\n",
       "    'As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more.'],\n",
       "   ['Another important distinction between contemporary LLMs and earlier NLP models is that earlier NLP models were typically designed for specific tasks, such as text categorization, language translation, etc.',\n",
       "    'While those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks.',\n",
       "    'The success behind LLMs can be attributed to the transformer architecture that underpins many LLMs and the vast amounts of data on which LLMs are trained, allowing them to capture a wide variety of linguistic nuances, contexts, and patterns that would be challenging to encode manually.',\n",
       "    'This shift toward implementing models based on the transformer architecture and using large training datasets to train LLMs has fundamentally transformed NLP, providing more capable tools for understanding and interacting with human language.',\n",
       "    'The following discussion sets a foundation to accomplish the primary objective of this book: understanding LLMs by implementing a ChatGPT-like LLM based on the transformer architecture step by step in code.',\n",
       "    'What is an LLM ?',\n",
       "    'An LLM is a neural network designed to understand, generate, and respond to human-like text.',\n",
       "    'These models are deep neural networks trained on massive amounts of text data, sometimes encompassing large portions of the entire publicly available text on the internet.',\n",
       "    'The “large” in “large language model” refers to both the model’s size in terms of parameters and the immense dataset on which it’s trained.',\n",
       "    'Models like this often have tens or even hundreds of billions of parameters, which are the adjustable weights in the network that are optimized during training to predict the next word in a sequence.'],\n",
       "   ['Next-word prediction is sensible because it harnesses the inherent sequential nature of language to train models on understanding context, structure, and relationships within text.',\n",
       "    'Yet, it is a very simple task, and so it is surprising to many researchers that it can produce such capable models.',\n",
       "    'In later chapters, we will discuss and implement the nextword training procedure step by step.',\n",
       "    'LLMs utilize an architecture called the transformer, which allows them to pay selective attention to different parts of the input when making predictions, making them especially adept at handling the nuances and complexities of human language.']],\n",
       "  'chunk_size': 3},\n",
       " {'Page No.': 1,\n",
       "  'page_char_count': 4270,\n",
       "  'page_word_count': 649,\n",
       "  'page_sentence_count (Not accurate)': 28,\n",
       "  'page_token_count': 1067.5,\n",
       "  'text': 'Since LLMs are capable of generating text, LLMs are also often referred to as a form of generative artificial intelligence, often abbreviated as generative AI or GenAI. As illustrated in figure 1.1, AI encompasses the broader field of creating machines that can perform tasks requiring humanlike intelligence, including understanding language, recognizing patterns, and making decisions, and includes subfields like machine learning and deep learning. The algorithms used to implement AI are the focus of the field of machine learning. Specifically, machine learning involves the development of algorithms that can learn from and make predictions or decisions based on data without being explicitly programmed. To illustrate this, imagine a spam filter as a practical application of machine learning. Instead of manually writing rules to identify spam emails, a machine learning algorithm is fed examples of emails labeled as spam and legitimate emails. By minimizing the error in its predictions on a training dataset, the model then learns to recognize patterns and characteristics indicative of spam, enabling it to classify new emails as either spam or not spam. deep learning is a subset of machine learning that focuses on utilizing neural networks with three or more layers (also called deep neural networks) to model complex patterns and abstractions in data. In contrast to deep learning, traditional machine learning requires manual feature extraction. This means that human experts need to identify and select the most relevant features for the model. While the field of AI is now dominated by machine learning and deep learning, it also includes other approaches—for example, using rule-based systems, genetic algorithms, expert systems, fuzzy logic, or symbolic reasoning. Returning to the spam classification example, in traditional machine learning, human experts might manually extract features from email text such as the frequency of certain trigger words (for example, “prize,” “win,” “free”), the number of exclamation marks, use of all uppercase words, or the presence of suspicious links. This dataset, created based on these expert- defined features, would then be used to train the model. In contrast to traditional machine learning, deep learning does not require manual feature extraction. This means that human experts do not need to identify and select the most relevant features for a deep learning model. (However, both traditional machine learning and deep learning for spam classification still require the collection of labels, such as spam or non-spam, which need to be gathered either by an expert or users.) Let’s look at some of the problems LLMs can solve today, the challenges that LLMs address, and the general LLM architecture we will implement later. Applications of LLMs Owing to their advanced capabilities to parse and understand unstructured text data, LLMs have a broad range of applications across various domains. Today, LLMs are employed for machine translation, generation of novel texts, sentiment analysis, text summarization, and many other tasks. LLMs have recently been used for content creation, such as writing fiction, articles, and even computer code. LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s ChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries and augment traditional search engines such as Google Search or Microsoft Bing. Moreover, LLMs may be used for effective knowledge retrieval from vast volumes of text in specialized areas such as medicine or law. This includes sifting through documents, summarizing lengthy passages, and answering technical questions. In short, LLMs are invaluable for automating almost any task that involves parsing and generating text. Their applications are virtually endless, and as we continue to innovate and explore new ways to use these models, it’s clear that LLMs have the potential to redefine our relationship with technology, making it more conversational, intuitive, and accessible. We will focus on understanding how LLMs work from the ground up, coding an LLM that can generate texts. You will also learn about techniques that allow LLMs to carry out queries, ranging from answering questions',\n",
       "  'sentences': ['Since LLMs are capable of generating text, LLMs are also often referred to as a form of generative artificial intelligence, often abbreviated as generative AI or GenAI.',\n",
       "   'As illustrated in figure 1.1, AI encompasses the broader field of creating machines that can perform tasks requiring humanlike intelligence, including understanding language, recognizing patterns, and making decisions, and includes subfields like machine learning and deep learning.',\n",
       "   'The algorithms used to implement AI are the focus of the field of machine learning.',\n",
       "   'Specifically, machine learning involves the development of algorithms that can learn from and make predictions or decisions based on data without being explicitly programmed.',\n",
       "   'To illustrate this, imagine a spam filter as a practical application of machine learning.',\n",
       "   'Instead of manually writing rules to identify spam emails, a machine learning algorithm is fed examples of emails labeled as spam and legitimate emails.',\n",
       "   'By minimizing the error in its predictions on a training dataset, the model then learns to recognize patterns and characteristics indicative of spam, enabling it to classify new emails as either spam or not spam.',\n",
       "   'deep learning is a subset of machine learning that focuses on utilizing neural networks with three or more layers (also called deep neural networks) to model complex patterns and abstractions in data.',\n",
       "   'In contrast to deep learning, traditional machine learning requires manual feature extraction.',\n",
       "   'This means that human experts need to identify and select the most relevant features for the model.',\n",
       "   'While the field of AI is now dominated by machine learning and deep learning, it also includes other approaches—for example, using rule-based systems, genetic algorithms, expert systems, fuzzy logic, or symbolic reasoning.',\n",
       "   'Returning to the spam classification example, in traditional machine learning, human experts might manually extract features from email text such as the frequency of certain trigger words (for example, “prize,” “win,” “free”), the number of exclamation marks, use of all uppercase words, or the presence of suspicious links.',\n",
       "   'This dataset, created based on these expert- defined features, would then be used to train the model.',\n",
       "   'In contrast to traditional machine learning, deep learning does not require manual feature extraction.',\n",
       "   'This means that human experts do not need to identify and select the most relevant features for a deep learning model. (',\n",
       "   'However, both traditional machine learning and deep learning for spam classification still require the collection of labels, such as spam or non-spam, which need to be gathered either by an expert or users.)',\n",
       "   'Let’s look at some of the problems LLMs can solve today, the challenges that LLMs address, and the general LLM architecture we will implement later.',\n",
       "   'Applications of LLMs Owing to their advanced capabilities to parse and understand unstructured text data, LLMs have a broad range of applications across various domains.',\n",
       "   'Today, LLMs are employed for machine translation, generation of novel texts, sentiment analysis, text summarization, and many other tasks.',\n",
       "   'LLMs have recently been used for content creation, such as writing fiction, articles, and even computer code.',\n",
       "   'LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s ChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries and augment traditional search engines such as Google Search or Microsoft Bing.',\n",
       "   'Moreover, LLMs may be used for effective knowledge retrieval from vast volumes of text in specialized areas such as medicine or law.',\n",
       "   'This includes sifting through documents, summarizing lengthy passages, and answering technical questions.',\n",
       "   'In short, LLMs are invaluable for automating almost any task that involves parsing and generating text.',\n",
       "   'Their applications are virtually endless, and as we continue to innovate and explore new ways to use these models, it’s clear that LLMs have the potential to redefine our relationship with technology, making it more conversational, intuitive, and accessible.',\n",
       "   'We will focus on understanding how LLMs work from the ground up, coding an LLM that can generate texts.',\n",
       "   'You will also learn about techniques that allow LLMs to carry out queries, ranging from answering questions'],\n",
       "  'sentences_count_spacy': 27,\n",
       "  'text_chunks': [['Since LLMs are capable of generating text, LLMs are also often referred to as a form of generative artificial intelligence, often abbreviated as generative AI or GenAI.',\n",
       "    'As illustrated in figure 1.1, AI encompasses the broader field of creating machines that can perform tasks requiring humanlike intelligence, including understanding language, recognizing patterns, and making decisions, and includes subfields like machine learning and deep learning.',\n",
       "    'The algorithms used to implement AI are the focus of the field of machine learning.',\n",
       "    'Specifically, machine learning involves the development of algorithms that can learn from and make predictions or decisions based on data without being explicitly programmed.',\n",
       "    'To illustrate this, imagine a spam filter as a practical application of machine learning.',\n",
       "    'Instead of manually writing rules to identify spam emails, a machine learning algorithm is fed examples of emails labeled as spam and legitimate emails.',\n",
       "    'By minimizing the error in its predictions on a training dataset, the model then learns to recognize patterns and characteristics indicative of spam, enabling it to classify new emails as either spam or not spam.',\n",
       "    'deep learning is a subset of machine learning that focuses on utilizing neural networks with three or more layers (also called deep neural networks) to model complex patterns and abstractions in data.',\n",
       "    'In contrast to deep learning, traditional machine learning requires manual feature extraction.',\n",
       "    'This means that human experts need to identify and select the most relevant features for the model.'],\n",
       "   ['While the field of AI is now dominated by machine learning and deep learning, it also includes other approaches—for example, using rule-based systems, genetic algorithms, expert systems, fuzzy logic, or symbolic reasoning.',\n",
       "    'Returning to the spam classification example, in traditional machine learning, human experts might manually extract features from email text such as the frequency of certain trigger words (for example, “prize,” “win,” “free”), the number of exclamation marks, use of all uppercase words, or the presence of suspicious links.',\n",
       "    'This dataset, created based on these expert- defined features, would then be used to train the model.',\n",
       "    'In contrast to traditional machine learning, deep learning does not require manual feature extraction.',\n",
       "    'This means that human experts do not need to identify and select the most relevant features for a deep learning model. (',\n",
       "    'However, both traditional machine learning and deep learning for spam classification still require the collection of labels, such as spam or non-spam, which need to be gathered either by an expert or users.)',\n",
       "    'Let’s look at some of the problems LLMs can solve today, the challenges that LLMs address, and the general LLM architecture we will implement later.',\n",
       "    'Applications of LLMs Owing to their advanced capabilities to parse and understand unstructured text data, LLMs have a broad range of applications across various domains.',\n",
       "    'Today, LLMs are employed for machine translation, generation of novel texts, sentiment analysis, text summarization, and many other tasks.',\n",
       "    'LLMs have recently been used for content creation, such as writing fiction, articles, and even computer code.'],\n",
       "   ['LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s ChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries and augment traditional search engines such as Google Search or Microsoft Bing.',\n",
       "    'Moreover, LLMs may be used for effective knowledge retrieval from vast volumes of text in specialized areas such as medicine or law.',\n",
       "    'This includes sifting through documents, summarizing lengthy passages, and answering technical questions.',\n",
       "    'In short, LLMs are invaluable for automating almost any task that involves parsing and generating text.',\n",
       "    'Their applications are virtually endless, and as we continue to innovate and explore new ways to use these models, it’s clear that LLMs have the potential to redefine our relationship with technology, making it more conversational, intuitive, and accessible.',\n",
       "    'We will focus on understanding how LLMs work from the ground up, coding an LLM that can generate texts.',\n",
       "    'You will also learn about techniques that allow LLMs to carry out queries, ranging from answering questions']],\n",
       "  'chunk_size': 3},\n",
       " {'Page No.': 2,\n",
       "  'page_char_count': 4204,\n",
       "  'page_word_count': 639,\n",
       "  'page_sentence_count (Not accurate)': 32,\n",
       "  'page_token_count': 1051.0,\n",
       "  'text': 'to summarizing text, translating text into different languages, and more. In other words, you will learn how complex LLM assistants such as ChatGPT work by building one step by step. Stages of building and using LLMs Why should we build our own LLMs? Coding an LLM from the ground up is an excellent exercise to understand its mechanics and limitations. Also, it equips us with the required knowledge for pretraining or fine-tuning existing open source LLM architectures to our own domain-specific datasets or tasks. Research has shown that when it comes to modeling performance, custom-built LLMs—those tailored for specific tasks or domains—can outperform general-purpose LLMs, such as those provided by ChatGPT, which are designed for a wide array of applications. Examples of these include BloombergGPT (specialized for finance) and LLMs tailored for medical question answering. Using custom-built LLMs offers several advantages, particularly regarding data privacy. For instance, companies may prefer not to share sensitive data with third-party LLM providers like OpenAI due to confidentiality concerns. Additionally, developing smaller custom LLMs enables deployment directly on customer devices, such as laptops and smartphones, which is something companies like Apple are currently exploring. This local implementation can significantly decrease latency and reduce server-related costs. Furthermore, custom LLMs grant developers complete autonomy, allowing them to control updates and modifications to the model as needed. The general process of creating an LLM includes pretraining and fine-tuning. The “pre” in “pretraining” refers to the initial phase where a model like an LLM is trained on a large, diverse dataset to develop a broad understanding of language. This pretrained model then serves as a foundational resource that can be further refined through fine-tuning, a process where the model is specifically trained on a narrower dataset that is more specific to particular tasks or domains. The first step in creating an LLM is to train it on a large corpus of text data, sometimes referred to as raw text. Here, “raw” refers to the fact that this data is just regular text without any labeling information. (Filtering may be applied, such as removing formatting characters or documents in unknown languages.) This first training stage of an LLM is also known as pretraining, creating an initial pretrained LLM, often called a base or foundation model. A typical example of such a model is the GPT-3 model (the precursor of the original model offered in ChatGPT). This model is capable of text completion—that is, finishing a half-written sentence provided by a user. It also has limited few-shot capabilities, which means it can learn to perform new tasks based on only a few examples instead of needing extensive training data. After obtaining a pretrained LLM from training on large text datasets, where the LLM is trained to predict the next word in the text, we can further train the LLM on labeled data, also known as fine-tuning. The two most popular categories of fine-tuning LLMs are instruction fine-tuning and classification fine-tuning. In instruction fine-tuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text. In classification fine-tuning, the labeled dataset consists of texts and associated class labels—for example, emails associated with “spam” and “not spam” labels. Introduction to Transformers Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper “Attention Is All You Need” (https://arxiv.org/abs/1706. 03762). To understand LLMs, we must understand the original transformer, which was developed for machine translation, translating English texts to German and French. The transformer architecture consists of two submodules: an encoder and a decoder. The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input. Then, the decoder module takes these encoded vectors and generates the',\n",
       "  'sentences': ['to summarizing text, translating text into different languages, and more.',\n",
       "   'In other words, you will learn how complex LLM assistants such as ChatGPT work by building one step by step.',\n",
       "   'Stages of building and using LLMs Why should we build our own LLMs?',\n",
       "   'Coding an LLM from the ground up is an excellent exercise to understand its mechanics and limitations.',\n",
       "   'Also, it equips us with the required knowledge for pretraining or fine-tuning existing open source LLM architectures to our own domain-specific datasets or tasks.',\n",
       "   'Research has shown that when it comes to modeling performance, custom-built LLMs—those tailored for specific tasks or domains—can outperform general-purpose LLMs, such as those provided by ChatGPT, which are designed for a wide array of applications.',\n",
       "   'Examples of these include BloombergGPT (specialized for finance) and LLMs tailored for medical question answering.',\n",
       "   'Using custom-built LLMs offers several advantages, particularly regarding data privacy.',\n",
       "   'For instance, companies may prefer not to share sensitive data with third-party LLM providers like OpenAI due to confidentiality concerns.',\n",
       "   'Additionally, developing smaller custom LLMs enables deployment directly on customer devices, such as laptops and smartphones, which is something companies like Apple are currently exploring.',\n",
       "   'This local implementation can significantly decrease latency and reduce server-related costs.',\n",
       "   'Furthermore, custom LLMs grant developers complete autonomy, allowing them to control updates and modifications to the model as needed.',\n",
       "   'The general process of creating an LLM includes pretraining and fine-tuning.',\n",
       "   'The “pre” in “pretraining” refers to the initial phase where a model like an LLM is trained on a large, diverse dataset to develop a broad understanding of language.',\n",
       "   'This pretrained model then serves as a foundational resource that can be further refined through fine-tuning, a process where the model is specifically trained on a narrower dataset that is more specific to particular tasks or domains.',\n",
       "   'The first step in creating an LLM is to train it on a large corpus of text data, sometimes referred to as raw text.',\n",
       "   'Here, “raw” refers to the fact that this data is just regular text without any labeling information. (',\n",
       "   'Filtering may be applied, such as removing formatting characters or documents in unknown languages.)',\n",
       "   'This first training stage of an LLM is also known as pretraining, creating an initial pretrained LLM, often called a base or foundation model.',\n",
       "   'A typical example of such a model is the GPT-3 model (the precursor of the original model offered in ChatGPT).',\n",
       "   'This model is capable of text completion—that is, finishing a half-written sentence provided by a user.',\n",
       "   'It also has limited few-shot capabilities, which means it can learn to perform new tasks based on only a few examples instead of needing extensive training data.',\n",
       "   'After obtaining a pretrained LLM from training on large text datasets, where the LLM is trained to predict the next word in the text, we can further train the LLM on labeled data, also known as fine-tuning.',\n",
       "   'The two most popular categories of fine-tuning LLMs are instruction fine-tuning and classification fine-tuning.',\n",
       "   'In instruction fine-tuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text.',\n",
       "   'In classification fine-tuning, the labeled dataset consists of texts and associated class labels—for example, emails associated with “spam” and “not spam” labels.',\n",
       "   'Introduction to Transformers Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper “Attention Is All You Need” (https://arxiv.org/abs/1706.',\n",
       "   '03762).',\n",
       "   'To understand LLMs, we must understand the original transformer, which was developed for machine translation, translating English texts to German and French.',\n",
       "   'The transformer architecture consists of two submodules: an encoder and a decoder.',\n",
       "   'The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input.',\n",
       "   'Then, the decoder module takes these encoded vectors and generates the'],\n",
       "  'sentences_count_spacy': 32,\n",
       "  'text_chunks': [['to summarizing text, translating text into different languages, and more.',\n",
       "    'In other words, you will learn how complex LLM assistants such as ChatGPT work by building one step by step.',\n",
       "    'Stages of building and using LLMs Why should we build our own LLMs?',\n",
       "    'Coding an LLM from the ground up is an excellent exercise to understand its mechanics and limitations.',\n",
       "    'Also, it equips us with the required knowledge for pretraining or fine-tuning existing open source LLM architectures to our own domain-specific datasets or tasks.',\n",
       "    'Research has shown that when it comes to modeling performance, custom-built LLMs—those tailored for specific tasks or domains—can outperform general-purpose LLMs, such as those provided by ChatGPT, which are designed for a wide array of applications.',\n",
       "    'Examples of these include BloombergGPT (specialized for finance) and LLMs tailored for medical question answering.',\n",
       "    'Using custom-built LLMs offers several advantages, particularly regarding data privacy.',\n",
       "    'For instance, companies may prefer not to share sensitive data with third-party LLM providers like OpenAI due to confidentiality concerns.',\n",
       "    'Additionally, developing smaller custom LLMs enables deployment directly on customer devices, such as laptops and smartphones, which is something companies like Apple are currently exploring.'],\n",
       "   ['This local implementation can significantly decrease latency and reduce server-related costs.',\n",
       "    'Furthermore, custom LLMs grant developers complete autonomy, allowing them to control updates and modifications to the model as needed.',\n",
       "    'The general process of creating an LLM includes pretraining and fine-tuning.',\n",
       "    'The “pre” in “pretraining” refers to the initial phase where a model like an LLM is trained on a large, diverse dataset to develop a broad understanding of language.',\n",
       "    'This pretrained model then serves as a foundational resource that can be further refined through fine-tuning, a process where the model is specifically trained on a narrower dataset that is more specific to particular tasks or domains.',\n",
       "    'The first step in creating an LLM is to train it on a large corpus of text data, sometimes referred to as raw text.',\n",
       "    'Here, “raw” refers to the fact that this data is just regular text without any labeling information. (',\n",
       "    'Filtering may be applied, such as removing formatting characters or documents in unknown languages.)',\n",
       "    'This first training stage of an LLM is also known as pretraining, creating an initial pretrained LLM, often called a base or foundation model.',\n",
       "    'A typical example of such a model is the GPT-3 model (the precursor of the original model offered in ChatGPT).'],\n",
       "   ['This model is capable of text completion—that is, finishing a half-written sentence provided by a user.',\n",
       "    'It also has limited few-shot capabilities, which means it can learn to perform new tasks based on only a few examples instead of needing extensive training data.',\n",
       "    'After obtaining a pretrained LLM from training on large text datasets, where the LLM is trained to predict the next word in the text, we can further train the LLM on labeled data, also known as fine-tuning.',\n",
       "    'The two most popular categories of fine-tuning LLMs are instruction fine-tuning and classification fine-tuning.',\n",
       "    'In instruction fine-tuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text.',\n",
       "    'In classification fine-tuning, the labeled dataset consists of texts and associated class labels—for example, emails associated with “spam” and “not spam” labels.',\n",
       "    'Introduction to Transformers Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper “Attention Is All You Need” (https://arxiv.org/abs/1706.',\n",
       "    '03762).',\n",
       "    'To understand LLMs, we must understand the original transformer, which was developed for machine translation, translating English texts to German and French.',\n",
       "    'The transformer architecture consists of two submodules: an encoder and a decoder.'],\n",
       "   ['The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input.',\n",
       "    'Then, the decoder module takes these encoded vectors and generates the']],\n",
       "  'chunk_size': 4},\n",
       " {'Page No.': 3,\n",
       "  'page_char_count': 3729,\n",
       "  'page_word_count': 561,\n",
       "  'page_sentence_count (Not accurate)': 30,\n",
       "  'page_token_count': 932.25,\n",
       "  'text': 'output text. In a translation task, for example, the encoder would encode the text from the source language into vectors, and the decoder would decode these vectors to generate text in the target language. Both the encoder and decoder consist of many layers connected by a so-called self- attention mechanism. You may have many questions regarding how the inputs are preprocessed and encoded. These will be addressed in a step-by-step implementation in subsequent chapters. A key component of transformers and LLMs is the selfattention mechanism (not shown), which allows the model to weigh the importance of different words or tokens in a sequence relative to each other. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output. Later variants of the transformer architecture, such as BERT (short for bidirectional encoder representations from transformers) and the various GPT models (short for generative pretrained transformers), built on this concept to adapt this architecture for different tasks. BERT, which is built upon the original transformer’s encoder submodule, differs in its training approach from GPT. While GPT is designed for generative tasks, BERT and its variants specialize in masked word prediction, where the model predicts masked or hidden words in a given sentence. This unique training strategy equips BERT with strengths in text classification tasks, including sentiment prediction and document categorization. As an application of its capabilities, as of this writing, X (formerly Twitter) uses BERT to detect toxic content. GPT, on the other hand, focuses on the decoder portion of the original transformer architecture and is designed for tasks that require generating texts. This includes machine translation, text summarization, fiction writing, writing computer code, and more. GPT models, primarily designed and trained to perform text completion tasks, also show remarkable versatility in their capabilities. These models are adept at executing both zeroshot and few-shot learning tasks. Zero-shot learning refers to the ability to generalize to completely unseen tasks without any prior specific examples. On the other hand, fewshot learning involves learning from a minimal number of examples the user provides as input. Transformers vs. LLM Today’s LLMs are based on the transformer architecture. Hence, transformers and LLMs are terms that are often used synonymously in the literature. However, note that not all transformers are LLMs since transformers can also be used for computer vision. Also, not all LLMs are transformers, as there are LLMs based on recurrent and convolutional architectures. The main motivation behind these alternative approaches is to improve the computational efficiency of LLMs. Whether these alternative LLM architectures can compete with the capabilities of transformer-based LLMs and whether they are going to be adopted in practice remains to be seen. For simplicity, I use the term “LLM” to refer to transformer-based LLMs similar to GPT. Utilizing large datasets The large training datasets for popular GPT- and BERT-like models represent diverse and comprehensive text corpora encompassing billions of words, which include a vast array of topics and natural and computer languages. To provide a concrete example, table 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base model for the first version of ChatGPT. Dataset Name Dataset Description Number of tokens Proportion in training data Common Crawl    (filtered) Web crawl data 410 billion 60% WebText2 Web crawl data 19 billion 22%',\n",
       "  'sentences': ['output text.',\n",
       "   'In a translation task, for example, the encoder would encode the text from the source language into vectors, and the decoder would decode these vectors to generate text in the target language.',\n",
       "   'Both the encoder and decoder consist of many layers connected by a so-called self- attention mechanism.',\n",
       "   'You may have many questions regarding how the inputs are preprocessed and encoded.',\n",
       "   'These will be addressed in a step-by-step implementation in subsequent chapters.',\n",
       "   'A key component of transformers and LLMs is the selfattention mechanism (not shown), which allows the model to weigh the importance of different words or tokens in a sequence relative to each other.',\n",
       "   'This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output.',\n",
       "   'Later variants of the transformer architecture, such as BERT (short for bidirectional encoder representations from transformers) and the various GPT models (short for generative pretrained transformers), built on this concept to adapt this architecture for different tasks.',\n",
       "   'BERT, which is built upon the original transformer’s encoder submodule, differs in its training approach from GPT.',\n",
       "   'While GPT is designed for generative tasks, BERT and its variants specialize in masked word prediction, where the model predicts masked or hidden words in a given sentence.',\n",
       "   'This unique training strategy equips BERT with strengths in text classification tasks, including sentiment prediction and document categorization.',\n",
       "   'As an application of its capabilities, as of this writing, X (formerly Twitter) uses BERT to detect toxic content.',\n",
       "   'GPT, on the other hand, focuses on the decoder portion of the original transformer architecture and is designed for tasks that require generating texts.',\n",
       "   'This includes machine translation, text summarization, fiction writing, writing computer code, and more.',\n",
       "   'GPT models, primarily designed and trained to perform text completion tasks, also show remarkable versatility in their capabilities.',\n",
       "   'These models are adept at executing both zeroshot and few-shot learning tasks.',\n",
       "   'Zero-shot learning refers to the ability to generalize to completely unseen tasks without any prior specific examples.',\n",
       "   'On the other hand, fewshot learning involves learning from a minimal number of examples the user provides as input.',\n",
       "   'Transformers vs. LLM Today’s LLMs are based on the transformer architecture.',\n",
       "   'Hence, transformers and LLMs are terms that are often used synonymously in the literature.',\n",
       "   'However, note that not all transformers are LLMs since transformers can also be used for computer vision.',\n",
       "   'Also, not all LLMs are transformers, as there are LLMs based on recurrent and convolutional architectures.',\n",
       "   'The main motivation behind these alternative approaches is to improve the computational efficiency of LLMs.',\n",
       "   'Whether these alternative LLM architectures can compete with the capabilities of transformer-based LLMs and whether they are going to be adopted in practice remains to be seen.',\n",
       "   'For simplicity, I use the term “LLM” to refer to transformer-based LLMs similar to GPT.',\n",
       "   'Utilizing large datasets The large training datasets for popular GPT- and BERT-like models represent diverse and comprehensive text corpora encompassing billions of words, which include a vast array of topics and natural and computer languages.',\n",
       "   'To provide a concrete example, table 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base model for the first version of ChatGPT.',\n",
       "   'Dataset Name Dataset Description Number of tokens Proportion in training data Common Crawl    (filtered) Web crawl data 410 billion 60% WebText2 Web crawl data 19 billion 22%'],\n",
       "  'sentences_count_spacy': 28,\n",
       "  'text_chunks': [['output text.',\n",
       "    'In a translation task, for example, the encoder would encode the text from the source language into vectors, and the decoder would decode these vectors to generate text in the target language.',\n",
       "    'Both the encoder and decoder consist of many layers connected by a so-called self- attention mechanism.',\n",
       "    'You may have many questions regarding how the inputs are preprocessed and encoded.',\n",
       "    'These will be addressed in a step-by-step implementation in subsequent chapters.',\n",
       "    'A key component of transformers and LLMs is the selfattention mechanism (not shown), which allows the model to weigh the importance of different words or tokens in a sequence relative to each other.',\n",
       "    'This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output.',\n",
       "    'Later variants of the transformer architecture, such as BERT (short for bidirectional encoder representations from transformers) and the various GPT models (short for generative pretrained transformers), built on this concept to adapt this architecture for different tasks.',\n",
       "    'BERT, which is built upon the original transformer’s encoder submodule, differs in its training approach from GPT.',\n",
       "    'While GPT is designed for generative tasks, BERT and its variants specialize in masked word prediction, where the model predicts masked or hidden words in a given sentence.'],\n",
       "   ['This unique training strategy equips BERT with strengths in text classification tasks, including sentiment prediction and document categorization.',\n",
       "    'As an application of its capabilities, as of this writing, X (formerly Twitter) uses BERT to detect toxic content.',\n",
       "    'GPT, on the other hand, focuses on the decoder portion of the original transformer architecture and is designed for tasks that require generating texts.',\n",
       "    'This includes machine translation, text summarization, fiction writing, writing computer code, and more.',\n",
       "    'GPT models, primarily designed and trained to perform text completion tasks, also show remarkable versatility in their capabilities.',\n",
       "    'These models are adept at executing both zeroshot and few-shot learning tasks.',\n",
       "    'Zero-shot learning refers to the ability to generalize to completely unseen tasks without any prior specific examples.',\n",
       "    'On the other hand, fewshot learning involves learning from a minimal number of examples the user provides as input.',\n",
       "    'Transformers vs. LLM Today’s LLMs are based on the transformer architecture.',\n",
       "    'Hence, transformers and LLMs are terms that are often used synonymously in the literature.'],\n",
       "   ['However, note that not all transformers are LLMs since transformers can also be used for computer vision.',\n",
       "    'Also, not all LLMs are transformers, as there are LLMs based on recurrent and convolutional architectures.',\n",
       "    'The main motivation behind these alternative approaches is to improve the computational efficiency of LLMs.',\n",
       "    'Whether these alternative LLM architectures can compete with the capabilities of transformer-based LLMs and whether they are going to be adopted in practice remains to be seen.',\n",
       "    'For simplicity, I use the term “LLM” to refer to transformer-based LLMs similar to GPT.',\n",
       "    'Utilizing large datasets The large training datasets for popular GPT- and BERT-like models represent diverse and comprehensive text corpora encompassing billions of words, which include a vast array of topics and natural and computer languages.',\n",
       "    'To provide a concrete example, table 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base model for the first version of ChatGPT.',\n",
       "    'Dataset Name Dataset Description Number of tokens Proportion in training data Common Crawl    (filtered) Web crawl data 410 billion 60% WebText2 Web crawl data 19 billion 22%']],\n",
       "  'chunk_size': 3},\n",
       " {'Page No.': 4,\n",
       "  'page_char_count': 3720,\n",
       "  'page_word_count': 582,\n",
       "  'page_sentence_count (Not accurate)': 33,\n",
       "  'page_token_count': 930.0,\n",
       "  'text': 'Dataset Name Dataset Description Number of tokens Proportion in training data Books1 Internet-based book corpus 12 billion 8% Books2 Internet-based book corpus 55 billion 8% Wikipedia High-quality text 3 billion 3% The table above, reports the number of tokens, where a token is a unit of text that a model reads and the number of tokens in a dataset is roughly equivalent to the number of words and punctuation characters in the text. The main takeaway is that the scale and diversity of this training dataset allow these models to perform well on diverse tasks, including language syntax, semantics, and context—even some requiring general knowledge. The pretrained nature of these models makes them incredibly versatile for further fine-tuning on downstream tasks, which is why they are also known as base or foundation models. Pretraining LLMs requires access to significant resources and is very expensive. For example, the GPT-3 pretraining cost is estimated to be 4.6 million USD in terms of cloud computing credits (https://mng.bz/VxEW). The good news is that many pretrained LLMs, available as open source models, can be used as general-purpose tools to write, extract, and edit texts that were not part of the training data. Also, LLMs can be fine-tuned on specific tasks with relatively smaller datasets, reducing the computational resources needed and improving performance. We will implement the code for pretraining and use it to pretrain an LLM for educational purposes. All computations are executable on consumer hardware. After implementing the pretraining code, we will learn how to reuse openly available model weights and load them into the architecture we will implement, allowing us to skip the expensive pretraining stage when we fine-tune our LLM. GPT-3 Dataset Details The Table above displays the dataset used for GPT-3. The proportions column in the table sums up to 100% of the sampled data, adjusted for rounding errors. Although the subsets in the Number of Tokens column total 499 billion, the model was trained on only 300 billion tokens. The authors of the GPT-3 paper did not specify why the model was not trained on all 499 billion tokens. For context, consider the size of the CommonCrawl dataset, which alone consists of 410 billion tokens and requires about 570 GB of storage. In comparison, later iterations of models like GPT-3, such as Meta’s LLaMA, have expanded their training scope to include additional data sources like Arxiv research papers (92 GB) and StackExchange’s code-related Q&As (78 GB). The authors of the GPT-3 paper did not share the training dataset, but a comparable dataset that is publicly available is Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research by Soldaini et al. 2024 (https://arxiv.org/abs/2402.00159). However, the collection may contain copyrighted works, and the exact usage terms may depend on the intended use case and country. A closer look at the GPT architecture GPT was originally introduced in the paper “Improving Language Understanding by Generative Pre- Training” (https://mng.bz/x2qg) by Radford et al. from OpenAI. GPT-3 is a scaled-up version of this model that has more parameters and was trained on a larger dataset. In addition, the original model offered in ChatGPT was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI’s InstructGPT paper (https://arxiv.org/abs/2203.02155). As figure 1.6 shows, these models are competent text completion models and can carry out other tasks such as spelling correction, classification, or language translation. This is actually very remarkable given that GPT models are pretrained on a relatively simple next-word prediction task',\n",
       "  'sentences': ['Dataset Name Dataset Description Number of tokens Proportion in training data Books1 Internet-based book corpus 12 billion 8% Books2 Internet-based book corpus 55 billion 8% Wikipedia High-quality text 3 billion 3% The table above, reports the number of tokens, where a token is a unit of text that a model reads and the number of tokens in a dataset is roughly equivalent to the number of words and punctuation characters in the text.',\n",
       "   'The main takeaway is that the scale and diversity of this training dataset allow these models to perform well on diverse tasks, including language syntax, semantics, and context—even some requiring general knowledge.',\n",
       "   'The pretrained nature of these models makes them incredibly versatile for further fine-tuning on downstream tasks, which is why they are also known as base or foundation models.',\n",
       "   'Pretraining LLMs requires access to significant resources and is very expensive.',\n",
       "   'For example, the GPT-3 pretraining cost is estimated to be 4.6 million USD in terms of cloud computing credits (https://mng.bz/VxEW).',\n",
       "   'The good news is that many pretrained LLMs, available as open source models, can be used as general-purpose tools to write, extract, and edit texts that were not part of the training data.',\n",
       "   'Also, LLMs can be fine-tuned on specific tasks with relatively smaller datasets, reducing the computational resources needed and improving performance.',\n",
       "   'We will implement the code for pretraining and use it to pretrain an LLM for educational purposes.',\n",
       "   'All computations are executable on consumer hardware.',\n",
       "   'After implementing the pretraining code, we will learn how to reuse openly available model weights and load them into the architecture we will implement, allowing us to skip the expensive pretraining stage when we fine-tune our LLM.',\n",
       "   'GPT-3 Dataset Details The Table above displays the dataset used for GPT-3.',\n",
       "   'The proportions column in the table sums up to 100% of the sampled data, adjusted for rounding errors.',\n",
       "   'Although the subsets in the Number of Tokens column total 499 billion, the model was trained on only 300 billion tokens.',\n",
       "   'The authors of the GPT-3 paper did not specify why the model was not trained on all 499 billion tokens.',\n",
       "   'For context, consider the size of the CommonCrawl dataset, which alone consists of 410 billion tokens and requires about 570 GB of storage.',\n",
       "   'In comparison, later iterations of models like GPT-3, such as Meta’s LLaMA, have expanded their training scope to include additional data sources like Arxiv research papers (92 GB) and StackExchange’s code-related Q&As (78 GB).',\n",
       "   'The authors of the GPT-3 paper did not share the training dataset, but a comparable dataset that is publicly available is Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research by Soldaini et al.',\n",
       "   '2024 (https://arxiv.org/abs/2402.00159).',\n",
       "   'However, the collection may contain copyrighted works, and the exact usage terms may depend on the intended use case and country.',\n",
       "   'A closer look at the GPT architecture GPT was originally introduced in the paper “Improving Language Understanding by Generative Pre- Training” (https://mng.bz/x2qg) by Radford et al.',\n",
       "   'from OpenAI.',\n",
       "   'GPT-3 is a scaled-up version of this model that has more parameters and was trained on a larger dataset.',\n",
       "   'In addition, the original model offered in ChatGPT was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI’s InstructGPT paper (https://arxiv.org/abs/2203.02155).',\n",
       "   'As figure 1.6 shows, these models are competent text completion models and can carry out other tasks such as spelling correction, classification, or language translation.',\n",
       "   'This is actually very remarkable given that GPT models are pretrained on a relatively simple next-word prediction task'],\n",
       "  'sentences_count_spacy': 25,\n",
       "  'text_chunks': [['Dataset Name Dataset Description Number of tokens Proportion in training data Books1 Internet-based book corpus 12 billion 8% Books2 Internet-based book corpus 55 billion 8% Wikipedia High-quality text 3 billion 3% The table above, reports the number of tokens, where a token is a unit of text that a model reads and the number of tokens in a dataset is roughly equivalent to the number of words and punctuation characters in the text.',\n",
       "    'The main takeaway is that the scale and diversity of this training dataset allow these models to perform well on diverse tasks, including language syntax, semantics, and context—even some requiring general knowledge.',\n",
       "    'The pretrained nature of these models makes them incredibly versatile for further fine-tuning on downstream tasks, which is why they are also known as base or foundation models.',\n",
       "    'Pretraining LLMs requires access to significant resources and is very expensive.',\n",
       "    'For example, the GPT-3 pretraining cost is estimated to be 4.6 million USD in terms of cloud computing credits (https://mng.bz/VxEW).',\n",
       "    'The good news is that many pretrained LLMs, available as open source models, can be used as general-purpose tools to write, extract, and edit texts that were not part of the training data.',\n",
       "    'Also, LLMs can be fine-tuned on specific tasks with relatively smaller datasets, reducing the computational resources needed and improving performance.',\n",
       "    'We will implement the code for pretraining and use it to pretrain an LLM for educational purposes.',\n",
       "    'All computations are executable on consumer hardware.',\n",
       "    'After implementing the pretraining code, we will learn how to reuse openly available model weights and load them into the architecture we will implement, allowing us to skip the expensive pretraining stage when we fine-tune our LLM.'],\n",
       "   ['GPT-3 Dataset Details The Table above displays the dataset used for GPT-3.',\n",
       "    'The proportions column in the table sums up to 100% of the sampled data, adjusted for rounding errors.',\n",
       "    'Although the subsets in the Number of Tokens column total 499 billion, the model was trained on only 300 billion tokens.',\n",
       "    'The authors of the GPT-3 paper did not specify why the model was not trained on all 499 billion tokens.',\n",
       "    'For context, consider the size of the CommonCrawl dataset, which alone consists of 410 billion tokens and requires about 570 GB of storage.',\n",
       "    'In comparison, later iterations of models like GPT-3, such as Meta’s LLaMA, have expanded their training scope to include additional data sources like Arxiv research papers (92 GB) and StackExchange’s code-related Q&As (78 GB).',\n",
       "    'The authors of the GPT-3 paper did not share the training dataset, but a comparable dataset that is publicly available is Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research by Soldaini et al.',\n",
       "    '2024 (https://arxiv.org/abs/2402.00159).',\n",
       "    'However, the collection may contain copyrighted works, and the exact usage terms may depend on the intended use case and country.',\n",
       "    'A closer look at the GPT architecture GPT was originally introduced in the paper “Improving Language Understanding by Generative Pre- Training” (https://mng.bz/x2qg) by Radford et al.'],\n",
       "   ['from OpenAI.',\n",
       "    'GPT-3 is a scaled-up version of this model that has more parameters and was trained on a larger dataset.',\n",
       "    'In addition, the original model offered in ChatGPT was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI’s InstructGPT paper (https://arxiv.org/abs/2203.02155).',\n",
       "    'As figure 1.6 shows, these models are competent text completion models and can carry out other tasks such as spelling correction, classification, or language translation.',\n",
       "    'This is actually very remarkable given that GPT models are pretrained on a relatively simple next-word prediction task']],\n",
       "  'chunk_size': 3},\n",
       " {'Page No.': 5,\n",
       "  'page_char_count': 4350,\n",
       "  'page_word_count': 655,\n",
       "  'page_sentence_count (Not accurate)': 34,\n",
       "  'page_token_count': 1087.5,\n",
       "  'text': 'The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling. This means that we don’t need to collect labels for the training data explicitly but can use the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict. Since this next-word prediction task allows us to create labels “on the fly,” it is possible to use massive unlabeled text datasets to train LLMs. Compared to the original transformer architecture, the general GPT architecture is relatively simple. Essentially, it’s just the decoder part without the encoder. Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of autoregressive model. Autoregressive models incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves the coherence of the resulting text. Architectures such as GPT-3 are also significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion parameters in total. GPT-3 was introduced in 2020, which, by the standards of deep learning and large language model development, is considered a long time ago. However, more recent architectures, such as Meta’s Llama models, are still based on the same underlying concepts, introducing only minor modifications. Hence, understanding GPT remains as relevant as ever, so I focus on implementing the prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs. Although the original transformer model, consisting of encoder and decoder blocks, was explicitly designed for language translation, GPT models—despite their larger yet simpler decoder-only architecture aimed at next-word prediction—are also capable of performing translation tasks. This capability was initially unexpected to researchers, as it emerged from a model primarily trained on a next-word prediction task, which is a task that did not specifically target translation. The ability to perform tasks that the model wasn’t explicitly trained to perform is called an emergent behavior. This capability isn’t explicitly taught during training but emerges as a natural consequence of the model’s exposure to vast quantities of multilingual data in diverse contexts. The fact that GPT models can “learn” the translation patterns between languages and perform translation tasks even though they weren’t specifically trained for it demonstrates the benefits and capabilities of these large-scale, generative language models. We can perform diverse tasks without using diverse models for each. Working with text data During the pretraining stage, LLMs process text one word at a time. Training LLMs with millions to billions of parameters using a next-word prediction task yields models with impressive capabilities. These models can then be further finetuned to follow general instructions or perform specific target tasks. But before we can implement and train LLMs, we need to prepare the training dataset. This involves splitting text into individual word and subword tokens, which can then be encoded into vector representations for the LLM. You’ll also learn about advanced tokenization schemes like byte pair encoding, which is utilized in popular LLMs like GPT. Lastly, we’ll implement a sampling and data-loading strategy to produce the input-output pairs necessary for training LLMs. Understanding word embeddings Deep neural network models, including LLMs, cannot process raw text directly. Since text is categorical, it isn’t compatible with the mathematical operations used to implement and train neural networks. Therefore, we need a way to represent words as continuous-valued vectors. The concept of converting data into a vector format is often referred to as embedding. Using a specific neural network layer or another pretrained neural network model, we can embed different data types—for example, video, audio, and text. However, it’s important to note that different data formats require distinct embedding models. For example, an embedding model designed for text would not be',\n",
       "  'sentences': ['The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling.',\n",
       "   'This means that we don’t need to collect labels for the training data explicitly but can use the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict.',\n",
       "   'Since this next-word prediction task allows us to create labels “on the fly,” it is possible to use massive unlabeled text datasets to train LLMs.',\n",
       "   'Compared to the original transformer architecture, the general GPT architecture is relatively simple.',\n",
       "   'Essentially, it’s just the decoder part without the encoder.',\n",
       "   'Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of autoregressive model.',\n",
       "   'Autoregressive models incorporate their previous outputs as inputs for future predictions.',\n",
       "   'Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves the coherence of the resulting text.',\n",
       "   'Architectures such as GPT-3 are also significantly larger than the original transformer model.',\n",
       "   'For instance, the original transformer repeated the encoder and decoder blocks six times.',\n",
       "   'GPT-3 has 96 transformer layers and 175 billion parameters in total.',\n",
       "   'GPT-3 was introduced in 2020, which, by the standards of deep learning and large language model development, is considered a long time ago.',\n",
       "   'However, more recent architectures, such as Meta’s Llama models, are still based on the same underlying concepts, introducing only minor modifications.',\n",
       "   'Hence, understanding GPT remains as relevant as ever, so I focus on implementing the prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs.',\n",
       "   'Although the original transformer model, consisting of encoder and decoder blocks, was explicitly designed for language translation, GPT models—despite their larger yet simpler decoder-only architecture aimed at next-word prediction—are also capable of performing translation tasks.',\n",
       "   'This capability was initially unexpected to researchers, as it emerged from a model primarily trained on a next-word prediction task, which is a task that did not specifically target translation.',\n",
       "   'The ability to perform tasks that the model wasn’t explicitly trained to perform is called an emergent behavior.',\n",
       "   'This capability isn’t explicitly taught during training but emerges as a natural consequence of the model’s exposure to vast quantities of multilingual data in diverse contexts.',\n",
       "   'The fact that GPT models can “learn” the translation patterns between languages and perform translation tasks even though they weren’t specifically trained for it demonstrates the benefits and capabilities of these large-scale, generative language models.',\n",
       "   'We can perform diverse tasks without using diverse models for each.',\n",
       "   'Working with text data During the pretraining stage, LLMs process text one word at a time.',\n",
       "   'Training LLMs with millions to billions of parameters using a next-word prediction task yields models with impressive capabilities.',\n",
       "   'These models can then be further finetuned to follow general instructions or perform specific target tasks.',\n",
       "   'But before we can implement and train LLMs, we need to prepare the training dataset.',\n",
       "   'This involves splitting text into individual word and subword tokens, which can then be encoded into vector representations for the LLM.',\n",
       "   'You’ll also learn about advanced tokenization schemes like byte pair encoding, which is utilized in popular LLMs like GPT.',\n",
       "   'Lastly, we’ll implement a sampling and data-loading strategy to produce the input-output pairs necessary for training LLMs.',\n",
       "   'Understanding word embeddings Deep neural network models, including LLMs, cannot process raw text directly.',\n",
       "   'Since text is categorical, it isn’t compatible with the mathematical operations used to implement and train neural networks.',\n",
       "   'Therefore, we need a way to represent words as continuous-valued vectors.',\n",
       "   'The concept of converting data into a vector format is often referred to as embedding.',\n",
       "   'Using a specific neural network layer or another pretrained neural network model, we can embed different data types—for example, video, audio, and text.',\n",
       "   'However, it’s important to note that different data formats require distinct embedding models.',\n",
       "   'For example, an embedding model designed for text would not be'],\n",
       "  'sentences_count_spacy': 34,\n",
       "  'text_chunks': [['The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling.',\n",
       "    'This means that we don’t need to collect labels for the training data explicitly but can use the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict.',\n",
       "    'Since this next-word prediction task allows us to create labels “on the fly,” it is possible to use massive unlabeled text datasets to train LLMs.',\n",
       "    'Compared to the original transformer architecture, the general GPT architecture is relatively simple.',\n",
       "    'Essentially, it’s just the decoder part without the encoder.',\n",
       "    'Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of autoregressive model.',\n",
       "    'Autoregressive models incorporate their previous outputs as inputs for future predictions.',\n",
       "    'Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves the coherence of the resulting text.',\n",
       "    'Architectures such as GPT-3 are also significantly larger than the original transformer model.',\n",
       "    'For instance, the original transformer repeated the encoder and decoder blocks six times.'],\n",
       "   ['GPT-3 has 96 transformer layers and 175 billion parameters in total.',\n",
       "    'GPT-3 was introduced in 2020, which, by the standards of deep learning and large language model development, is considered a long time ago.',\n",
       "    'However, more recent architectures, such as Meta’s Llama models, are still based on the same underlying concepts, introducing only minor modifications.',\n",
       "    'Hence, understanding GPT remains as relevant as ever, so I focus on implementing the prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs.',\n",
       "    'Although the original transformer model, consisting of encoder and decoder blocks, was explicitly designed for language translation, GPT models—despite their larger yet simpler decoder-only architecture aimed at next-word prediction—are also capable of performing translation tasks.',\n",
       "    'This capability was initially unexpected to researchers, as it emerged from a model primarily trained on a next-word prediction task, which is a task that did not specifically target translation.',\n",
       "    'The ability to perform tasks that the model wasn’t explicitly trained to perform is called an emergent behavior.',\n",
       "    'This capability isn’t explicitly taught during training but emerges as a natural consequence of the model’s exposure to vast quantities of multilingual data in diverse contexts.',\n",
       "    'The fact that GPT models can “learn” the translation patterns between languages and perform translation tasks even though they weren’t specifically trained for it demonstrates the benefits and capabilities of these large-scale, generative language models.',\n",
       "    'We can perform diverse tasks without using diverse models for each.'],\n",
       "   ['Working with text data During the pretraining stage, LLMs process text one word at a time.',\n",
       "    'Training LLMs with millions to billions of parameters using a next-word prediction task yields models with impressive capabilities.',\n",
       "    'These models can then be further finetuned to follow general instructions or perform specific target tasks.',\n",
       "    'But before we can implement and train LLMs, we need to prepare the training dataset.',\n",
       "    'This involves splitting text into individual word and subword tokens, which can then be encoded into vector representations for the LLM.',\n",
       "    'You’ll also learn about advanced tokenization schemes like byte pair encoding, which is utilized in popular LLMs like GPT.',\n",
       "    'Lastly, we’ll implement a sampling and data-loading strategy to produce the input-output pairs necessary for training LLMs.',\n",
       "    'Understanding word embeddings Deep neural network models, including LLMs, cannot process raw text directly.',\n",
       "    'Since text is categorical, it isn’t compatible with the mathematical operations used to implement and train neural networks.',\n",
       "    'Therefore, we need a way to represent words as continuous-valued vectors.'],\n",
       "   ['The concept of converting data into a vector format is often referred to as embedding.',\n",
       "    'Using a specific neural network layer or another pretrained neural network model, we can embed different data types—for example, video, audio, and text.',\n",
       "    'However, it’s important to note that different data formats require distinct embedding models.',\n",
       "    'For example, an embedding model designed for text would not be']],\n",
       "  'chunk_size': 4},\n",
       " {'Page No.': 6,\n",
       "  'page_char_count': 4223,\n",
       "  'page_word_count': 653,\n",
       "  'page_sentence_count (Not accurate)': 37,\n",
       "  'page_token_count': 1055.75,\n",
       "  'text': 'suitable for embedding audio or video data. At its core, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space— the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process. While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for retrieval-augmented generation. Retrieval-augmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text. Since our goal is to train GPT- like LLMs, which learn to generate text one word at a time, we will focus on word embeddings. Several algorithms and frameworks have been developed to generate word embeddings. One of the earlier and most popular examples is the Word2Vec approach. Word2Vec trained neural network architecture to generate word embeddings by predicting the context of a word given the target word or vice versa. The main idea behind Word2Vec is that words that appear in similar contexts tend to have similar meanings. Consequently, when projected into twodimensional word embeddings for visualization purposes, similar terms are clustered together. Word embeddings can have varying dimensions, from one to thousands. A higher dimensionality might capture more nuanced relationships but at the cost of computational efficiency. While we can use pretrained models such as Word2Vec to generate embeddings for machine learning models, LLMs commonly produce their own embeddings that are part of the input layer and are updated during training. The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand. Unfortunately, high-dimensional embeddings present a challenge for visualization because our sensory perception and common graphical representations are inherently limited to three dimensions or fewer, which is why figure 2.3 shows two-dimensional embeddings in a two-dimensional scatterplot. However, when working with LLMs, we typically use embeddings with a much higher dimensionality. For both GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality of the model’s hidden states) varies based on the specific model variant and size. It is a tradeoff between performance and efficiency. The smallest GPT-2 models (117M and 125M parameters) use an embedding size of 768 dimensions to provide concrete examples. The largest GPT-3 model (175B parameters) uses an embedding size of 12,288 dimensions. Tokenizing text Tokenization is a fundamental step in Natural Language Processing (NLP). It involves dividing a Textual input into smaller units known as tokens. These tokens can be in the form of words, characters, sub-words, or sentences. It helps in improving interpretability of text by different models. Let’s understand How Tokenization Works. Depending on the LLM, some researchers also consider additional special tokens such as the following: • [BOS] (beginning of sequence) —This token marks the start of a text. It signifies to the LLM where a piece of content begins. • [EOS] (end of sequence) —This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one ends and the next begins. • [PAD] (padding) —When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or “padded” using the [PAD] token, up to the length of the longest text in the batch. The tokenizer used for GPT models does not need any of these tokens; it only uses an <|endoftext|> token for simplicity. <|endoftext|> is analogous to the [EOS] token. <|endoftext|> is also used for padding. When training on batched inputs, we typically use a mask, meaning we don’t attend to',\n",
       "  'sentences': ['suitable for embedding audio or video data.',\n",
       "   'At its core, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space— the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process.',\n",
       "   'While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents.',\n",
       "   'Sentence or paragraph embeddings are popular choices for retrieval-augmented generation.',\n",
       "   'Retrieval-augmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text.',\n",
       "   'Since our goal is to train GPT- like LLMs, which learn to generate text one word at a time, we will focus on word embeddings.',\n",
       "   'Several algorithms and frameworks have been developed to generate word embeddings.',\n",
       "   'One of the earlier and most popular examples is the Word2Vec approach.',\n",
       "   'Word2Vec trained neural network architecture to generate word embeddings by predicting the context of a word given the target word or vice versa.',\n",
       "   'The main idea behind Word2Vec is that words that appear in similar contexts tend to have similar meanings.',\n",
       "   'Consequently, when projected into twodimensional word embeddings for visualization purposes, similar terms are clustered together.',\n",
       "   'Word embeddings can have varying dimensions, from one to thousands.',\n",
       "   'A higher dimensionality might capture more nuanced relationships but at the cost of computational efficiency.',\n",
       "   'While we can use pretrained models such as Word2Vec to generate embeddings for machine learning models, LLMs commonly produce their own embeddings that are part of the input layer and are updated during training.',\n",
       "   'The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand.',\n",
       "   'Unfortunately, high-dimensional embeddings present a challenge for visualization because our sensory perception and common graphical representations are inherently limited to three dimensions or fewer, which is why figure 2.3 shows two-dimensional embeddings in a two-dimensional scatterplot.',\n",
       "   'However, when working with LLMs, we typically use embeddings with a much higher dimensionality.',\n",
       "   'For both GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality of the model’s hidden states) varies based on the specific model variant and size.',\n",
       "   'It is a tradeoff between performance and efficiency.',\n",
       "   'The smallest GPT-2 models (117M and 125M parameters) use an embedding size of 768 dimensions to provide concrete examples.',\n",
       "   'The largest GPT-3 model (175B parameters) uses an embedding size of 12,288 dimensions.',\n",
       "   'Tokenizing text Tokenization is a fundamental step in Natural Language Processing (NLP).',\n",
       "   'It involves dividing a Textual input into smaller units known as tokens.',\n",
       "   'These tokens can be in the form of words, characters, sub-words, or sentences.',\n",
       "   'It helps in improving interpretability of text by different models.',\n",
       "   'Let’s understand How Tokenization Works.',\n",
       "   'Depending on the LLM, some researchers also consider additional special tokens such as the following: • [BOS] (beginning of sequence) —This token marks the start of a text.',\n",
       "   'It signifies to the LLM where a piece of content begins. • [',\n",
       "   'EOS] (end of sequence) —This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>.',\n",
       "   'For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one ends and the next begins. • [',\n",
       "   'PAD] (padding) —When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths.',\n",
       "   'To ensure all texts have the same length, the shorter texts are extended or “padded” using the [PAD] token, up to the length of the longest text in the batch.',\n",
       "   'The tokenizer used for GPT models does not need any of these tokens; it only uses an <|endoftext|> token for simplicity.',\n",
       "   '<|endoftext|> is analogous to the [EOS] token.',\n",
       "   '<|endoftext|> is also used for padding.',\n",
       "   'When training on batched inputs, we typically use a mask, meaning we don’t attend to'],\n",
       "  'sentences_count_spacy': 36,\n",
       "  'text_chunks': [['suitable for embedding audio or video data.',\n",
       "    'At its core, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space— the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process.',\n",
       "    'While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents.',\n",
       "    'Sentence or paragraph embeddings are popular choices for retrieval-augmented generation.',\n",
       "    'Retrieval-augmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text.',\n",
       "    'Since our goal is to train GPT- like LLMs, which learn to generate text one word at a time, we will focus on word embeddings.',\n",
       "    'Several algorithms and frameworks have been developed to generate word embeddings.',\n",
       "    'One of the earlier and most popular examples is the Word2Vec approach.',\n",
       "    'Word2Vec trained neural network architecture to generate word embeddings by predicting the context of a word given the target word or vice versa.',\n",
       "    'The main idea behind Word2Vec is that words that appear in similar contexts tend to have similar meanings.'],\n",
       "   ['Consequently, when projected into twodimensional word embeddings for visualization purposes, similar terms are clustered together.',\n",
       "    'Word embeddings can have varying dimensions, from one to thousands.',\n",
       "    'A higher dimensionality might capture more nuanced relationships but at the cost of computational efficiency.',\n",
       "    'While we can use pretrained models such as Word2Vec to generate embeddings for machine learning models, LLMs commonly produce their own embeddings that are part of the input layer and are updated during training.',\n",
       "    'The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand.',\n",
       "    'Unfortunately, high-dimensional embeddings present a challenge for visualization because our sensory perception and common graphical representations are inherently limited to three dimensions or fewer, which is why figure 2.3 shows two-dimensional embeddings in a two-dimensional scatterplot.',\n",
       "    'However, when working with LLMs, we typically use embeddings with a much higher dimensionality.',\n",
       "    'For both GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality of the model’s hidden states) varies based on the specific model variant and size.',\n",
       "    'It is a tradeoff between performance and efficiency.',\n",
       "    'The smallest GPT-2 models (117M and 125M parameters) use an embedding size of 768 dimensions to provide concrete examples.'],\n",
       "   ['The largest GPT-3 model (175B parameters) uses an embedding size of 12,288 dimensions.',\n",
       "    'Tokenizing text Tokenization is a fundamental step in Natural Language Processing (NLP).',\n",
       "    'It involves dividing a Textual input into smaller units known as tokens.',\n",
       "    'These tokens can be in the form of words, characters, sub-words, or sentences.',\n",
       "    'It helps in improving interpretability of text by different models.',\n",
       "    'Let’s understand How Tokenization Works.',\n",
       "    'Depending on the LLM, some researchers also consider additional special tokens such as the following: • [BOS] (beginning of sequence) —This token marks the start of a text.',\n",
       "    'It signifies to the LLM where a piece of content begins. • [',\n",
       "    'EOS] (end of sequence) —This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>.',\n",
       "    'For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one ends and the next begins. • ['],\n",
       "   ['PAD] (padding) —When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths.',\n",
       "    'To ensure all texts have the same length, the shorter texts are extended or “padded” using the [PAD] token, up to the length of the longest text in the batch.',\n",
       "    'The tokenizer used for GPT models does not need any of these tokens; it only uses an <|endoftext|> token for simplicity.',\n",
       "    '<|endoftext|> is analogous to the [EOS] token.',\n",
       "    '<|endoftext|> is also used for padding.',\n",
       "    'When training on batched inputs, we typically use a mask, meaning we don’t attend to']],\n",
       "  'chunk_size': 4},\n",
       " {'Page No.': 7,\n",
       "  'page_char_count': 3745,\n",
       "  'page_word_count': 559,\n",
       "  'page_sentence_count (Not accurate)': 26,\n",
       "  'page_token_count': 936.25,\n",
       "  'text': 'padded tokens. Thus, the specific token chosen for padding becomes inconsequential. Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token for out-of-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks words down into subword units. Attention in LLMs In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by “soft” weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size. Unlike “hard” weights, which are computed during the backwards training pass, “soft” weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme. Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state. Transformers In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets. The modern version of the transformer was proposed in the 2017 paper “Attention Is All You Need” by researchers at Google.Transformers were first developed as an improvement over previous architectures for machine translation,but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,audio,multimodal learning, robotics,[9] and even playing chess.It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers). Attention Mechanisms Before we dive into the self-attention mechanism at the heart of LLMs, let’s consider the problem with pre-LLM architectures that do not include attention mechanisms. Suppose we want to develop a language translation model that translates text from one language into another. We can’t simply translate a text word by word due to the grammatical structures in the source and target language. To address this problem, it is common to use a deep neural network with two submodules, an encoder and a decoder. The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text.',\n",
       "  'sentences': ['padded tokens.',\n",
       "   'Thus, the specific token chosen for padding becomes inconsequential.',\n",
       "   'Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token for out-of-vocabulary words.',\n",
       "   'Instead, GPT models use a byte pair encoding tokenizer, which breaks words down into subword units.',\n",
       "   'Attention in LLMs In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence.',\n",
       "   'In natural language processing, importance is represented by “soft” weights assigned to each word in a sentence.',\n",
       "   'More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.',\n",
       "   'Unlike “hard” weights, which are computed during the backwards training pass, “soft” weights exist only in the forward pass and therefore change with every step of the input.',\n",
       "   'Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.',\n",
       "   'Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks.',\n",
       "   'Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated.',\n",
       "   'Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.',\n",
       "   'Transformers In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.',\n",
       "   'At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.',\n",
       "   'Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.',\n",
       "   'The modern version of the transformer was proposed in the 2017 paper “Attention Is All You Need” by researchers at Google.',\n",
       "   'Transformers were first developed as an improvement over previous architectures for machine translation,but have found many applications since.',\n",
       "   'They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,audio,multimodal learning, robotics,[9] and even playing chess.',\n",
       "   'It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).',\n",
       "   'Attention Mechanisms Before we dive into the self-attention mechanism at the heart of LLMs, let’s consider the problem with pre-LLM architectures that do not include attention mechanisms.',\n",
       "   'Suppose we want to develop a language translation model that translates text from one language into another.',\n",
       "   'We can’t simply translate a text word by word due to the grammatical structures in the source and target language.',\n",
       "   'To address this problem, it is common to use a deep neural network with two submodules, an encoder and a decoder.',\n",
       "   'The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text.'],\n",
       "  'sentences_count_spacy': 24,\n",
       "  'text_chunks': [['padded tokens.',\n",
       "    'Thus, the specific token chosen for padding becomes inconsequential.',\n",
       "    'Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token for out-of-vocabulary words.',\n",
       "    'Instead, GPT models use a byte pair encoding tokenizer, which breaks words down into subword units.',\n",
       "    'Attention in LLMs In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence.',\n",
       "    'In natural language processing, importance is represented by “soft” weights assigned to each word in a sentence.',\n",
       "    'More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.',\n",
       "    'Unlike “hard” weights, which are computed during the backwards training pass, “soft” weights exist only in the forward pass and therefore change with every step of the input.',\n",
       "    'Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.',\n",
       "    'Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks.'],\n",
       "   ['Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated.',\n",
       "    'Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.',\n",
       "    'Transformers In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.',\n",
       "    'At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.',\n",
       "    'Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.',\n",
       "    'The modern version of the transformer was proposed in the 2017 paper “Attention Is All You Need” by researchers at Google.',\n",
       "    'Transformers were first developed as an improvement over previous architectures for machine translation,but have found many applications since.',\n",
       "    'They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,audio,multimodal learning, robotics,[9] and even playing chess.',\n",
       "    'It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).',\n",
       "    'Attention Mechanisms Before we dive into the self-attention mechanism at the heart of LLMs, let’s consider the problem with pre-LLM architectures that do not include attention mechanisms.'],\n",
       "   ['Suppose we want to develop a language translation model that translates text from one language into another.',\n",
       "    'We can’t simply translate a text word by word due to the grammatical structures in the source and target language.',\n",
       "    'To address this problem, it is common to use a deep neural network with two submodules, an encoder and a decoder.',\n",
       "    'The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text.']],\n",
       "  'chunk_size': 3},\n",
       " {'Page No.': 8,\n",
       "  'page_char_count': 2651,\n",
       "  'page_word_count': 420,\n",
       "  'page_sentence_count (Not accurate)': 18,\n",
       "  'page_token_count': 662.75,\n",
       "  'text': 'Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder–decoder architecture for language translation. An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text. If you are unfamiliar with RNNs, don’t worry—you don’t need to know the detailed workings of RNNs to follow this discussion; our focus here is more on the general concept of the encoder–decoder setup. In an encoder–decoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence in the final hidden state. The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction. While we don’t need to know the inner workings of these encoder–decoder RNNs, the key idea here is that the encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in this hidden state to produce the output. You can think of this hidden state as an embedding vector. The big limitation of encoder–decoder RNNs is that the RNN can’t directly access earlier hidden states from the encoder during the decoding phase. Consequently, it relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances. Capturing data dependencies with attention mechanisms Although RNNs work fine for translating short sentences, they don’t work well for longer texts as they don’t have direct access to previous words in the input. One major shortcoming in this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder. Hence, researchers developed the Bahdanau attention mechanism for RNNs in 2014 (named after the first author of the respective paper), which modifies the encoder– decoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step. Interestingly, only three years later, researchers found that RNN architectures are not required for building deep neural networks for natural language processing and proposed the original transformer architecture including a self-attention mechanism inspired by the Bahdanau attention mechanism.',\n",
       "  'sentences': ['Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder–decoder architecture for language translation.',\n",
       "   'An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text.',\n",
       "   'If you are unfamiliar with RNNs, don’t worry—you don’t need to know the detailed workings of RNNs to follow this discussion; our focus here is more on the general concept of the encoder–decoder setup.',\n",
       "   'In an encoder–decoder RNN, the input text is fed into the encoder, which processes it sequentially.',\n",
       "   'The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence in the final hidden state.',\n",
       "   'The decoder then takes this final hidden state to start generating the translated sentence, one word at a time.',\n",
       "   'It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction.',\n",
       "   'While we don’t need to know the inner workings of these encoder–decoder RNNs, the key idea here is that the encoder part processes the entire input text into a hidden state (memory cell).',\n",
       "   'The decoder then takes in this hidden state to produce the output.',\n",
       "   'You can think of this hidden state as an embedding vector.',\n",
       "   'The big limitation of encoder–decoder RNNs is that the RNN can’t directly access earlier hidden states from the encoder during the decoding phase.',\n",
       "   'Consequently, it relies solely on the current hidden state, which encapsulates all relevant information.',\n",
       "   'This can lead to a loss of context, especially in complex sentences where dependencies might span long distances.',\n",
       "   'Capturing data dependencies with attention mechanisms Although RNNs work fine for translating short sentences, they don’t work well for longer texts as they don’t have direct access to previous words in the input.',\n",
       "   'One major shortcoming in this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder.',\n",
       "   'Hence, researchers developed the Bahdanau attention mechanism for RNNs in 2014 (named after the first author of the respective paper), which modifies the encoder– decoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step.',\n",
       "   'Interestingly, only three years later, researchers found that RNN architectures are not required for building deep neural networks for natural language processing and proposed the original transformer architecture including a self-attention mechanism inspired by the Bahdanau attention mechanism.'],\n",
       "  'sentences_count_spacy': 17,\n",
       "  'text_chunks': [['Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder–decoder architecture for language translation.',\n",
       "    'An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text.',\n",
       "    'If you are unfamiliar with RNNs, don’t worry—you don’t need to know the detailed workings of RNNs to follow this discussion; our focus here is more on the general concept of the encoder–decoder setup.',\n",
       "    'In an encoder–decoder RNN, the input text is fed into the encoder, which processes it sequentially.',\n",
       "    'The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence in the final hidden state.',\n",
       "    'The decoder then takes this final hidden state to start generating the translated sentence, one word at a time.',\n",
       "    'It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction.',\n",
       "    'While we don’t need to know the inner workings of these encoder–decoder RNNs, the key idea here is that the encoder part processes the entire input text into a hidden state (memory cell).',\n",
       "    'The decoder then takes in this hidden state to produce the output.',\n",
       "    'You can think of this hidden state as an embedding vector.'],\n",
       "   ['The big limitation of encoder–decoder RNNs is that the RNN can’t directly access earlier hidden states from the encoder during the decoding phase.',\n",
       "    'Consequently, it relies solely on the current hidden state, which encapsulates all relevant information.',\n",
       "    'This can lead to a loss of context, especially in complex sentences where dependencies might span long distances.',\n",
       "    'Capturing data dependencies with attention mechanisms Although RNNs work fine for translating short sentences, they don’t work well for longer texts as they don’t have direct access to previous words in the input.',\n",
       "    'One major shortcoming in this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder.',\n",
       "    'Hence, researchers developed the Bahdanau attention mechanism for RNNs in 2014 (named after the first author of the respective paper), which modifies the encoder– decoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step.',\n",
       "    'Interestingly, only three years later, researchers found that RNN architectures are not required for building deep neural networks for natural language processing and proposed the original transformer architecture including a self-attention mechanism inspired by the Bahdanau attention mechanism.']],\n",
       "  'chunk_size': 2}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb3ec5",
   "metadata": {},
   "source": [
    "#### Splitting Chunks for ease of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ac6f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 6410.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "page_chunk = []\n",
    "for item in tqdm(text_info):\n",
    "    for parts in item[\"text_chunks\"]:  # each chunk is already a list of sentences\n",
    "        chunk_store = {}\n",
    "        chunk_store[\"page_number\"] = item['Page No.']\n",
    "\n",
    "        # Merge sentences into one paragraph\n",
    "        joined_sentence_chunk = \" \".join(parts).replace(\" \", \" \").strip()\n",
    "\n",
    "        # Optional: Ensure space after a period when followed by capital letter\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)\n",
    "\n",
    "        chunk_store[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "        chunk_store[\"sentence_chunk_size\"] = len(joined_sentence_chunk)  # char count\n",
    "        chunk_store[\"sentence_chunk_word_count\"] = len(joined_sentence_chunk.split())  # word count\n",
    "        chunk_store[\"sentence_chunk_tokens\"] = len(joined_sentence_chunk) / 4  # approx token count\n",
    "\n",
    "        page_chunk.append(chunk_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5822d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 5,\n",
       "  'sentence_chunk': 'GPT-3 has 96 transformer layers and 175 billion parameters in total. GPT-3 was introduced in 2020, which, by the standards of deep learning and large language model development, is considered a long time ago. However, more recent architectures, such as Meta’s Llama models, are still based on the same underlying concepts, introducing only minor modifications. Hence, understanding GPT remains as relevant as ever, so I focus on implementing the prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs. Although the original transformer model, consisting of encoder and decoder blocks, was explicitly designed for language translation, GPT models—despite their larger yet simpler decoder-only architecture aimed at next-word prediction—are also capable of performing translation tasks. This capability was initially unexpected to researchers, as it emerged from a model primarily trained on a next-word prediction task, which is a task that did not specifically target translation. The ability to perform tasks that the model wasn’t explicitly trained to perform is called an emergent behavior. This capability isn’t explicitly taught during training but emerges as a natural consequence of the model’s exposure to vast quantities of multilingual data in diverse contexts. The fact that GPT models can “learn” the translation patterns between languages and perform translation tasks even though they weren’t specifically trained for it demonstrates the benefits and capabilities of these large-scale, generative language models. We can perform diverse tasks without using diverse models for each.',\n",
       "  'sentence_chunk_size': 1647,\n",
       "  'sentence_chunk_word_count': 238,\n",
       "  'sentence_chunk_tokens': 411.75}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(page_chunk, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00af4c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk_size</th>\n",
       "      <th>sentence_chunk_word_count</th>\n",
       "      <th>sentence_chunk_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.90</td>\n",
       "      <td>1205.59</td>\n",
       "      <td>183.86</td>\n",
       "      <td>301.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.48</td>\n",
       "      <td>440.98</td>\n",
       "      <td>64.70</td>\n",
       "      <td>110.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>237.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>59.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.00</td>\n",
       "      <td>1058.00</td>\n",
       "      <td>161.00</td>\n",
       "      <td>264.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.00</td>\n",
       "      <td>1301.00</td>\n",
       "      <td>196.00</td>\n",
       "      <td>325.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.00</td>\n",
       "      <td>1431.00</td>\n",
       "      <td>224.00</td>\n",
       "      <td>357.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.00</td>\n",
       "      <td>1926.00</td>\n",
       "      <td>278.00</td>\n",
       "      <td>481.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  sentence_chunk_size  sentence_chunk_word_count  \\\n",
       "count        29.00                29.00                      29.00   \n",
       "mean          3.90              1205.59                     183.86   \n",
       "std           2.48               440.98                      64.70   \n",
       "min           0.00               237.00                      37.00   \n",
       "25%           2.00              1058.00                     161.00   \n",
       "50%           4.00              1301.00                     196.00   \n",
       "75%           6.00              1431.00                     224.00   \n",
       "max           8.00              1926.00                     278.00   \n",
       "\n",
       "       sentence_chunk_tokens  \n",
       "count                  29.00  \n",
       "mean                  301.40  \n",
       "std                   110.25  \n",
       "min                    59.25  \n",
       "25%                   264.50  \n",
       "50%                   325.25  \n",
       "75%                   357.75  \n",
       "max                   481.50  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(page_chunk)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882deb8",
   "metadata": {},
   "source": [
    "### Filtering out texts with low token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15c163d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_token_length = 25\n",
    "# for row in df[df[\"sentence_chunk_tokens\"]<=minimum_token_length].sample().iterrows():\n",
    "#     print(f\"Chunk token count: {row[1][\"sentence_chunk_tokens\"]} | Text: {row[1][\"sentence_chunk\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "989d7a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_chunk_min_token_filter = df[df[\"sentence_chunk_tokens\"]>minimum_token_length].to_dict(orient='records')\n",
    "len(page_chunk_min_token_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18a10b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 5,\n",
       "  'sentence_chunk': 'The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling. This means that we don’t need to collect labels for the training data explicitly but can use the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict. Since this next-word prediction task allows us to create labels “on the fly,” it is possible to use massive unlabeled text datasets to train LLMs. Compared to the original transformer architecture, the general GPT architecture is relatively simple. Essentially, it’s just the decoder part without the encoder. Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of autoregressive model. Autoregressive models incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves the coherence of the resulting text. Architectures such as GPT-3 are also significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times.',\n",
       "  'sentence_chunk_size': 1197,\n",
       "  'sentence_chunk_word_count': 190,\n",
       "  'sentence_chunk_tokens': 299.25},\n",
       " {'page_number': 0,\n",
       "  'sentence_chunk': 'Building LLMs Understanding Large Langauge Models Large language models (LLMs), such as those offered in OpenAI’s ChatGPT, are deep neural network models that have been developed over the past few years. They ushered in a new era for natural language processing (NLP). Before the advent of LLMs, traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern recognition that could be captured with handcrafted rules or simpler models. However, they typically underperformed in language tasks that demanded complex understanding and generation abilities, such as parsing detailed instructions, conducting contextual analysis, and creating coherent and contextually appropriate original text. For example, previous generations of language models could not write an email from a list of keywords—a task that is trivial for contemporary LLMs. LLMs have remarkable capabilities to understand, generate, and interpret human language. However, it’s important to clarify that when we say language models “understand,” we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension. Enabled by advancements in deep learning, which is a subset of machine learning and artificial intelligence (AI) focused on neural networks, LLMs are trained on vast quantities of text data. This large-scale training allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches. As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more.',\n",
       "  'sentence_chunk_size': 1738,\n",
       "  'sentence_chunk_word_count': 248,\n",
       "  'sentence_chunk_tokens': 434.5}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(page_chunk_min_token_filter, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ec8ba1",
   "metadata": {},
   "source": [
    "#### Embedding our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a001ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da77c7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:01<00:00, 25.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.97 s\n",
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embedding_model.to(\"cuda\")\n",
    "\n",
    "#embedding chunks \n",
    "for item in tqdm(page_chunk_min_token_filter):\n",
    "    item[\"embedding\"]=embedding_model.encode(item['sentence_chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7449bf2",
   "metadata": {},
   "source": [
    "#### Running Encoding in Batch Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c5a941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 37.4 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_chunks_batch = [item[\"sentence_chunk\"] for item in page_chunk_min_token_filter]\n",
    "len(text_chunks_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d564151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.36 s\n",
      "Wall time: 760 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0497,  0.0489, -0.0100,  ...,  0.0106, -0.0750, -0.0096],\n",
       "        [ 0.0550,  0.0377, -0.0213,  ...,  0.0049, -0.0822, -0.0202],\n",
       "        [ 0.0629, -0.0123,  0.0018,  ...,  0.0186, -0.0843, -0.0322],\n",
       "        ...,\n",
       "        [ 0.0039,  0.0779,  0.0072,  ..., -0.0410, -0.0429, -0.0501],\n",
       "        [ 0.0220, -0.0329, -0.0213,  ..., -0.0156, -0.0374, -0.0483],\n",
       "        [ 0.0111,  0.0677, -0.0058,  ...,  0.0233, -0.0760, -0.0426]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# encodding in batch\n",
    "embedding_model.to(\"cuda\")\n",
    "text_chunks_batch_encoding = embedding_model.encode(text_chunks_batch, batch_size=32, convert_to_tensor=True)\n",
    "text_chunks_batch_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a3f7d5",
   "metadata": {},
   "source": [
    "#### Saving the data in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0eb91655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving in file\n",
    "text_chunk_data = pd.DataFrame(page_chunk_min_token_filter)\n",
    "emded_savepath = \"text_chunk_data.csv\"\n",
    "text_chunk_data.to_csv(emded_savepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3085122",
   "metadata": {},
   "source": [
    "#### Importing our csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "859fe1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "594c31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('text_chunk_data.csv')\n",
    "\n",
    "pages_chunker = dataset.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c79e38db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 0,\n",
       "  'sentence_chunk': 'Building LLMs Understanding Large Langauge Models Large language models (LLMs), such as those offered in OpenAI’s ChatGPT, are deep neural network models that have been developed over the past few years. They ushered in a new era for natural language processing (NLP). Before the advent of LLMs, traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern recognition that could be captured with handcrafted rules or simpler models. However, they typically underperformed in language tasks that demanded complex understanding and generation abilities, such as parsing detailed instructions, conducting contextual analysis, and creating coherent and contextually appropriate original text. For example, previous generations of language models could not write an email from a list of keywords—a task that is trivial for contemporary LLMs. LLMs have remarkable capabilities to understand, generate, and interpret human language. However, it’s important to clarify that when we say language models “understand,” we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension. Enabled by advancements in deep learning, which is a subset of machine learning and artificial intelligence (AI) focused on neural networks, LLMs are trained on vast quantities of text data. This large-scale training allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches. As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more.',\n",
       "  'sentence_chunk_size': 1738,\n",
       "  'sentence_chunk_word_count': 248,\n",
       "  'sentence_chunk_tokens': 434.5,\n",
       "  'embedding': '[ 4.97364588e-02  4.89069782e-02 -9.98813193e-03  7.88642690e-02\\n -4.06036004e-02  3.98051403e-02 -1.10539319e-02  4.00389032e-03\\n  2.35506464e-02 -7.12331682e-02 -4.58530448e-02 -5.26028983e-02\\n -2.08384767e-02  3.51001322e-02 -1.13105103e-02 -2.45500170e-02\\n  2.22483203e-02 -1.91955157e-02 -4.79189400e-03  1.01800831e-02\\n -5.90443471e-03  3.33568491e-02  1.70573816e-02  6.00161105e-02\\n -8.53453763e-03 -3.16472165e-02 -1.63407549e-02 -1.93214475e-03\\n -3.25298272e-02  2.50922143e-02  5.54244127e-03 -6.64348574e-03\\n  3.62849683e-02  8.35586935e-02  1.88232571e-06 -5.40941954e-02\\n  4.68835002e-03 -1.92465056e-02 -5.93935549e-02 -1.59204623e-03\\n  5.77702373e-02 -4.20972668e-02 -1.56210596e-02  2.33407854e-03\\n -4.28366773e-02  1.65398736e-02  2.94420589e-02  5.21456897e-02\\n  5.43904640e-02  8.90871063e-02  1.46443248e-02 -5.82640283e-02\\n  6.27977699e-02 -6.41628122e-03  2.47400198e-02 -6.30742311e-02\\n  2.27146968e-02 -1.95537638e-02 -3.17795831e-03  1.64653845e-02\\n -2.84538115e-03  4.14032117e-02  3.81696522e-02 -1.15452670e-02\\n -1.29570474e-03  2.10960638e-02 -7.14749051e-03 -3.77576873e-02\\n -1.36187812e-02  6.25639558e-02 -3.72853614e-02 -2.30988171e-02\\n -6.49816683e-03 -6.55175466e-03 -1.51889464e-02 -2.91257109e-02\\n -4.48290296e-02  5.60788363e-02  1.01631703e-02  5.97723909e-02\\n  4.23805006e-02 -8.93161446e-03  4.96231839e-02  5.94869889e-02\\n -3.17886844e-02  4.81066070e-02 -1.93360671e-02 -1.83276050e-02\\n  2.34481506e-02  2.41959170e-02 -1.51005583e-02 -5.79868145e-02\\n  2.32544318e-02  7.08250776e-02 -2.72749458e-03  3.12295668e-02\\n -1.19089605e-02 -1.75940637e-02  3.66102308e-02 -9.16795582e-02\\n  2.19137464e-02  2.60313805e-02 -4.57085855e-02  3.47918682e-02\\n -3.46651226e-02  4.65847589e-02 -3.90588827e-02  1.69158857e-02\\n -5.98794892e-02 -3.80750261e-02 -5.61155602e-02 -3.89162675e-02\\n -1.12406956e-02  3.27554978e-02 -4.93276939e-02 -2.74640359e-02\\n -3.66116948e-02  3.70505191e-02  6.07641181e-03  1.06952302e-02\\n  7.29462132e-03  2.33902577e-02 -3.53521854e-02 -1.89613886e-02\\n -4.47307564e-02 -2.69453344e-03  8.44625174e-04  2.08860189e-02\\n  5.94775146e-03 -6.63076416e-02  9.68074705e-03  3.80761623e-02\\n -2.99259671e-03 -2.20707338e-02  1.41460551e-02  5.49779050e-02\\n  4.96794563e-03 -1.95069686e-02 -7.63467327e-02 -1.36689283e-02\\n -3.61944400e-02 -7.02882707e-02  2.59290021e-02 -4.79538552e-02\\n  1.67725105e-02 -1.46038039e-03 -2.12080050e-02 -3.41421813e-02\\n  5.75026032e-03  1.39101343e-02 -5.97021878e-02  6.49805292e-02\\n -9.00202058e-03 -1.53763201e-02  4.38633487e-02  1.48178460e-02\\n  1.50705799e-02  4.75415476e-02  2.85504907e-02  4.69268225e-02\\n  3.17739323e-02  1.98483746e-02  1.65400468e-02  3.66813429e-02\\n -7.38499220e-03 -1.27385482e-02 -3.29273008e-02 -2.28878669e-03\\n -2.94574834e-02  4.37438898e-02  2.16562487e-02  4.84917313e-02\\n -1.61414605e-03 -4.53758333e-03  5.43746948e-02  5.46797216e-02\\n  2.12371629e-02  6.75853416e-02  7.61578232e-02 -1.02787465e-03\\n  4.35625087e-04  1.25328433e-02 -3.80502827e-02  7.33743981e-03\\n -8.48715566e-03  8.83785449e-03 -1.39467977e-02  4.23339661e-03\\n -3.04320455e-02 -5.53196855e-02 -1.71188787e-02 -1.12519898e-02\\n -6.87967101e-03  7.30686029e-03  5.01847789e-02 -8.76918063e-03\\n -1.04792155e-02  5.17563298e-02  3.10492925e-02 -2.69813687e-02\\n  2.50203032e-02 -9.22025144e-02  7.94115383e-03  8.48812982e-02\\n  1.58474911e-02 -1.91114191e-02 -2.92452853e-02 -1.59026608e-02\\n  2.34446004e-02  5.84406927e-02  2.87527870e-02  5.19421045e-03\\n  7.46579748e-03 -3.94125171e-02  2.63917781e-02 -2.08172295e-02\\n  4.77525592e-03  7.21099554e-03 -5.62746637e-03 -1.83167700e-02\\n  1.13235200e-02 -1.40783451e-02 -4.31897938e-02  4.98150364e-02\\n -5.18886000e-02 -4.63410392e-02  1.61077213e-02  5.74227702e-03\\n  4.96428572e-02  4.00343165e-02  3.56689980e-03  1.90319738e-03\\n  2.72189844e-02 -6.50283098e-02 -2.23409068e-02 -9.51627735e-03\\n -3.51906680e-02  4.13257815e-03  4.29214835e-02  4.88502672e-03\\n -2.67194733e-02  3.31908315e-02 -5.68306409e-02 -3.29679325e-02\\n  1.29421055e-03 -2.24661324e-02  3.77221666e-02 -6.41453341e-02\\n -2.17335522e-02  1.35622714e-02  2.25623976e-02 -3.43019925e-02\\n  7.48429447e-02 -6.12743199e-03 -3.83633077e-02  1.66324545e-02\\n  1.53416395e-02  1.62544698e-02 -2.86300946e-02 -1.09205414e-02\\n  2.37390641e-02 -9.69778467e-03  1.24027757e-02 -1.91030111e-02\\n  4.78313453e-02 -5.26548252e-02 -2.93322727e-02 -9.03746858e-02\\n  1.92831326e-02 -2.12627146e-02  3.51150706e-02  1.85366701e-02\\n -7.46796280e-03  3.34275467e-03  1.93727091e-02 -4.13649529e-02\\n  3.57800126e-02  3.15610096e-02 -5.89616643e-03  5.16218692e-02\\n  1.26325842e-02 -3.55394110e-02 -4.18728292e-02 -4.62969719e-03\\n -2.26406232e-02  5.73495962e-02  1.26379691e-02 -1.27914632e-02\\n -6.45142868e-02  1.25809340e-02 -2.72034798e-02  2.35161595e-02\\n  8.56585242e-03 -5.84920906e-02  7.99576286e-03 -3.76036800e-02\\n -2.67860014e-02  6.38423041e-02  1.25747826e-02  8.38849172e-02\\n  1.42527837e-02 -5.95146464e-03 -5.88099472e-02  1.16931759e-02\\n -2.20026728e-02 -1.84060205e-02 -4.02231477e-02 -5.39046153e-03\\n  2.61809421e-03  6.71147034e-02 -2.25172844e-03 -1.71013176e-02\\n -2.51143761e-02 -2.95440760e-03  5.45276841e-03 -3.34773064e-02\\n -4.17967364e-02 -7.04203453e-03 -3.44397873e-02  3.43291424e-02\\n -2.82438509e-02 -3.10932472e-02  3.90131921e-02 -2.62931101e-02\\n  1.42021384e-02 -6.16620779e-02 -4.85926345e-02 -5.67235239e-02\\n  6.96628988e-02 -2.00728271e-02  2.89601944e-02 -1.23710232e-02\\n -1.87856983e-02 -2.15635579e-02  3.04362876e-03 -3.46894562e-02\\n -1.01672187e-02  5.38937524e-02 -5.33297248e-02  1.52977575e-02\\n  8.02231766e-03 -8.56014993e-03 -7.35180750e-02 -1.75566282e-02\\n -4.13479991e-02  2.90712826e-02  1.08036166e-02  6.50516991e-03\\n -2.76340973e-02 -1.52704371e-02  8.96764174e-03  5.33078890e-03\\n -1.98615529e-03 -1.62812192e-02 -4.87377085e-02  4.05838899e-02\\n  2.39821132e-02  1.37611199e-02  3.83961871e-02 -2.66590826e-02\\n  4.06433754e-02  6.11171946e-02  7.10482756e-03 -7.56688137e-03\\n -6.53134361e-02  7.74164451e-03  3.45617756e-02 -6.15536720e-02\\n -1.54197197e-02  1.73576958e-02  3.48435976e-02 -6.17244579e-02\\n -3.02939657e-02  1.27649993e-01  3.67083326e-02 -5.17473482e-02\\n  6.42060861e-03  8.43492709e-03  1.16679957e-03 -1.29141370e-02\\n  4.04961146e-02 -8.63922611e-02 -3.91278155e-02  1.56081319e-02\\n -1.49267707e-02 -1.75535940e-02 -2.85290438e-03 -9.26672854e-03\\n -7.61930719e-02  3.23133804e-02 -2.76572537e-03 -5.69530837e-02\\n -1.79951638e-02 -5.88437589e-03 -3.60208303e-02  2.24037115e-02\\n  6.69779554e-02 -4.21861978e-03  5.74213453e-03 -3.55582684e-02\\n  2.34033093e-02 -1.46787269e-02 -4.54076342e-02  2.98083797e-02\\n -3.95772941e-02  6.11927845e-02  7.57313967e-02  6.22749962e-02\\n -2.09066384e-02  5.46773337e-03  6.40269890e-02 -1.60676725e-02\\n  4.86396663e-02 -2.40340512e-02  3.49623859e-02 -6.45420048e-03\\n -8.90791300e-04  2.59245280e-03  4.23803404e-02 -9.35095251e-02\\n -1.91232283e-02 -1.51900360e-02  7.80778453e-02 -3.21518816e-02\\n -1.44835571e-02 -6.95586205e-03  3.71388439e-03 -1.37147307e-02\\n  6.85248002e-02  3.65887620e-02 -4.55945581e-02 -5.83573394e-02\\n -4.75826412e-02 -1.18239550e-02 -2.84904018e-02 -3.43621038e-02\\n  3.76384296e-02  1.03529583e-05 -3.10184862e-02 -5.33435978e-02\\n -3.40308845e-02  2.02793479e-02 -1.95644312e-02  8.63139704e-03\\n  5.61567163e-03  1.49090989e-02 -3.68350162e-03 -3.28310765e-02\\n  4.16282006e-02 -3.43845300e-02 -2.10303813e-04 -4.26629223e-02\\n -1.57347955e-02  1.78141110e-02 -5.43815531e-02 -1.34053361e-02\\n -5.70500793e-04 -8.23336095e-02 -5.08893393e-02  4.30782093e-03\\n  1.94757879e-02  3.23672332e-02  1.52578680e-02 -3.12060560e-03\\n -7.45972618e-02  9.40577686e-03  4.28976268e-02 -2.56011449e-02\\n -8.10167007e-03  2.18982939e-02  4.17401642e-02  3.54380868e-02\\n  4.78912629e-02 -3.45721915e-02  3.66485715e-02  3.92825566e-02\\n  9.14976839e-03 -2.07890384e-02 -4.62622792e-02  1.78640727e-02\\n  1.21090990e-02  1.56567506e-02  2.25666771e-03 -2.13325378e-02\\n -3.35392393e-02 -2.26791226e-03  1.50809651e-02 -2.44065700e-03\\n -4.96462807e-02 -1.27460491e-02  3.48753482e-02 -5.99927036e-03\\n -1.95115665e-03 -1.96338375e-03 -2.75189206e-02  2.53544580e-02\\n -4.48127612e-02  6.26626611e-03  8.19174387e-03  1.63966753e-02\\n  2.50501023e-03  1.14590917e-02 -3.26813348e-02  2.50469074e-02\\n -3.46979462e-02  2.03097723e-02 -5.31083113e-03  3.23625207e-02\\n  3.27742705e-03  1.26928939e-02 -2.55474094e-02  2.17263848e-02\\n -8.21658894e-02 -5.76665066e-02 -4.95570479e-03  6.14312552e-02\\n  9.07366425e-02 -1.15552088e-02  1.38810044e-02 -2.96970792e-02\\n -7.12283654e-03 -3.86553211e-03  3.66769843e-02 -5.15960203e-03\\n -1.11059397e-02 -1.02435483e-03  1.47820106e-02 -2.04351861e-02\\n -1.92147065e-02  7.31695592e-02  8.59730598e-03  2.92375349e-02\\n  2.70815045e-02 -1.47468802e-02 -3.36982943e-02 -2.73399986e-02\\n  3.78509015e-02  1.70289651e-02 -1.32017229e-02  2.60447208e-02\\n -2.06877273e-02 -5.83664700e-02  2.02830303e-02  8.24459456e-03\\n  9.17626824e-03 -4.73334454e-02  3.23154144e-02  7.69412443e-02\\n  8.17017071e-03 -2.27618497e-03 -1.89520121e-02  1.91863105e-02\\n -1.29681863e-02 -3.88739742e-02  8.30450212e-04 -6.08107031e-33\\n  2.02143788e-02 -2.06405148e-02  2.27995068e-02  3.83176580e-02\\n -2.46755071e-02  7.82167539e-03 -2.38127112e-02 -6.44036802e-03\\n -2.78199073e-02  1.67187527e-02 -1.76097024e-02  2.03991123e-02\\n  1.99468341e-02  4.17466462e-02  4.52070381e-04 -3.47229503e-02\\n  1.73009280e-02 -2.69800313e-02  3.73113416e-02  1.29985735e-02\\n  4.62115146e-02  5.99599108e-02  4.65740748e-02 -5.90308495e-02\\n  5.00059761e-02 -3.49562173e-03  1.46412884e-03  4.99681383e-02\\n -2.28973683e-02  7.11278394e-02 -6.80613965e-02  5.23614883e-02\\n  5.19353375e-02 -3.88695858e-02  6.00280939e-03  4.09973748e-02\\n -3.82659473e-02 -1.55516779e-02  2.52691731e-02  1.76088661e-02\\n -4.55486849e-02 -7.26249143e-02  8.84876996e-02  1.25749977e-02\\n -6.38242229e-04  1.15179820e-02  3.06311455e-02 -3.30830812e-02\\n -7.29031104e-04 -1.49043556e-02 -5.61227389e-02 -6.97964977e-04\\n  2.62283231e-03 -1.44501841e-02  2.16659345e-02 -1.66954715e-02\\n -3.58184651e-02  6.44564480e-02 -6.53824508e-02 -1.60117447e-02\\n  1.95460417e-03  2.35260911e-02  7.28728250e-02  5.73299453e-03\\n -8.30853824e-03  7.70855546e-02  1.73515547e-02 -1.71486363e-02\\n -5.53061143e-02 -9.70121566e-03  2.35492326e-02  4.43093106e-02\\n  2.10135449e-02  5.02839796e-02  3.76996286e-02 -1.95075554e-04\\n -9.34277475e-02 -3.49997962e-03  3.96477059e-02  2.00760849e-02\\n  4.53137122e-02  4.72860364e-03 -2.83388551e-02 -6.37693889e-03\\n -1.82497166e-02 -8.37218948e-03 -4.53550592e-02 -1.34418439e-02\\n  3.07609905e-02 -7.80633185e-03 -1.54709890e-02 -4.37175706e-02\\n  2.02569142e-02 -8.23984146e-02  3.11807822e-03 -1.01801166e-02\\n -4.20925347e-03  1.02719050e-02  2.23063044e-02  2.14381889e-02\\n -5.10757565e-02 -6.66684285e-02 -3.51079851e-02 -1.77117698e-02\\n  1.14604151e-02  1.41201764e-02  3.57558094e-02  9.79328714e-03\\n -7.76926242e-03  7.41946278e-03 -3.74859385e-02  4.30153823e-03\\n -1.57181954e-03  5.71257137e-02  3.30363237e-03  4.83456673e-03\\n  3.62543948e-03  6.82518184e-02  1.54716419e-02  2.83808867e-03\\n  5.41384274e-04 -1.26643339e-02  3.57313268e-03  5.66136464e-03\\n -7.03477040e-02 -1.96396746e-02 -3.41742896e-02  9.63043049e-03\\n  3.84427570e-02 -7.18298703e-02 -6.55590836e-03  1.80125944e-02\\n  2.54132232e-07  7.62515292e-02  4.82920557e-02  2.34253779e-02\\n  3.33248526e-02  4.67902571e-02  3.92309874e-02 -2.85915332e-03\\n  2.21642619e-03 -2.98983767e-03 -5.39924763e-02 -6.38346421e-03\\n  7.32225999e-02  4.32183146e-02  1.55844465e-02 -8.80492926e-02\\n -8.00656062e-03 -4.81072478e-02  1.16409734e-03 -4.97457795e-02\\n  1.51291490e-02  7.95155913e-02  1.21210456e-01 -5.64422319e-03\\n -5.43479212e-02  2.43741414e-03 -4.30598818e-02  7.05952570e-02\\n -3.62963378e-02 -1.70867692e-03  2.24154256e-02  3.48959044e-02\\n  4.47231345e-03  8.53501726e-03 -4.39559110e-03  3.60406050e-03\\n  3.18025611e-03 -1.98093019e-02  9.15193856e-02  8.18466433e-05\\n -1.96170751e-02  4.13479470e-03 -3.03338729e-02  2.37598233e-02\\n -6.89588040e-02  7.14626163e-02 -7.31406137e-02 -3.95311788e-02\\n -4.23060022e-02  3.91521864e-02 -4.24914472e-02 -1.37123922e-02\\n -2.13225596e-02  2.06955150e-02  7.83053692e-03  1.64671484e-02\\n -5.04575968e-02 -8.30574334e-03 -7.86142796e-02 -8.99523497e-03\\n -2.31727604e-02 -6.18801638e-02  8.45085946e-04 -4.80419323e-02\\n  3.00132181e-03  6.54824749e-02 -3.87890227e-02  1.06872600e-02\\n  2.47406962e-34  1.75218657e-02  5.53340069e-04 -3.98463526e-05\\n  5.17022870e-02  2.81654894e-02  1.51705509e-02 -7.30763981e-03\\n  2.59545650e-02  1.05997948e-02 -7.49749169e-02 -9.61396378e-03]'},\n",
       " {'page_number': 0,\n",
       "  'sentence_chunk': 'Another important distinction between contemporary LLMs and earlier NLP models is that earlier NLP models were typically designed for specific tasks, such as text categorization, language translation, etc. While those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks. The success behind LLMs can be attributed to the transformer architecture that underpins many LLMs and the vast amounts of data on which LLMs are trained, allowing them to capture a wide variety of linguistic nuances, contexts, and patterns that would be challenging to encode manually. This shift toward implementing models based on the transformer architecture and using large training datasets to train LLMs has fundamentally transformed NLP, providing more capable tools for understanding and interacting with human language. The following discussion sets a foundation to accomplish the primary objective of this book: understanding LLMs by implementing a ChatGPT-like LLM based on the transformer architecture step by step in code. What is an LLM ? An LLM is a neural network designed to understand, generate, and respond to human-like text. These models are deep neural networks trained on massive amounts of text data, sometimes encompassing large portions of the entire publicly available text on the internet. The “large” in “large language model” refers to both the model’s size in terms of parameters and the immense dataset on which it’s trained. Models like this often have tens or even hundreds of billions of parameters, which are the adjustable weights in the network that are optimized during training to predict the next word in a sequence.',\n",
       "  'sentence_chunk_size': 1707,\n",
       "  'sentence_chunk_word_count': 264,\n",
       "  'sentence_chunk_tokens': 426.75,\n",
       "  'embedding': '[ 5.50463200e-02  3.76939327e-02 -2.12665554e-02  6.55955672e-02\\n -3.55205908e-02  1.90979782e-02 -4.02562842e-02  3.50446217e-02\\n  7.37809855e-03 -7.09264278e-02 -2.73692291e-02 -8.98161717e-03\\n -1.74899362e-02 -3.07756918e-03  2.87670828e-02 -3.07460949e-02\\n  4.60908115e-02 -7.48747122e-03 -3.70575264e-02  1.38921347e-02\\n -1.04863336e-02  1.31514324e-02  1.26708206e-02  5.06196022e-02\\n -1.19479175e-03 -1.54320961e-02 -2.22522542e-02  1.82001907e-02\\n -2.47618444e-02 -1.24417432e-02  1.41077796e-02  1.84495430e-02\\n  3.86923328e-02  2.00223885e-02  1.84329394e-06 -4.35791425e-02\\n  1.37462188e-03 -2.56407671e-02 -2.65203509e-02 -1.48278745e-02\\n  3.97214517e-02 -1.26595851e-02 -2.21490525e-02  1.48862470e-02\\n -5.09431437e-02  3.34670860e-03  5.03199808e-02  7.64460042e-02\\n  5.93680441e-02  1.07271351e-01  4.36049467e-03 -6.91602677e-02\\n  5.18185310e-02 -1.17699278e-03  1.75578445e-02 -4.31957431e-02\\n  3.76732424e-02 -1.47711849e-02  2.06658170e-02  4.27475236e-02\\n -2.24699713e-02  2.30880026e-02  1.26661249e-02  1.16938232e-02\\n  1.50437467e-02  1.62127782e-02 -3.03372219e-02 -5.37511744e-02\\n -4.56333272e-02  5.11960201e-02 -1.32642053e-02 -3.69215868e-02\\n  2.70227101e-02  1.52858784e-02 -5.01015922e-03  2.35868571e-03\\n -4.10126150e-02  4.16033827e-02  1.76395010e-02  4.43032198e-02\\n  1.01003638e-02 -1.58860590e-02  2.55990066e-02  4.02405336e-02\\n -2.58418098e-02  6.24856539e-02 -4.59915493e-03 -3.16242012e-03\\n -1.47526730e-02  1.60248280e-02  2.45800950e-02 -6.74315840e-02\\n  4.77468632e-02  7.60914087e-02  2.58830143e-03  2.70520374e-02\\n -1.21597704e-02  1.55186364e-02  4.70590964e-02 -7.66212642e-02\\n  2.07790472e-02  1.22410227e-02 -2.30672937e-02  2.61252560e-02\\n -4.56071980e-02  5.09206764e-02 -3.53569835e-02  1.33480979e-02\\n -4.71051671e-02  4.92261536e-03 -6.36836365e-02 -3.16751860e-02\\n -2.70951819e-02  4.48578186e-02 -1.80829056e-02 -2.58895364e-02\\n -4.76312228e-02  4.89183813e-02  2.01347936e-02  5.11178374e-03\\n  2.41435152e-02  2.63609868e-02 -1.27403848e-02  6.29715202e-03\\n -5.44803292e-02  1.41868601e-03  3.54820350e-03 -4.17756196e-03\\n -2.11189478e-03 -7.72836134e-02  5.53531758e-03  3.30720581e-02\\n -1.62840746e-02 -1.33666107e-02  1.97767597e-02  1.01026922e-01\\n  7.72570074e-03 -3.45784649e-02 -8.04499686e-02 -1.03319325e-02\\n -2.67970189e-02 -3.96438353e-02  2.15698816e-02 -8.02627802e-02\\n  2.13393942e-02  2.64173164e-03  1.19549665e-03 -4.27753925e-02\\n  2.02552490e-02  3.61650661e-02 -5.00662625e-02  7.49024823e-02\\n -2.77820192e-02  8.38164613e-03  2.95386519e-02  1.45152630e-02\\n  2.29629017e-02  5.12573123e-02  3.67145352e-02  3.24792601e-02\\n  4.95847389e-02 -1.17180562e-02  2.08214130e-02  4.92123589e-02\\n -3.00468132e-02 -1.69851109e-02 -2.56434325e-02 -1.18002631e-02\\n -1.95993707e-02  4.81050499e-02  2.91622803e-02  5.91137111e-02\\n -1.73720513e-02 -1.84641983e-02  6.59265742e-02  8.45180750e-02\\n  5.70870899e-02  7.74731040e-02  3.58274803e-02  3.20289209e-02\\n  6.97715627e-03 -5.45968534e-03 -4.78593558e-02  2.91790254e-02\\n -3.23156826e-02 -7.17199733e-03 -2.14765333e-02  3.54661271e-02\\n -2.45399289e-02 -5.74970469e-02 -4.10160124e-02 -1.28853582e-02\\n -7.64962612e-03 -7.24421768e-03  2.03079917e-02  2.66773161e-02\\n -2.95859817e-02  1.21135667e-01  1.56121142e-02 -2.94026639e-02\\n -5.17365895e-03 -8.05793032e-02 -1.16281537e-03  5.94179481e-02\\n  3.39531228e-02 -2.60974690e-02 -3.07617001e-02 -2.41711140e-02\\n -3.45611083e-03  5.94294369e-02  3.01883277e-02  8.85798968e-03\\n -2.22250093e-02 -4.77115512e-02  7.81420991e-03  1.74162704e-02\\n  9.62970778e-03  1.46562913e-02  9.48077068e-03 -1.01061584e-02\\n  4.87352442e-03 -7.97158014e-03 -3.86987105e-02  5.39095700e-02\\n -4.89130542e-02 -3.86120863e-02  4.50629089e-03 -1.68876108e-02\\n  2.49702446e-02  2.80473828e-02  2.49653240e-03 -6.29245362e-04\\n  6.38389811e-02 -1.68915447e-02 -3.88928838e-02 -1.51918158e-02\\n -3.72671969e-02 -6.49435446e-03  4.58976850e-02 -3.46000604e-02\\n  1.51156858e-02  3.13548148e-02 -3.99712771e-02 -3.53967212e-02\\n -9.09215398e-03 -5.51314652e-02  5.24118356e-02 -5.24506420e-02\\n  2.64147613e-02 -1.60388951e-03  2.93215401e-02 -5.68791479e-02\\n  4.83795516e-02  1.56408711e-03  3.97996744e-03 -7.95782916e-03\\n  1.45297684e-02  1.16396218e-03 -4.17482592e-02 -1.29112089e-02\\n  4.71937880e-02 -2.33027581e-02  2.05669403e-02  7.83483312e-03\\n  4.07634452e-02 -5.94663732e-02 -6.32964149e-02 -7.37744346e-02\\n  1.99549105e-02 -8.24432820e-03  2.10808832e-02  1.86150931e-02\\n -7.95095577e-04 -1.33280802e-04 -1.53476128e-03 -3.79771963e-02\\n  3.65895182e-02  7.29437731e-03 -5.20454068e-03  7.38016814e-02\\n -2.14155186e-02 -4.66972776e-02 -4.23838533e-02 -5.09984838e-03\\n -1.19979028e-02  5.58000505e-02 -1.25029692e-02 -4.68646660e-02\\n -8.29697028e-02  3.01494058e-02 -2.90740281e-02  1.72268525e-02\\n -1.12097906e-02 -6.13084659e-02  8.66213068e-03 -2.30350066e-02\\n  2.40125954e-02  4.31767069e-02  2.70282477e-02  5.41439876e-02\\n  2.62399539e-02 -1.00701470e-02 -5.46929576e-02  3.84668959e-03\\n -3.73843946e-02 -3.86895761e-02 -5.18344007e-02 -2.20494047e-02\\n  4.53309249e-03  8.80737007e-02 -6.23531872e-04 -3.68349603e-03\\n -2.56285071e-02  4.29197494e-03  1.96322352e-02 -1.84213221e-02\\n  1.70512721e-02  5.08565968e-03 -6.28471002e-02  3.01422551e-02\\n -7.93786626e-03 -4.74450476e-02  3.10552139e-02 -3.30319256e-02\\n -6.65699458e-03 -4.30662930e-02 -5.28868400e-02 -6.45279512e-02\\n  6.20715432e-02  9.58343409e-03  1.56284589e-02 -1.79300306e-03\\n -4.30173613e-02 -1.06176957e-02  1.12807509e-02 -3.16009298e-02\\n -5.02178520e-02  8.13764557e-02 -3.27212699e-02  2.23119115e-03\\n -1.20229702e-02 -1.46669159e-02 -4.11574952e-02  5.16252220e-03\\n -4.45664451e-02  8.23630951e-04  1.73085555e-02 -9.03220009e-03\\n -5.61665334e-02 -1.58589203e-02  9.10220295e-03  1.66891888e-02\\n -1.02004688e-02 -2.14752108e-02 -5.26400022e-02  1.61220916e-02\\n  1.86513029e-02  3.04881781e-02  3.00672930e-02  1.42069869e-02\\n  2.02072542e-02  7.58019537e-02 -5.42912574e-04  2.47076526e-02\\n -5.36399372e-02  1.80226676e-02  6.66842982e-02 -7.13498369e-02\\n -3.94532755e-02  3.26575674e-02  4.13668640e-02 -5.04085645e-02\\n  6.24396699e-03  1.34501189e-01  2.29590312e-02 -6.45047277e-02\\n  1.80886127e-02 -6.73281774e-03 -1.58447828e-02  3.62885408e-02\\n  3.77991423e-02 -8.70445892e-02 -2.68296245e-02  6.57845847e-03\\n -1.44267436e-02 -1.18416278e-02 -1.63343493e-02 -2.43835188e-02\\n -5.90733401e-02  2.44601537e-02  1.07844789e-02 -5.75582264e-03\\n -1.23878103e-02 -3.03350836e-02 -2.59246454e-02  3.75637831e-03\\n  3.77268493e-02 -1.22012990e-03 -3.27862687e-02 -3.15522961e-02\\n  1.07376305e-02 -3.94326858e-02 -5.50281722e-03  1.52541196e-03\\n -3.51305492e-02  4.58138920e-02  5.64963482e-02  5.46832457e-02\\n -3.46339941e-02 -2.06668582e-02  5.20180389e-02  4.80795931e-03\\n  3.44131552e-02 -3.72106507e-02  4.33624052e-02 -1.23019628e-02\\n  3.02138575e-03 -6.91286614e-03  1.76893305e-02 -7.01961145e-02\\n -1.53679485e-02 -9.94858099e-04  7.96560347e-02 -3.57124284e-02\\n -2.65701301e-02 -1.33158658e-02  8.50723404e-03 -1.28048118e-02\\n  6.16919808e-02  2.15113088e-02 -2.90403664e-02 -5.09535223e-02\\n -4.58183661e-02 -1.59529001e-02 -1.06617026e-02 -6.76220283e-02\\n  1.23853674e-02 -7.97071494e-03 -1.16789052e-02 -6.20481037e-02\\n  6.53826137e-05 -2.71702232e-03 -1.41202891e-02  2.01091915e-02\\n  1.69423688e-03  2.90865917e-02 -1.35422796e-02 -5.26710711e-02\\n  3.30539308e-02 -2.63306051e-02  8.46845470e-03 -5.70863299e-02\\n -1.33241303e-02  8.69955346e-02 -5.50940484e-02 -1.00621497e-02\\n -9.37349722e-03 -8.71234164e-02 -6.27594963e-02 -1.21651944e-02\\n  2.35199369e-02  2.70079244e-02  1.69641748e-02  1.81171147e-03\\n -7.24608600e-02  1.86416190e-02  5.47983162e-02 -5.22757061e-02\\n  1.40664047e-02  2.90626753e-02  3.73690836e-02  9.94592719e-03\\n  3.18736732e-02 -3.48205082e-02  2.69579887e-02  2.25568976e-04\\n  1.34367254e-02 -4.21946831e-02 -6.16743825e-02  1.79713424e-02\\n  8.50119349e-03  3.93147208e-02  1.44871054e-02 -1.42574022e-02\\n -4.68738703e-03 -8.09993222e-03  8.80544074e-03  1.06607191e-02\\n -4.43653315e-02  4.94905142e-03  2.69634556e-02  4.59828088e-03\\n -9.78166889e-03 -1.32801840e-02 -2.47879699e-02  2.32142601e-02\\n -1.01566399e-02  6.29341453e-02  1.28145954e-02 -1.63242053e-02\\n  4.55082534e-03  2.23342627e-02 -1.83631647e-02  1.70362052e-02\\n -3.17087793e-03  3.33267152e-02 -1.39455060e-02  4.96023409e-02\\n  4.81428625e-03  2.17807163e-02 -2.69722976e-02  3.60045359e-02\\n -6.83975369e-02 -5.48907071e-02 -9.08920832e-04  5.01071736e-02\\n  7.50796422e-02  2.55821040e-03  4.04507034e-02 -2.15792265e-02\\n -1.23938220e-02 -6.14595087e-03  4.28004973e-02 -3.03873941e-02\\n -1.15649879e-03  3.34917456e-02  2.25909930e-02 -2.79732496e-02\\n -1.58204194e-02  5.00114486e-02 -1.50125772e-02  2.07095332e-02\\n  2.41107419e-02 -1.91884357e-02 -3.62551734e-02 -2.38506533e-02\\n  4.66511957e-02  5.18657221e-03  3.05772619e-03  3.99426417e-03\\n -1.78873371e-02 -4.20932062e-02  5.69474585e-02 -4.12539858e-03\\n  1.63445137e-02 -5.76968715e-02  3.96494456e-02  8.30878988e-02\\n  2.24481858e-02 -8.50236975e-03 -3.53503786e-02  8.83011799e-03\\n -3.88286338e-04 -3.80778611e-02 -2.59269103e-02 -6.06917688e-33\\n -2.01798379e-02 -5.86280935e-02  4.54085767e-02  4.84642647e-02\\n -5.54788634e-02 -1.07057542e-02 -8.34690500e-03 -4.17847419e-03\\n -2.54099406e-02 -1.61923375e-02 -2.93790568e-02  1.36968689e-02\\n  1.14386585e-02  2.26479191e-02  1.25143714e-02 -3.65526676e-02\\n  1.77416932e-02 -1.25914784e-02  2.41447389e-02 -5.11724688e-03\\n -3.76398861e-03  5.19865789e-02  7.55733177e-02 -5.09628505e-02\\n  1.81283765e-02  1.42456908e-02  1.06678950e-02  3.59803289e-02\\n -5.95510285e-03  6.05999976e-02 -4.52243350e-02  5.50424270e-02\\n  4.32634614e-02 -4.36052270e-02 -1.98797113e-03  3.96317355e-02\\n -6.47935495e-02 -4.62253094e-02  2.53931712e-02  3.95262428e-02\\n -1.07810795e-02 -6.37045279e-02  8.76232311e-02 -1.27657661e-02\\n -5.39411493e-02 -1.77955138e-03  4.30915095e-02 -3.06897536e-02\\n -2.60004699e-02 -4.09672484e-02 -3.82445417e-02  2.68120710e-02\\n  8.33891798e-03  6.44736178e-03  3.38370837e-02 -1.35932164e-03\\n -4.36009578e-02  3.73130478e-02 -5.56754097e-02 -5.09369420e-03\\n -1.92364084e-03  4.76215072e-02  3.79952490e-02  3.63190770e-02\\n -9.45591740e-03  6.03271239e-02  3.66960317e-02 -2.79051624e-02\\n -4.06169817e-02  1.21457987e-02  2.48583127e-02  6.33484572e-02\\n  3.36386934e-02  2.27456167e-02  7.75197707e-03 -1.88017208e-02\\n -2.96125505e-02  4.40242607e-03  4.46929522e-02  4.50559780e-02\\n  3.14614139e-02  2.22534612e-02 -1.56950857e-02 -1.95663702e-02\\n -2.41783299e-02 -3.10593117e-02 -1.90972909e-02 -3.70663591e-02\\n  1.61502231e-02  1.06026797e-04 -1.21701870e-03  5.31661185e-03\\n -3.08518251e-03 -7.77442753e-02 -9.33787320e-03 -1.79127604e-02\\n  3.97499371e-03  9.10110120e-03  9.89680924e-03  2.28342954e-02\\n -4.61137630e-02 -5.61437272e-02 -2.76009925e-02 -2.25044303e-02\\n  1.77034009e-02 -5.78544335e-03  1.70936249e-02  2.35105697e-02\\n -4.75855097e-02 -4.77328943e-03 -2.12134589e-02 -1.02012428e-02\\n  5.85707556e-03  3.75274345e-02  7.10314140e-04 -1.42583074e-02\\n -9.12766717e-03  5.04706278e-02  6.62812591e-03 -1.18801091e-02\\n -3.00551299e-02 -9.52774100e-03  2.62477458e-03  3.80743779e-02\\n -6.00813664e-02  6.55259937e-03 -5.19187003e-02  4.09955233e-02\\n  3.07269581e-02 -7.75862485e-02 -2.15591397e-02  2.08687782e-02\\n  2.60696766e-07  5.31002581e-02  7.16108233e-02  2.39830576e-02\\n  3.30717862e-02  3.66688631e-02  7.69333169e-03 -5.16432943e-03\\n  3.11773177e-02  1.40851103e-02 -3.58002596e-02  1.15793031e-02\\n  4.69024330e-02  4.39078808e-02 -2.91309482e-03 -1.08155377e-01\\n -4.40124469e-03 -5.69337867e-02  1.64987184e-02 -3.96498218e-02\\n  1.14884228e-02  6.79187849e-02  1.08206265e-01  8.21803231e-03\\n -2.31653266e-02 -9.80888493e-03 -3.34855542e-02  4.25363891e-02\\n -6.75783306e-02  1.94377359e-02  2.94078290e-02  4.54634354e-02\\n  1.23658543e-02  2.53152498e-03 -7.72183156e-03 -2.24067178e-03\\n  2.61260476e-02 -2.09423751e-02  9.88181084e-02 -9.58980527e-03\\n -2.45832950e-02  1.16106113e-02 -3.50507982e-02  6.95609953e-03\\n -5.06691076e-02  7.18483999e-02 -5.44562303e-02 -2.60370020e-02\\n -2.11575199e-02  4.49975301e-03 -3.89958471e-02 -1.91301089e-02\\n -1.99701134e-02 -3.49622010e-03  2.27564611e-02  3.34343687e-02\\n -2.67209094e-02 -1.65797304e-02 -9.65726003e-02 -8.85294471e-03\\n -5.06195659e-03 -6.81217611e-02 -2.62187477e-02 -4.34802771e-02\\n -2.03571375e-03  2.50748303e-02 -2.71467213e-02 -2.60554533e-02\\n  2.57462421e-34  1.35693755e-02 -1.25823859e-02  2.20845491e-02\\n  5.98151684e-02  2.77517159e-02  1.21638346e-02  1.78525150e-02\\n  3.06361206e-02  4.94727492e-03 -8.21888074e-02 -2.01873556e-02]'},\n",
       " {'page_number': 0,\n",
       "  'sentence_chunk': 'Next-word prediction is sensible because it harnesses the inherent sequential nature of language to train models on understanding context, structure, and relationships within text. Yet, it is a very simple task, and so it is surprising to many researchers that it can produce such capable models. In later chapters, we will discuss and implement the nextword training procedure step by step. LLMs utilize an architecture called the transformer, which allows them to pay selective attention to different parts of the input when making predictions, making them especially adept at handling the nuances and complexities of human language.',\n",
       "  'sentence_chunk_size': 635,\n",
       "  'sentence_chunk_word_count': 97,\n",
       "  'sentence_chunk_tokens': 158.75,\n",
       "  'embedding': '[ 6.28555268e-02 -1.23052867e-02  1.84929126e-03  4.13038880e-02\\n -4.05596904e-02  6.27803523e-03 -1.81849487e-02  1.81377791e-02\\n  1.01629198e-02 -4.79920991e-02 -4.99074645e-02 -4.63408791e-02\\n  1.60758838e-03  1.34253502e-03  4.47959788e-02 -5.17657362e-02\\n  4.93037552e-02 -2.50837151e-02 -8.27521458e-03  1.24662109e-02\\n -1.94876976e-02 -5.30078122e-03 -1.24636972e-02  3.07388995e-02\\n  1.24173495e-03 -8.34246352e-03 -2.52525639e-02 -1.86714120e-02\\n -3.29680555e-03 -4.64856103e-02  5.22873923e-02  2.30036378e-02\\n  2.13312823e-02 -1.20085292e-02  1.68399617e-06 -4.09739241e-02\\n  2.45226491e-02 -1.99027676e-02 -2.93857511e-03 -4.15658578e-02\\n  2.82303337e-02 -7.64585054e-03 -2.01808680e-02  2.45516356e-02\\n -7.29844421e-02  1.37479892e-02  3.72473262e-02  6.77740201e-02\\n  2.22652759e-02  6.49325103e-02 -6.89314213e-03 -5.55008687e-02\\n  4.13490236e-02 -1.64375119e-02  4.88571711e-02 -4.18255478e-02\\n  2.68356856e-02 -5.05347773e-02  2.00466556e-03  2.61050612e-02\\n -3.20082493e-02  2.46607680e-02 -2.25134958e-02  4.06773910e-02\\n  1.53419878e-02  4.18106653e-02  5.44349775e-02 -5.12148961e-02\\n -6.17492646e-02  1.54208755e-02 -2.61538476e-03  1.14846444e-02\\n  1.11285858e-02  1.76671543e-04 -1.46197425e-02  1.94212068e-02\\n -3.75659689e-02  2.93042921e-02 -5.05319890e-03  1.25750629e-02\\n  2.31602229e-02  1.81859341e-02  1.85582377e-02  4.06011194e-02\\n -4.63953651e-02  1.05807766e-01  3.67915933e-03 -4.68085781e-02\\n  2.84301285e-02 -1.05774477e-02  6.54513389e-02 -5.81537932e-02\\n  2.66137067e-02  5.76203167e-02 -8.89916997e-03  3.78311314e-02\\n -1.71928424e-02 -7.60481786e-03  3.37593295e-02 -4.52580526e-02\\n -7.11490773e-03  3.80425970e-03 -1.86300948e-02  6.79672603e-03\\n -2.32218336e-02  1.84207298e-02 -5.92697458e-03  2.28346530e-02\\n -1.78038776e-02  2.04664804e-02 -7.44965451e-04 -2.01947074e-02\\n  9.52901901e-04  2.79191583e-02 -9.71390773e-03 -5.45163415e-02\\n -5.27902134e-02  3.25562619e-02  3.02437115e-02 -3.28565128e-02\\n  2.57977583e-02  1.72415413e-02 -2.23645102e-02 -1.96203757e-02\\n -7.51141682e-02  1.51316337e-02 -4.10798937e-03  2.93845981e-02\\n -2.51825042e-02 -3.75393964e-02 -1.25289727e-02  2.12184899e-02\\n -8.81495699e-03 -1.54302809e-02  2.10358091e-02  9.97628272e-02\\n -1.34783164e-02 -7.03526214e-02 -4.57682349e-02  1.04688965e-02\\n -3.23182059e-04 -3.28084640e-02  4.25792560e-02 -3.07155196e-02\\n -2.04726402e-02  3.19789574e-02 -1.20292762e-02 -2.19287574e-02\\n  3.72352563e-02  5.75556085e-02 -4.63276282e-02  1.08761132e-01\\n -1.61337573e-02 -6.10366371e-03  1.25443153e-02  7.34476093e-03\\n  3.12704444e-02  4.15879004e-02  3.88502963e-02  3.78974900e-02\\n  3.98869924e-02  3.87568981e-03  3.16199823e-03  4.92917262e-02\\n -2.84027010e-02 -2.46081110e-02 -3.54455449e-02 -1.58309545e-02\\n -2.64745709e-02  4.98347878e-02  4.04451415e-02  4.87280898e-02\\n -5.22055710e-03  1.37300445e-02  3.46016437e-02  1.01411924e-01\\n  6.08037598e-02  4.23167646e-02  4.50758934e-02  2.32389215e-02\\n -3.13652605e-02  3.84609364e-02 -5.49926162e-02  4.41944040e-02\\n -1.75332241e-02  1.79807190e-02  8.74847826e-03  2.57256087e-02\\n  1.40181766e-03 -4.52958308e-02 -2.95320135e-02 -2.91440766e-02\\n  3.88983428e-03 -5.25383726e-02  1.09660486e-02  3.59195881e-02\\n -7.93574452e-02  9.60497111e-02  1.75878592e-02 -2.49103419e-02\\n -2.61202827e-02 -5.07696681e-02  2.36281641e-02  6.03482910e-02\\n  4.69945408e-02 -1.33284610e-02 -3.36427279e-02 -1.39263747e-02\\n  1.85640994e-02  5.81719242e-02  2.46030409e-02  2.45798787e-04\\n -1.42790405e-02 -3.91692147e-02 -3.18831727e-02  3.98722030e-02\\n -9.20843426e-03 -3.59113025e-03  1.02667566e-02  4.33563534e-03\\n  8.44320748e-03  8.47929099e-04  9.37711727e-03  7.65026584e-02\\n -3.75836641e-02 -4.87826914e-02 -1.33831101e-02  6.24295883e-03\\n  1.24305654e-02 -2.30873898e-02  1.42267533e-02  1.80627163e-02\\n  6.95641711e-02 -7.45985378e-03 -1.71505250e-02 -3.12619447e-03\\n -7.65241683e-02  1.80818215e-02  3.88692655e-02 -2.55146734e-02\\n  3.84954177e-02  1.10234646e-02 -6.21866668e-03 -3.46661620e-02\\n -2.02319995e-02 -4.66603562e-02  7.18785077e-02 -4.35062796e-02\\n  3.51118743e-02  3.84315550e-02  2.03694906e-02 -1.92520749e-02\\n  5.85126206e-02 -2.69091632e-02  5.02477176e-02 -1.69947487e-03\\n  3.03072715e-03 -1.27666164e-02 -2.77596089e-04  4.74471413e-03\\n  1.62593685e-02  2.95697525e-03 -1.88109446e-02  1.35670109e-02\\n  1.52080385e-02 -6.20250069e-02 -5.70905022e-02 -5.08341752e-02\\n -2.25137314e-03 -1.18345546e-03  4.76867072e-02 -7.13948114e-03\\n -5.46251098e-03 -1.39922118e-02 -3.14340815e-02 -2.93711051e-02\\n  2.87231971e-02 -7.75417779e-03 -3.11345868e-02  7.71654770e-02\\n  1.40938219e-02 -2.11263336e-02 -7.47420117e-02  1.28487460e-04\\n -3.07301283e-02  4.08446677e-02 -9.34523158e-03 -5.12622893e-02\\n -6.43391237e-02  2.19885949e-02 -2.75688656e-02  1.43463248e-02\\n -2.74077896e-02 -6.24254756e-02  1.27196573e-02 -3.26777995e-02\\n  5.67534901e-02  2.49597728e-02  6.87760934e-02  2.69698445e-02\\n  2.08677631e-02 -2.17483751e-02 -2.26397756e-02 -8.19217414e-04\\n -3.36505100e-02 -2.24473290e-02 -6.61063492e-02 -1.61820091e-02\\n -8.65217671e-03  8.96737128e-02  3.18301283e-02  2.59041935e-02\\n -2.54070014e-02  8.15015007e-03  3.81006561e-02 -2.01792270e-02\\n  1.26783289e-02  2.06774417e-02 -8.63928869e-02  2.48678997e-02\\n  7.39722140e-03 -5.25881425e-02  4.68982309e-02 -1.68401226e-02\\n -1.42665412e-02 -1.97163802e-02 -1.15185762e-02 -2.17343960e-02\\n  3.81493419e-02  1.20658446e-02 -2.11922042e-02 -1.09125087e-02\\n -2.32671406e-02 -3.77772115e-02  1.24905165e-02 -3.49061489e-02\\n -2.26185303e-02  6.00933284e-02 -2.77320445e-02 -1.13124549e-02\\n  1.48721980e-02 -1.83376446e-02 -6.50511310e-02 -2.76865941e-02\\n -6.30964190e-02 -1.23367626e-02  1.27509031e-02 -1.59075260e-02\\n -6.75775483e-02 -3.23777571e-02  2.32850295e-02  3.22682932e-02\\n -2.59262715e-02 -1.93484575e-02 -3.01576070e-02  3.30636427e-02\\n  5.20286225e-02  2.60879658e-02  3.62563841e-02 -7.37157557e-03\\n  5.35023771e-02  4.81279455e-02  2.03056857e-02 -1.71743333e-02\\n -1.90491639e-02 -8.24930705e-03  5.08264340e-02 -6.55874535e-02\\n -3.48071009e-02  6.77025765e-02  3.07871066e-02 -5.23923188e-02\\n -9.38033685e-03  1.20537564e-01  2.41542589e-02 -4.57904078e-02\\n  1.65760238e-02 -1.60024352e-02  1.97134688e-02  5.50020002e-02\\n  2.47936416e-02 -7.02336505e-02 -4.76496518e-02  7.62007199e-03\\n  1.30112972e-02 -9.49767977e-03 -3.30777355e-02 -3.35404687e-02\\n -4.81869616e-02  1.24105839e-02  1.50512476e-02 -1.97060071e-02\\n -3.06688505e-03 -5.32095181e-03  5.71476063e-03  2.24733017e-02\\n  2.06268393e-02  2.54220348e-02 -6.28466830e-02 -2.02462003e-02\\n -2.94479844e-03 -5.70061840e-02  3.50522925e-03  8.46203603e-03\\n -2.18740366e-02  3.72024775e-02  6.41458556e-02  1.11611178e-02\\n -3.13276090e-02 -4.46443632e-02  3.97802107e-02 -2.41919700e-02\\n  4.09197845e-02 -3.46453413e-02  5.82124256e-02 -3.57093625e-02\\n  1.29127167e-02 -1.92666370e-02  7.10996147e-03 -3.14033292e-02\\n -1.56103680e-02  1.47131151e-02  7.48017132e-02 -5.23949713e-02\\n -3.83244604e-02 -1.78373754e-02  3.78331952e-02  1.18706776e-02\\n  5.49358390e-02  2.93209013e-02 -3.26610543e-02 -4.55006100e-02\\n -2.68320777e-02  1.05819255e-02 -2.35919431e-02 -5.25493398e-02\\n -2.50184890e-02 -9.39928554e-03  1.14060044e-02 -7.01400861e-02\\n -6.28186166e-02 -1.90464093e-03 -3.58936540e-03  4.16061049e-03\\n  6.97108218e-03 -2.35784892e-02 -2.23323107e-02 -5.80640994e-02\\n -2.20667087e-02  2.35841200e-02  3.46390456e-02 -7.30228052e-02\\n -9.27774608e-03  6.96351826e-02 -2.30599083e-02 -2.27522794e-02\\n  7.13337306e-03 -8.88097659e-02 -3.86916474e-02 -1.48018659e-03\\n  7.83605129e-02  1.56043516e-02  1.15285553e-02  2.07635835e-02\\n -6.54195175e-02  4.39418033e-02  2.84916759e-02 -5.20859472e-02\\n  2.56962404e-02  5.56665175e-02  2.59619057e-02  1.89098340e-04\\n  2.76996028e-02  1.77969493e-03  2.73842886e-02 -1.39309578e-02\\n  2.50590499e-02 -1.48818055e-02 -8.82235393e-02 -1.20393827e-03\\n -3.68598737e-02  6.25524223e-02 -4.40226346e-02  2.86933277e-02\\n -1.96029097e-02  2.80754808e-02  7.28135183e-03  1.04256785e-02\\n -2.36288598e-03  3.09050251e-02  6.62243646e-03  1.44226775e-02\\n -2.98795905e-02 -3.07859592e-02 -7.17324167e-02  1.63955230e-03\\n -2.67394930e-02  6.52966648e-02  2.35723816e-02 -1.32702114e-02\\n  7.20370514e-03 -2.39224499e-03 -1.61671266e-02  1.15225017e-02\\n  1.67163648e-02  4.63861339e-02 -1.05885370e-03  7.06911013e-02\\n -4.60607279e-03  2.02645790e-02 -1.39413048e-02  3.25190462e-02\\n -8.40984285e-02 -1.65000428e-02 -2.96208803e-02  4.24179211e-02\\n  2.47661043e-02 -6.39861217e-03  3.15453522e-02 -2.16222759e-02\\n  8.40339903e-03  9.10517387e-03  3.25568914e-02 -6.91362619e-02\\n -1.97384842e-02  8.16185102e-02  1.64119247e-02 -2.04150733e-02\\n -1.23509355e-02  5.49322292e-02 -6.08664053e-03  3.78496274e-02\\n  6.20420277e-03 -2.80137919e-02 -5.59950285e-02 -4.71889190e-02\\n  2.72235237e-02  2.72015445e-02  1.32695977e-02  1.30411461e-02\\n -3.31121646e-02 -4.44385037e-02  9.45660919e-02  5.55132329e-03\\n  6.30970020e-03 -7.73272365e-02  5.47466055e-02  4.31055352e-02\\n  6.53242343e-04 -3.76750752e-02 -3.30435559e-02  8.03538412e-03\\n  3.03975847e-02 -2.84104981e-02 -2.12682746e-02 -5.56586877e-33\\n -4.72483560e-02 -6.25278428e-02  5.26558086e-02  5.28343394e-02\\n -3.25621925e-02 -2.88264565e-02 -1.55477785e-02 -1.76372044e-02\\n -3.57154408e-04  8.19087680e-03 -4.57555912e-02  2.26763934e-02\\n  2.19831150e-02 -1.39083285e-02  4.14432473e-02 -3.47313769e-02\\n  1.60443950e-02 -1.58902239e-02  1.83279868e-02 -2.58688219e-02\\n -2.08520871e-02  9.01038125e-02  8.40221718e-02 -3.49237733e-02\\n  3.71199362e-02 -1.49769150e-02 -6.70390669e-03  1.13529575e-04\\n  2.07462832e-02  4.51209657e-02 -4.54672985e-02  1.65369678e-02\\n  3.12320385e-02 -3.68105620e-02 -3.56195718e-02  4.62920740e-02\\n -5.02556786e-02 -4.69569042e-02  2.95994394e-02  3.46417762e-02\\n -4.36265878e-02 -2.08191182e-02  4.71650846e-02 -3.87974940e-02\\n -5.83598651e-02  1.54105946e-02  4.77723069e-02 -4.23694290e-02\\n -5.51469959e-02 -1.36798946e-02 -4.57725450e-02  1.56527571e-02\\n  2.77723633e-02 -5.88834519e-03  2.56166440e-02  1.34394756e-02\\n -1.99945085e-02 -1.70043502e-02 -4.43969145e-02  2.92483401e-02\\n -3.67474137e-03  5.62564693e-02  1.36716878e-02  4.76584695e-02\\n -1.77176502e-02  7.93287680e-02  3.37629318e-02 -6.45329654e-02\\n -1.95090007e-02  1.83212627e-02  1.01838689e-02  4.60200086e-02\\n -7.54378317e-03  6.16694521e-03  1.13876192e-02 -1.05046891e-02\\n  2.96197948e-03  1.46061592e-02  3.22116353e-02  7.30959699e-02\\n  7.32299825e-03 -9.36876703e-03 -9.40324459e-03 -1.65883880e-02\\n -1.46465478e-02 -1.71675850e-02  1.00802723e-02 -4.70036082e-02\\n  2.24907156e-02  1.46251405e-02  2.57814974e-02 -1.57119322e-03\\n  3.78202763e-03 -3.61836255e-02 -6.46213768e-03  3.42479087e-02\\n  6.43291103e-04 -1.22372024e-02 -2.02556024e-03  2.07387675e-02\\n -6.94228038e-02 -1.36456694e-02 -2.53508217e-04  9.86041687e-03\\n  7.23498408e-03 -1.13725045e-03 -1.50889307e-02  2.76734792e-02\\n -1.06565252e-01 -3.21230553e-02 -1.37918759e-02 -1.17612472e-02\\n  6.84872735e-03  1.00825550e-02  3.42033729e-02 -1.45322960e-02\\n -1.01686763e-02  5.19436784e-02  7.79486215e-03 -4.09610644e-02\\n -2.47164294e-02 -2.59569963e-03  1.63450204e-02  5.17440625e-02\\n -4.20005471e-02  1.41830873e-02 -3.68425436e-02  4.55707535e-02\\n  8.27576313e-03 -7.75572807e-02 -7.91623350e-03  5.73245399e-02\\n  2.41188019e-07  5.17234057e-02  6.10724613e-02  3.82218733e-02\\n  7.61765661e-03  3.49580985e-03  5.85623924e-03 -1.85109191e-02\\n  2.22728420e-02  9.44285654e-03 -1.33439275e-02  4.85679731e-02\\n  2.89925151e-02  2.45658457e-02 -2.34349407e-02 -9.91428420e-02\\n -1.87932514e-02 -8.23277384e-02  1.42736332e-02 -3.08639668e-02\\n -2.49735955e-02  4.46231924e-02  7.68953934e-02  1.21023960e-03\\n -3.66030149e-02 -3.62386443e-02 -3.12091820e-02  3.94408815e-02\\n -4.14084457e-02  1.94943734e-02 -2.14099046e-03  3.38135660e-02\\n  1.60419978e-02 -2.19207946e-02  8.71564727e-03 -1.25764338e-02\\n  4.93266061e-02 -3.83692086e-02  6.46090508e-02  2.61122175e-03\\n -4.62344401e-02  3.96889411e-02  4.66479873e-03 -1.23517150e-02\\n -6.76689949e-03  5.43461442e-02 -2.89282463e-02  6.74911449e-03\\n -2.09076665e-02  2.77347621e-02 -1.04280899e-03 -3.51840188e-03\\n -3.46683972e-02 -9.00391769e-03  3.09267337e-03  2.58725900e-02\\n -1.96872186e-02 -1.41848577e-02 -9.41160321e-02 -2.08219141e-02\\n  1.53578566e-02 -3.94482948e-02 -2.02444103e-02 -6.68761134e-02\\n -1.16449762e-02  3.39116231e-02 -1.80248078e-02 -2.95042079e-02\\n  1.77401130e-34  2.69829202e-02 -3.25481705e-02 -1.14351902e-02\\n  5.06119728e-02  1.79392640e-02 -1.12937363e-02  5.33850379e-02\\n  3.38361450e-02  1.86178740e-02 -8.43330324e-02 -3.21882814e-02]'},\n",
       " {'page_number': 1,\n",
       "  'sentence_chunk': 'Since LLMs are capable of generating text, LLMs are also often referred to as a form of generative artificial intelligence, often abbreviated as generative AI or GenAI. As illustrated in figure 1.1, AI encompasses the broader field of creating machines that can perform tasks requiring humanlike intelligence, including understanding language, recognizing patterns, and making decisions, and includes subfields like machine learning and deep learning. The algorithms used to implement AI are the focus of the field of machine learning. Specifically, machine learning involves the development of algorithms that can learn from and make predictions or decisions based on data without being explicitly programmed. To illustrate this, imagine a spam filter as a practical application of machine learning. Instead of manually writing rules to identify spam emails, a machine learning algorithm is fed examples of emails labeled as spam and legitimate emails. By minimizing the error in its predictions on a training dataset, the model then learns to recognize patterns and characteristics indicative of spam, enabling it to classify new emails as either spam or not spam. deep learning is a subset of machine learning that focuses on utilizing neural networks with three or more layers (also called deep neural networks) to model complex patterns and abstractions in data. In contrast to deep learning, traditional machine learning requires manual feature extraction. This means that human experts need to identify and select the most relevant features for the model.',\n",
       "  'sentence_chunk_size': 1562,\n",
       "  'sentence_chunk_word_count': 237,\n",
       "  'sentence_chunk_tokens': 390.5,\n",
       "  'embedding': '[ 2.98256185e-02 -3.56071419e-03 -5.73035888e-02  1.39373289e-02\\n -7.38074407e-02  4.34086025e-02  4.10149880e-02  2.60766447e-02\\n  4.01288457e-02 -3.01000979e-02  4.37139831e-02 -2.42534168e-02\\n -2.47624964e-02  6.19379133e-02  5.26701398e-02 -2.14760248e-02\\n  8.02917778e-03 -4.57044058e-02  2.16138400e-02 -5.98428771e-03\\n -3.08978166e-02 -1.35537870e-02  1.23166051e-02 -1.17966486e-02\\n -3.52073684e-02 -3.85894394e-03 -6.84168609e-03 -2.33679917e-02\\n -2.10544597e-02  9.25622042e-03  1.83537928e-03 -5.42408638e-02\\n  3.15077975e-02  1.50894765e-02  2.10834810e-06 -4.64361086e-02\\n  7.77399773e-03  9.46787558e-03 -2.66031381e-02 -1.87524315e-02\\n -1.66030265e-02  2.28083879e-02 -2.43267510e-03  2.83318274e-02\\n -5.00693545e-02  5.17724790e-02  6.71388656e-02  4.96217087e-02\\n -1.43061802e-02  7.49627501e-02 -8.75582453e-03 -5.78901805e-02\\n  7.25620762e-02 -1.90753327e-03  3.67805585e-02 -8.20153113e-03\\n  5.08543849e-02 -5.94399534e-02 -2.43626628e-02 -2.62404373e-03\\n  2.02990565e-02  3.15947942e-02  3.40322172e-03 -4.79558762e-03\\n  2.55016945e-02  6.29642699e-03  3.23287658e-02 -5.01497239e-02\\n -2.08331775e-02  1.54345650e-02 -1.90260205e-02  1.07579855e-02\\n  2.34949775e-02 -1.67916119e-02 -5.43546770e-03  9.56009421e-03\\n -3.52248028e-02  4.10702191e-02 -1.01193460e-02  2.53123753e-02\\n  1.56060550e-02 -6.89624110e-03  4.62993495e-02  8.45818128e-03\\n -3.84499095e-02  4.42495495e-02 -2.72417478e-02 -5.68946870e-03\\n -1.58606786e-02 -2.37681884e-02  7.50612766e-02 -5.47357425e-02\\n  3.49090658e-02  6.80508539e-02  4.38133031e-02 -1.00009062e-03\\n -1.89101975e-02  1.21348100e-02  2.75492109e-02 -3.44889350e-02\\n  1.22265788e-02 -2.76235715e-02 -7.45085254e-03  3.91742866e-03\\n -2.90695932e-02 -9.78130731e-04 -2.52488181e-02  6.71306476e-02\\n -4.46422398e-03 -6.91359676e-03 -3.08546666e-02 -3.99770215e-02\\n  7.32590025e-03  8.39948505e-02  3.35032418e-02  1.43436762e-02\\n -6.15440905e-02  2.49478556e-02 -5.91934398e-02 -3.75268608e-03\\n -4.62201647e-02  2.47870618e-03 -3.22582899e-03  3.06671355e-02\\n -8.68711621e-02 -4.73081060e-02 -1.40173621e-02  4.10103835e-02\\n  2.08807793e-02 -4.99670506e-02  3.72207388e-02  3.94116342e-02\\n -1.29823922e-03  1.28719779e-02  3.15366462e-02  9.42828953e-02\\n  4.07450460e-03 -9.07214824e-03 -9.90376025e-02 -6.34789979e-03\\n  2.20222864e-03 -5.95236681e-02  3.32004130e-02 -1.92663278e-02\\n -2.03219056e-02 -6.03908859e-03 -2.18798257e-02 -4.55180220e-02\\n -2.43381970e-02  6.28039427e-03 -3.32953781e-02  3.55478935e-02\\n  7.72854127e-03 -4.59449412e-03  7.69834742e-02  2.05927470e-04\\n  3.81990336e-02  2.28595152e-03  3.59180826e-03  3.62854265e-02\\n  1.22489352e-02 -1.22575825e-02  7.05544055e-02  1.33640869e-02\\n -1.81474630e-02 -4.65176022e-03 -3.58201936e-02 -4.11545783e-02\\n -8.87935460e-02  3.17377946e-03 -2.40167789e-02  2.55514719e-02\\n -6.48446009e-03  5.09757660e-02  7.87944272e-02  6.50361627e-02\\n  4.60827649e-02  1.11053661e-01  4.66385484e-02  5.21427672e-03\\n  3.00867036e-02  4.35024612e-02 -2.17382386e-02  4.31892537e-02\\n -4.51776013e-02  8.08089972e-03 -1.12582557e-02 -1.41517799e-02\\n -3.10540535e-02 -9.34174508e-02  9.24494397e-03 -4.43336256e-02\\n -1.65357497e-02 -1.06550800e-02  4.59747724e-02  4.41709097e-04\\n -2.43982654e-02 -1.37928722e-03 -5.11116162e-02  3.24725248e-02\\n -2.79314462e-02 -1.08810142e-01  7.03460677e-03  5.18177971e-02\\n -1.27704544e-02 -4.31569666e-02 -1.84735805e-02 -2.88625732e-02\\n -2.36387067e-02  3.03052180e-02 -1.55847054e-02  5.47396541e-02\\n -5.73904626e-02 -1.52741466e-02  1.17145712e-03 -2.01324783e-02\\n -1.84020959e-02 -3.30732577e-03 -3.33997421e-02 -3.46921831e-02\\n -5.07658534e-03  2.15896443e-02 -5.97956404e-02  7.71709671e-03\\n -4.27950919e-02 -3.53851281e-02  1.59969572e-02  1.15624825e-02\\n  1.16521381e-02  2.63900086e-02  1.48042478e-02  1.15963444e-02\\n  4.31528278e-02 -3.34773585e-02 -5.45618907e-02 -5.28165027e-02\\n  3.61400028e-03  1.28038451e-01  5.21506229e-03 -6.79058135e-02\\n -3.23319295e-03  3.43262330e-02 -5.67843486e-03 -4.98536276e-03\\n -1.46661163e-03 -6.83617173e-03  2.29102746e-02 -6.66983351e-02\\n -9.62069258e-03 -2.21499130e-02  1.27106644e-02 -3.56846079e-02\\n  4.73599620e-02 -7.48047372e-04 -5.20851724e-02  1.89733896e-02\\n  8.37648660e-03 -2.30485247e-03 -1.22454278e-02 -1.02635787e-03\\n  5.02331033e-02 -3.98079380e-02  7.40530388e-03  1.31922017e-03\\n -5.78866899e-03 -5.13459649e-03 -6.59428025e-03 -3.74519676e-02\\n -2.37687072e-03  2.85434397e-03 -1.02149667e-02 -2.31712144e-02\\n -9.84526612e-03 -7.56526086e-03 -5.54439984e-03 -5.01874425e-02\\n  5.47621623e-02  3.37536745e-02 -2.45243460e-02  8.66343081e-02\\n  1.20955380e-02 -3.00963633e-02 -2.08734926e-02 -2.99824518e-03\\n -1.56362522e-02  1.90147273e-02  6.44094944e-02 -2.39453930e-02\\n -1.63215324e-02  1.87341496e-02 -1.27530200e-02  6.31427253e-03\\n  1.00920098e-02 -5.22192605e-02 -1.97021607e-02 -1.45687778e-02\\n -2.80536991e-02  3.25331055e-02  3.07022594e-02  4.73596342e-02\\n  1.24340421e-02  6.11399375e-02 -1.79761760e-02  2.19362602e-02\\n -1.85450669e-02  1.11041740e-02 -2.04299651e-02  1.78876426e-02\\n  1.60898026e-02  3.16709578e-02  2.01780014e-02 -2.43221447e-02\\n  3.60836498e-02  8.63488913e-02  6.10896125e-02 -4.21513021e-02\\n -3.16137960e-03 -4.66386043e-02 -3.57425474e-02  2.93044224e-02\\n -5.55926794e-03 -1.09186806e-01  7.75307268e-02  3.78103065e-03\\n  2.51772553e-02 -8.20818078e-03 -7.27878660e-02  2.37340201e-03\\n -1.34122511e-02 -7.70053361e-03  3.24653997e-03 -9.20703169e-03\\n -3.22230123e-02  1.21507039e-02  4.01023738e-02  1.46951713e-02\\n -7.14482460e-03  4.52356786e-02 -1.98258758e-02 -2.15822691e-03\\n  7.14640878e-03  2.17115786e-02 -6.11122362e-02  4.18650508e-02\\n -6.15490077e-04 -2.29513738e-03  3.04608308e-02 -2.26320680e-02\\n -2.36074738e-02 -2.45939754e-02  1.85582824e-02 -1.34632760e-03\\n  1.60954858e-03 -8.97450559e-03 -4.13363129e-02 -2.58436408e-02\\n -4.75451536e-03 -1.85428746e-02 -1.30560156e-02  3.21700685e-02\\n  4.75235991e-02  4.79454361e-03 -1.04272487e-02 -1.35769350e-02\\n -4.40250337e-02  2.78361589e-02  3.34912501e-02 -3.87185663e-02\\n -1.91140361e-02  3.54469046e-02 -3.55662331e-02 -1.63598508e-02\\n  8.89183371e-04  3.77261005e-02 -6.43638102e-03 -4.15266752e-02\\n -2.08608098e-02  1.32054940e-03 -2.00753901e-02 -3.29486583e-03\\n  2.59225536e-02 -4.22484167e-02  1.34021286e-02  1.73823535e-02\\n -1.60640161e-02 -6.50242344e-02 -3.95130143e-02  1.84906982e-02\\n -2.90925838e-02  3.14917043e-02 -8.56340025e-03 -4.33891416e-02\\n -7.70066679e-02 -1.10990992e-02 -4.79001924e-02  7.33347535e-02\\n  1.79023631e-02 -4.47985269e-02  3.75869614e-03 -4.12971005e-02\\n  1.94357354e-02 -8.10089484e-02 -3.45684253e-02 -3.01846955e-02\\n -3.92434821e-02  8.75717402e-03  1.22023240e-01  3.49047072e-02\\n -9.55803916e-02 -3.11785266e-02  3.31065170e-02  2.25540176e-02\\n  4.92523126e-02 -4.90366621e-03  6.37521520e-02 -3.86036467e-04\\n  7.34681089e-04 -1.90830473e-02  3.03306878e-02 -6.11602841e-03\\n  2.50781048e-02 -3.67358234e-03  9.28337723e-02 -6.08497933e-02\\n -2.72186976e-02 -1.49926394e-02 -5.15598655e-02  1.01991044e-02\\n  2.98740864e-02  1.18085267e-02 -4.19253297e-02 -5.91518134e-02\\n -4.71780337e-02  1.34821776e-02  3.56141781e-03 -3.08360402e-02\\n  4.44758944e-02 -4.12166379e-02 -3.24938633e-02 -2.25248504e-02\\n -6.93235407e-03 -5.01844566e-03 -7.98589643e-03  5.16903698e-02\\n -1.63563865e-03  1.19405733e-02  1.51357194e-03 -5.04980274e-02\\n -2.00167429e-02  2.83489618e-02  1.76667478e-02 -3.87550369e-02\\n -3.59035023e-02  2.77615190e-02 -4.17661369e-02  1.19218314e-02\\n -4.26960960e-02 -4.09984626e-02 -3.42793092e-02 -1.61223374e-02\\n  6.30824789e-02  2.82908659e-02 -5.75914420e-03  1.38067408e-02\\n -6.67351931e-02  7.74249434e-02  3.72126885e-02  2.61049774e-02\\n -1.40074492e-02  3.20887156e-02 -4.93899919e-02  4.65241820e-03\\n -6.62398990e-03 -9.86952242e-03  4.53902446e-02  2.82116160e-02\\n -6.61552772e-02 -2.18110364e-02 -3.94090824e-02  3.47615518e-02\\n  5.28040156e-02  4.94500510e-02  1.17049897e-02 -3.53587382e-02\\n -3.34332464e-03  2.22145952e-03  3.67162563e-02 -3.10925604e-03\\n -1.13184387e-02 -4.29489650e-02  3.30951251e-02  3.11428234e-02\\n  2.72926446e-02  2.24189106e-02  8.76997132e-03 -3.58541720e-02\\n -9.38192289e-03  6.46914020e-02 -4.28646011e-03  4.02660184e-02\\n -1.53433224e-02  1.83807090e-02  5.32548502e-03  3.70962918e-02\\n -1.32780904e-02 -2.61736591e-03  2.06391606e-02 -1.29843960e-02\\n -1.51088769e-02  5.38908318e-02 -1.70647353e-02 -3.66821792e-03\\n -2.57321782e-02 -7.15297759e-02  1.03582861e-02  7.82388002e-02\\n  6.14666343e-02 -1.83921959e-03 -2.40946729e-02 -1.90416928e-02\\n  2.85514467e-03  1.88184611e-03 -6.25277730e-03 -1.02928625e-02\\n  5.66391647e-02 -2.18711011e-02 -5.33076329e-03 -1.56737007e-02\\n -3.22021879e-02  5.41837141e-02  3.08106560e-02  4.85921726e-02\\n -7.06187785e-02 -2.63926182e-02  5.18244738e-03 -1.48358969e-02\\n  6.54052794e-02  1.39387101e-02  3.44292112e-02 -2.00634655e-02\\n  4.19589179e-03 -3.44607234e-02  3.50272208e-02 -1.25153763e-02\\n  5.88577129e-02 -7.82740936e-02  1.42769460e-02  9.97870192e-02\\n  2.61816708e-03  1.99690126e-02 -2.91002374e-02  2.43756138e-02\\n  2.68520992e-02 -4.68628714e-03  5.97348809e-03 -6.30805680e-33\\n  2.31222827e-02 -1.02752604e-01  2.69557144e-02  4.49554026e-02\\n -2.62928922e-02 -2.79582595e-03  1.33258170e-02 -2.05779057e-02\\n  8.48944392e-03  1.51811671e-02 -2.04266254e-02 -2.22282927e-03\\n  9.13293939e-03  2.82515436e-02  1.01381820e-02 -2.58649886e-02\\n -2.66055553e-03 -5.04419655e-02  4.48780842e-02  9.40638687e-03\\n -1.77172590e-02  3.65739986e-02 -1.82840284e-02 -4.52906527e-02\\n  5.11585660e-02 -2.31560040e-02 -8.29914119e-03 -2.15798877e-02\\n -2.73653436e-02  4.70344871e-02 -6.52251672e-03  3.18011008e-02\\n  4.10657525e-02 -5.90067394e-02 -7.75384251e-03  7.49053061e-02\\n -5.28478203e-03 -1.62039436e-02 -4.03322233e-03 -6.68319594e-03\\n -3.87672707e-02 -8.75613913e-02  7.36113787e-02  2.37533320e-02\\n -3.11458781e-02  3.44227180e-02  4.10970226e-02 -4.90339771e-02\\n  9.19631869e-03 -9.91956890e-02 -6.48821704e-03 -4.28257231e-03\\n -2.79951338e-02  3.84597480e-02  5.23428693e-02 -4.29222407e-03\\n -3.62648107e-02  6.79731220e-02 -3.55431400e-02  3.34523469e-02\\n  4.65751924e-02  3.24413250e-03  5.37752360e-02  3.09747513e-02\\n -2.10398361e-02  3.40096690e-02  9.47176293e-03 -3.28636691e-02\\n -6.38304949e-02  3.93983629e-03 -6.03914633e-03  1.02703854e-01\\n  2.38453653e-02  4.24334891e-02  4.93073240e-02 -3.15520242e-02\\n -7.84524977e-02  2.75965631e-02 -3.13129686e-02 -8.94972216e-03\\n -7.61359977e-03 -3.55983875e-03 -1.02205537e-02 -4.44424897e-03\\n  1.05167953e-02 -5.18750586e-02 -2.14914400e-02 -2.95026395e-02\\n  6.58638962e-03 -1.28484117e-02 -2.85058692e-02  7.65544269e-03\\n -9.61768162e-03 -5.69602326e-02  3.10619716e-02 -1.69285517e-02\\n  3.62233780e-02  1.45490114e-02  2.51963604e-02 -6.94087567e-03\\n -1.72400766e-03 -6.92652017e-02 -5.14646135e-02 -8.28993320e-03\\n  4.98957336e-02  8.42811260e-03 -2.54367874e-03  2.82798875e-02\\n -2.14849524e-02  9.35970526e-03  9.31624696e-03  1.04309106e-02\\n  4.77955192e-02  1.53897610e-02 -3.32212262e-02 -4.76092324e-02\\n  2.06928980e-02  2.70479731e-02 -1.24125006e-02  8.28694925e-03\\n -2.72509698e-02 -1.66793261e-02 -5.18736541e-02  2.60695461e-02\\n -5.16104549e-02 -8.86474829e-03 -6.40612245e-02  1.91096850e-02\\n  4.56522368e-02 -8.89062658e-02 -7.43481377e-03  1.57536399e-02\\n  2.86843800e-07 -1.49165411e-02  6.26372695e-02  3.96631882e-02\\n  3.54778916e-02  5.39485738e-02  4.94935177e-02 -4.08151280e-03\\n -1.12951187e-04  1.39142731e-02 -1.39211714e-02  2.60480549e-02\\n  1.60162896e-02  8.68574437e-03 -1.75726544e-02 -4.64339443e-02\\n -6.51523797e-03 -5.11532687e-02  8.24644696e-03 -4.70484383e-02\\n  3.15333903e-02  3.68827954e-02  4.48749103e-02  1.23530105e-02\\n -2.44882535e-02 -1.80572588e-02 -6.95015043e-02  1.47904027e-02\\n -2.25711763e-02  5.28138950e-02  2.47856844e-02  9.54876840e-02\\n  1.22020207e-03  3.03511806e-02  3.50162387e-02 -2.77595837e-02\\n -2.55976040e-02 -9.60891042e-03  7.00105727e-02  6.56479923e-03\\n  8.04800366e-04  1.20721189e-02  4.67617298e-03  6.06802590e-02\\n -2.12821495e-02  5.41327745e-02 -2.05528494e-02 -3.26745585e-02\\n -3.77762727e-02  3.84009406e-02 -6.50224164e-02  1.04790619e-02\\n -2.82731988e-02  1.09892124e-02  1.69748086e-02 -1.67285949e-02\\n -3.37240398e-02  3.76252122e-02 -1.98898595e-02  2.10822262e-02\\n  2.38192594e-03 -4.86445650e-02 -3.16946232e-03 -2.28051674e-02\\n -4.01666798e-02  6.16493933e-02  3.34274918e-02  6.97709387e-03\\n  2.56198420e-34  2.90177064e-03 -3.01091261e-02 -2.14749109e-02\\n  5.44389747e-02 -1.10015571e-02  1.16628353e-02 -2.88219191e-02\\n  3.49098369e-02 -1.29646165e-02 -9.05379094e-03 -4.47377423e-03]'},\n",
       " {'page_number': 1,\n",
       "  'sentence_chunk': 'While the field of AI is now dominated by machine learning and deep learning, it also includes other approaches—for example, using rule-based systems, genetic algorithms, expert systems, fuzzy logic, or symbolic reasoning. Returning to the spam classification example, in traditional machine learning, human experts might manually extract features from email text such as the frequency of certain trigger words (for example, “prize,” “win,” “free”), the number of exclamation marks, use of all uppercase words, or the presence of suspicious links. This dataset, created based on these expert- defined features, would then be used to train the model. In contrast to traditional machine learning, deep learning does not require manual feature extraction. This means that human experts do not need to identify and select the most relevant features for a deep learning model. ( However, both traditional machine learning and deep learning for spam classification still require the collection of labels, such as spam or non-spam, which need to be gathered either by an expert or users.) Let’s look at some of the problems LLMs can solve today, the challenges that LLMs address, and the general LLM architecture we will implement later. Applications of LLMs Owing to their advanced capabilities to parse and understand unstructured text data, LLMs have a broad range of applications across various domains. Today, LLMs are employed for machine translation, generation of novel texts, sentiment analysis, text summarization, and many other tasks. LLMs have recently been used for content creation, such as writing fiction, articles, and even computer code.',\n",
       "  'sentence_chunk_size': 1649,\n",
       "  'sentence_chunk_word_count': 252,\n",
       "  'sentence_chunk_tokens': 412.25,\n",
       "  'embedding': '[ 7.31590614e-02  2.09242031e-02 -3.23481672e-02  2.98654828e-02\\n -7.80900791e-02  4.14614268e-02  1.22874174e-02  2.27400903e-02\\n  3.95770073e-02 -8.74450803e-02  1.29225096e-02 -1.70985702e-02\\n -4.83692810e-02  6.35862648e-02  2.36155977e-03 -1.60467345e-02\\n -1.31091578e-02 -2.59089358e-02  5.56993186e-02  7.96464086e-03\\n -6.97709061e-03  1.49963899e-02  1.33213913e-02 -3.76333715e-03\\n -5.04984632e-02  1.67496856e-02 -1.75119061e-02  9.20538884e-03\\n -1.02219349e-02  8.39601364e-03 -1.46155171e-02 -1.19091896e-02\\n  2.87242755e-02  1.06405895e-02  1.82455699e-06 -4.73701358e-02\\n -2.20540427e-02 -1.10230837e-02 -4.68499772e-02 -4.44586203e-02\\n  3.92535776e-02 -3.28985862e-02 -1.68422870e-02  1.15880286e-02\\n -5.36387116e-02  5.79250120e-02  2.34117601e-02  6.22888654e-02\\n  2.89121941e-02  1.02351725e-01 -1.99862253e-02 -4.95596714e-02\\n  6.26638532e-02  5.65919792e-03  1.76143236e-02 -2.30291695e-03\\n  3.61097232e-02 -6.39333278e-02 -2.20221728e-02  2.92444266e-02\\n  2.30983850e-02  2.47468948e-02 -6.43570791e-04 -1.76760962e-03\\n  1.38914119e-02  4.14964184e-03  1.13786981e-02 -2.36937627e-02\\n -4.29367535e-02  1.13675371e-02 -2.22138744e-02 -9.46002488e-04\\n  5.37937842e-02 -1.20649813e-02  2.42850073e-02 -1.41674718e-02\\n -1.90669354e-02  3.80644538e-02  1.29459077e-03  3.77653129e-02\\n  3.33660915e-02 -2.71503162e-02  1.67695135e-02  4.36462983e-02\\n -1.17052835e-03  4.76939008e-02 -4.26517986e-02 -2.07853075e-02\\n  1.30213760e-02  2.86742337e-02  5.92616275e-02 -5.69050536e-02\\n  5.02948910e-02  5.21715619e-02  5.95819205e-02  6.41977321e-03\\n -5.80554865e-02 -3.40568870e-02  3.08013335e-02 -6.12753630e-02\\n  9.05376859e-03 -3.03812679e-02 -3.77010368e-03  3.90804671e-02\\n -1.04081072e-02 -3.41462567e-02 -7.75574334e-03  4.32594828e-02\\n -1.58378985e-02  1.30771857e-03 -4.42439429e-02 -3.41826491e-02\\n  1.86935312e-03  9.19303596e-02  2.99560130e-02  2.04993766e-02\\n -3.70415822e-02  1.54837035e-02 -2.30242386e-02  2.77340021e-02\\n -2.43534576e-02  3.20379138e-02 -4.71542677e-04  1.95396002e-02\\n -7.51873329e-02 -4.31153141e-02 -1.13408463e-02  3.78062725e-02\\n  1.87192261e-02 -5.14634661e-02  3.89289223e-02  1.99203230e-02\\n  1.81157365e-02 -2.55664997e-02  6.70075938e-02  9.33833420e-02\\n  8.18836130e-03 -3.55918482e-02 -8.14561471e-02 -4.29398306e-02\\n -1.93212274e-02 -4.71167155e-02  3.10716536e-02 -4.35176678e-02\\n  3.26258960e-05 -1.21965201e-03 -3.74221280e-02 -4.51773554e-02\\n -2.26903129e-02  1.37180379e-02 -5.24441414e-02  2.50470974e-02\\n -1.99322283e-04 -1.51757905e-02  7.40841106e-02 -1.50194690e-02\\n  1.17182108e-02  1.70826179e-03  1.60917789e-02  8.62140805e-02\\n  3.84669825e-02 -3.14816460e-03  4.70868424e-02  2.87873670e-02\\n -2.53288895e-02 -2.36842390e-02 -2.10299324e-02 -2.20842510e-02\\n -8.12616274e-02  6.09378936e-03 -1.64883416e-02  4.50155325e-02\\n -1.25317753e-03 -4.79770498e-03  8.58506933e-02  6.39554337e-02\\n  1.85878947e-02  5.88504896e-02  3.67397107e-02 -3.15732276e-03\\n  3.07731926e-02  2.30856109e-02 -2.83982549e-02  1.24551952e-02\\n -3.30115482e-02 -4.26841248e-03 -3.06899771e-02  3.48564275e-02\\n -1.28984796e-02 -9.09869149e-02 -1.54217798e-02 -2.26735044e-02\\n  2.49860808e-03  1.90961827e-02  6.63157329e-02  1.03735528e-03\\n -9.19627398e-03  2.61085704e-02 -4.83704694e-02  3.33864661e-03\\n  1.17449407e-02 -7.01296851e-02 -1.59463997e-03  7.60686547e-02\\n  2.62342170e-02 -4.87222299e-02 -1.34353461e-02 -4.77754138e-02\\n  1.42375343e-02  7.86396954e-03  4.50738193e-03  4.62029390e-02\\n -3.67495157e-02 -1.38303023e-02 -2.24090163e-02 -6.70850184e-03\\n -2.31491346e-02 -2.27420889e-02 -2.56037097e-02  2.03783624e-03\\n  5.50127029e-03  2.01198738e-02 -6.75222874e-02  3.82981077e-02\\n  1.45232389e-02 -3.55538689e-02  1.64075196e-02  1.79835465e-02\\n  4.21070717e-02  7.03846812e-02 -1.23844538e-02 -4.74668713e-03\\n  5.14347702e-02 -5.49850687e-02 -5.55345155e-02 -5.08236736e-02\\n  4.12756624e-03  8.19013864e-02  4.24168631e-02 -6.82629794e-02\\n -1.75200365e-02  2.90585961e-02 -4.37845737e-02 -7.20353378e-03\\n -1.40536111e-03 -2.72315964e-02 -1.08549336e-03 -6.63584098e-02\\n -3.07472842e-03 -6.32004719e-03  8.98478366e-03 -4.79551554e-02\\n  4.93279882e-02 -5.37644722e-04 -5.24536334e-02 -1.63552742e-02\\n -2.63139401e-02 -1.04988622e-03 -2.04178542e-02 -1.87516082e-02\\n  7.94403162e-03 -1.74889900e-02  1.12992031e-02  1.56130397e-03\\n  7.39615411e-03 -4.30857800e-02 -2.29443088e-02 -5.52972779e-02\\n  9.52514261e-03 -7.44884310e-04  2.55346205e-02 -1.55347846e-02\\n -3.74017060e-02 -3.68855940e-03  9.40294284e-03 -7.31024742e-02\\n  6.81633083e-03  2.87467279e-02 -7.31863547e-03  7.45532662e-02\\n  5.05308481e-03 -2.82089389e-03 -3.00592165e-02 -8.00928741e-04\\n -1.11967297e-02  1.68473572e-02  6.02511391e-02 -2.05490999e-02\\n -8.95504374e-03  2.36532427e-02 -2.62697190e-02  2.31204703e-02\\n  1.21764792e-02 -4.12042774e-02 -2.35657934e-02 -1.98100582e-02\\n -5.80888875e-02  3.66469249e-02  3.13097835e-02  5.47546893e-02\\n  9.64884646e-03  2.50873826e-02 -2.83587985e-02  6.67531230e-03\\n -3.37738134e-02  3.25382501e-02 -4.79700789e-02  6.45386986e-03\\n  4.23048884e-02  2.19044276e-02  2.35746466e-02 -1.16371382e-02\\n  3.88720483e-02  6.01640530e-02  4.74138409e-02 -2.87254285e-02\\n -1.28877175e-03 -4.34057340e-02 -2.26357207e-02  4.94051985e-02\\n -8.92194640e-03 -7.47086853e-02  8.47401991e-02 -1.61780752e-02\\n  2.26721130e-02 -2.40650997e-02 -5.25607690e-02 -3.90614606e-02\\n  1.59747545e-02 -9.05043166e-03  2.27496587e-02 -2.46671923e-02\\n -2.61465218e-02  1.73682105e-02  1.38280690e-02  1.89175699e-02\\n -2.06376966e-02  3.65714468e-02 -2.93943509e-02  5.65982796e-03\\n  7.75048789e-03 -1.84880092e-03 -4.70565595e-02 -2.97408924e-02\\n -3.40349600e-02 -1.74555480e-02  3.10065057e-02  2.14370247e-03\\n -2.74041183e-02 -3.24381553e-02  2.61829216e-02  8.99787247e-03\\n  2.13513733e-03 -2.21501552e-02 -3.55770327e-02  1.90151408e-02\\n  1.28186047e-02 -1.22347502e-02 -8.90819542e-03 -4.37471754e-04\\n -1.08539723e-02  3.56853269e-02 -1.47388279e-02 -4.36868072e-02\\n -1.85641404e-02  3.78858633e-02  3.17582041e-02 -6.48262948e-02\\n -1.95103101e-02  1.38281975e-02 -1.59690063e-02 -4.26514708e-02\\n  4.31439728e-02  9.09298882e-02  2.74595246e-02 -4.14344110e-02\\n -3.59265809e-03 -4.49032057e-03 -4.39160764e-02 -8.42899550e-03\\n  1.86955389e-02 -6.82065785e-02 -5.43158827e-03  1.41593898e-02\\n  1.43120484e-03 -6.73830733e-02 -4.54707108e-02  1.00691589e-02\\n -4.98132631e-02  4.75596078e-02 -8.17892980e-03 -1.70609728e-02\\n -6.10240623e-02 -1.93950180e-02 -7.08144531e-02  4.70668748e-02\\n  2.73128529e-03 -4.17627804e-02 -8.66623502e-03 -4.18068469e-02\\n  2.06481032e-02 -5.88026345e-02 -1.78344063e-02 -2.28047185e-02\\n -4.58704606e-02  9.45030712e-03  8.07258710e-02  4.96861450e-02\\n -7.16092959e-02 -1.37150772e-02 -1.80720154e-03  2.78293751e-02\\n  6.53854683e-02 -3.35729979e-02  7.57663697e-02 -1.68838650e-02\\n  1.34051070e-02 -2.15879492e-02  3.63810249e-02 -4.28353623e-02\\n  1.69066042e-02  6.28826581e-03  1.03103913e-01 -5.69979735e-02\\n -3.95617038e-02 -3.37880477e-02 -3.93485203e-02 -1.24513702e-02\\n  5.32018095e-02  2.00191094e-03 -4.86697517e-02 -7.47602582e-02\\n -4.57124002e-02 -7.58518325e-03 -1.00505771e-03 -2.52176747e-02\\n  2.89723985e-02 -3.46351229e-02 -5.16411960e-02 -5.89649230e-02\\n -2.24541463e-02  4.06560116e-02  8.90447572e-03  4.14662547e-02\\n  3.30643542e-03 -6.04614150e-03  5.83574874e-03 -4.97275554e-02\\n -4.58332431e-03  4.23912052e-03  1.84736922e-02 -2.96924617e-02\\n -1.22936107e-02  3.64033692e-02 -6.00548945e-02  1.42774191e-02\\n -2.84700710e-02 -4.87006307e-02 -4.24014069e-02 -4.51643206e-03\\n  4.24131677e-02  2.46324018e-02  4.11190204e-02  1.13507574e-02\\n -9.63250473e-02  8.44339356e-02  5.72050512e-02 -3.25644426e-02\\n -1.78482961e-02  3.48068886e-02 -1.74774453e-02  1.75306927e-02\\n -3.87531915e-03 -2.69427989e-02  3.45465466e-02  4.95560206e-02\\n -6.66212887e-02 -3.45351212e-02 -4.77981567e-02  6.04953011e-03\\n  5.68048619e-02  2.83795763e-02  1.81707311e-02 -4.28545382e-03\\n -1.44934738e-02 -2.19534687e-03  2.60428898e-02 -9.99222603e-03\\n -2.10991316e-02 -1.11741330e-02  2.93242600e-04  2.46453211e-02\\n  6.22776151e-03  2.27186568e-02 -2.34116577e-02 -7.66108162e-04\\n  4.56517516e-03  4.45765927e-02 -1.38705689e-02  3.98336910e-02\\n -2.73700990e-03  4.51422669e-03 -9.75696370e-03  4.96625789e-02\\n -1.78387631e-02  9.90886148e-03  2.91167479e-03 -2.99970657e-02\\n -1.27524203e-02  3.87949981e-02 -1.90043766e-02  1.90774873e-02\\n -4.15257923e-02 -3.31965387e-02 -1.12060169e-02  2.87267957e-02\\n  5.56203015e-02 -1.84045453e-02  8.74030031e-03 -4.17509936e-02\\n  2.15689652e-03  3.18303914e-03  3.91349988e-03  3.27663874e-05\\n  2.95064505e-02 -3.02913729e-02  7.70135922e-03 -3.85253504e-02\\n -1.77560076e-02  5.27945273e-02  1.51189268e-02  9.44770221e-03\\n -4.00097203e-03 -6.38416633e-02 -2.25300733e-02 -4.12851246e-03\\n  5.31640463e-02  3.95422103e-03  2.87480894e-02 -1.34453736e-02\\n -1.90297365e-02 -3.72667611e-02  6.71639144e-02 -6.14268286e-03\\n  2.05033533e-02 -5.39843999e-02  3.59179154e-02  9.83613282e-02\\n  1.31417997e-02  3.34044509e-02 -5.32814190e-02  2.06420273e-02\\n  2.69429460e-02 -3.30939069e-02 -3.40819010e-03 -6.02585037e-33\\n  5.45647293e-02 -6.44420311e-02  7.30619160e-03  7.10601509e-02\\n -1.25707323e-02 -1.14254595e-03  1.79585759e-02 -7.10586738e-03\\n  1.53449606e-02  5.77231497e-03 -3.55117172e-02 -1.68127306e-02\\n  1.61093697e-02  1.35470172e-02  2.94636432e-02 -3.54577862e-02\\n  3.24016437e-02 -3.78142819e-02  2.08365321e-02  1.59985549e-03\\n  7.74693210e-03  4.92041446e-02  4.83480245e-02 -8.66733193e-02\\n  4.77154329e-02  3.80650308e-04 -8.21546372e-03  2.75514903e-03\\n -7.68055022e-03  3.49439457e-02 -4.47226595e-03  3.82147767e-02\\n  5.15053682e-02 -4.33016904e-02  2.78191082e-03  4.95464634e-03\\n -3.28106992e-02 -2.25949083e-02  4.87166829e-03  1.43905059e-02\\n  7.22095836e-03 -8.67595673e-02  1.11170642e-01  1.27384579e-02\\n -1.36542153e-02  3.58339511e-02  4.95327190e-02 -4.65460829e-02\\n  4.43398720e-03 -8.63637254e-02  2.10138620e-03  9.49957594e-03\\n -2.81458478e-02  4.36519347e-02  4.62209135e-02  5.33946836e-03\\n -3.08878366e-02  5.83580211e-02 -6.13794737e-02  2.70718820e-02\\n  4.13466953e-02  2.16055661e-03  8.04355741e-02 -1.68311559e-02\\n -2.60008276e-02  6.67191520e-02  1.34484163e-02  7.43101770e-03\\n -4.53911312e-02  1.78529546e-02  1.18085323e-02  8.45389143e-02\\n  3.33911069e-02  5.88498078e-02  3.33748944e-02 -7.14155100e-03\\n -5.72765917e-02  4.25532237e-02  1.34215802e-02 -1.37717528e-02\\n  1.43885454e-02 -7.37300329e-03 -4.09895740e-02 -1.08487606e-02\\n  1.07219657e-02  6.43874705e-03 -2.17836238e-02 -4.21121344e-02\\n -1.03671784e-02 -4.47792560e-03 -1.93205085e-02  7.78497569e-03\\n -1.77973206e-03 -3.29767875e-02  4.24186420e-03 -4.42546327e-03\\n  1.64198186e-02  3.10967211e-02  2.60284729e-02 -4.31684479e-02\\n -2.38939188e-02 -5.91008104e-02 -3.11810654e-02 -4.03756760e-02\\n  3.41805965e-02 -6.02163980e-03  7.63116684e-03  4.23808619e-02\\n -1.63974017e-02  1.36590970e-03  2.29629893e-02  3.41370329e-02\\n  4.15806621e-02  3.39412689e-02 -4.51425537e-02 -2.05845088e-02\\n  1.79404337e-02  4.54379693e-02  7.41521316e-03 -2.28824094e-03\\n -2.61548087e-02  3.02330330e-02 -1.70527585e-03  2.30390974e-03\\n -4.22558859e-02 -2.36634798e-02 -3.12203281e-02 -6.62311865e-03\\n  3.24854665e-02 -8.29157382e-02 -5.74680511e-03 -1.28936423e-02\\n  2.64809529e-07  3.87508161e-02  5.71367256e-02  3.14731710e-02\\n  3.82276648e-03  4.05449457e-02  6.27917945e-02  2.16967426e-02\\n  1.53892348e-02 -4.88370704e-03 -1.82003118e-02  3.61439399e-02\\n  5.00592105e-02  3.11274435e-02  2.87063010e-02 -7.32788444e-02\\n -1.37102231e-02 -3.73806991e-02 -2.38435920e-02 -7.12506995e-02\\n  3.56316268e-02  7.49519318e-02  7.80270547e-02  1.65662486e-02\\n -6.14597350e-02 -1.92082208e-02 -5.48003130e-02  1.72444824e-02\\n -3.80473360e-02  1.38464449e-02  1.12180095e-02  1.12270005e-01\\n -1.34546431e-02 -4.19928832e-03  2.34617107e-02 -2.15957668e-02\\n  9.37316474e-03 -1.86576787e-02  5.97703680e-02 -6.32874388e-03\\n -3.59011218e-02  1.75236017e-02 -1.96319539e-02  2.67333426e-02\\n -9.42856167e-03  7.32602999e-02 -2.24032868e-02 -4.28909287e-02\\n -3.48792076e-02  3.94265838e-02 -7.22793713e-02 -1.45693524e-02\\n -4.05606590e-02  8.00458901e-03  2.54973099e-02  2.79118121e-03\\n -3.76201123e-02  1.57903973e-02 -4.39450145e-02  2.17441823e-02\\n  1.55695798e-02 -3.73889655e-02 -8.89580697e-03 -1.24023734e-02\\n  1.65872350e-02  4.98578250e-02 -8.67665466e-03  1.69597398e-02\\n  2.43712007e-34  3.63920927e-02 -1.40247298e-02 -1.57869738e-02\\n  5.61983921e-02  2.10311591e-05  5.41924611e-02 -3.77015211e-02\\n  2.51129344e-02 -1.51029350e-02 -5.37726730e-02  1.04186423e-02]'},\n",
       " {'page_number': 1,\n",
       "  'sentence_chunk': 'LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s ChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries and augment traditional search engines such as Google Search or Microsoft Bing. Moreover, LLMs may be used for effective knowledge retrieval from vast volumes of text in specialized areas such as medicine or law. This includes sifting through documents, summarizing lengthy passages, and answering technical questions. In short, LLMs are invaluable for automating almost any task that involves parsing and generating text. Their applications are virtually endless, and as we continue to innovate and explore new ways to use these models, it’s clear that LLMs have the potential to redefine our relationship with technology, making it more conversational, intuitive, and accessible. We will focus on understanding how LLMs work from the ground up, coding an LLM that can generate texts. You will also learn about techniques that allow LLMs to carry out queries, ranging from answering questions',\n",
       "  'sentence_chunk_size': 1058,\n",
       "  'sentence_chunk_word_count': 161,\n",
       "  'sentence_chunk_tokens': 264.5,\n",
       "  'embedding': '[ 3.67642753e-02 -1.54117467e-02 -6.60755020e-03  2.11120714e-02\\n -3.41741927e-02  7.03067007e-03 -1.92803368e-02  4.02046964e-02\\n  3.62894274e-02 -5.38997278e-02  1.09346583e-02 -8.92841723e-03\\n -2.92780120e-02  2.55245902e-02  2.33073290e-02 -5.16148582e-02\\n  3.46521698e-02 -8.02284293e-03  6.32489147e-03  1.90995191e-03\\n -1.99605059e-03  1.74862519e-02 -2.16707028e-02  3.71433459e-02\\n -1.31964050e-02 -2.43168678e-02 -3.52548137e-02  2.72661559e-02\\n  8.77920014e-04 -2.58808788e-02 -2.76311915e-02  2.15329253e-03\\n -1.35984225e-03  4.55075235e-04  1.72287628e-06 -6.77806735e-02\\n -2.88325325e-02  5.32249594e-03 -3.05009261e-02 -4.14227284e-02\\n  4.44306768e-02  2.30621938e-02 -2.03853957e-02  3.38661484e-02\\n -4.94954772e-02  4.64232042e-02  2.68281028e-02  2.61238515e-02\\n  5.76881804e-02  1.11139901e-01 -5.03265951e-03 -1.35976942e-02\\n  3.49710099e-02  1.42692039e-02  6.24887533e-02 -3.93889062e-02\\n  8.81713908e-03 -7.39379786e-03  5.85559458e-02  1.27594154e-02\\n  3.61763239e-02  3.13256681e-02 -6.57903310e-03 -2.46475730e-02\\n  2.81512458e-02  3.37632410e-02 -2.15163641e-02 -3.81070338e-02\\n -3.31684798e-02  4.38434519e-02  4.11743708e-02  5.37808868e-04\\n  5.00464216e-02  1.54195586e-02  5.59792155e-03  2.10060924e-02\\n -6.47220984e-02  5.38452864e-02 -1.00672022e-02  2.06724666e-02\\n  6.82492228e-03 -1.47915976e-02  2.41617276e-03  5.50470594e-03\\n -1.67421009e-02  4.70491499e-02 -2.67859758e-03 -2.34076586e-02\\n -1.95171032e-02  1.99070033e-02  5.15461490e-02 -7.64500946e-02\\n  3.78468111e-02  1.09345913e-01 -2.85460893e-03  1.76934209e-02\\n -3.85839939e-02 -1.30461389e-02  5.44515662e-02 -3.50590684e-02\\n  2.91773374e-03  2.88305394e-02 -2.42293980e-02  3.59829031e-02\\n -2.12203581e-02 -3.54151963e-03 -1.17873130e-02  2.87877265e-02\\n -3.98980975e-02 -1.75467096e-02 -4.24760506e-02 -2.86121201e-02\\n -2.59521939e-02  7.43474364e-02 -4.62905020e-02 -2.34015416e-02\\n -1.52708041e-02  6.53318539e-02  3.65923531e-02  3.46650779e-02\\n  7.88231939e-03  1.18492255e-02 -1.98613256e-02  1.53320171e-02\\n -5.76671325e-02 -2.53024232e-02 -1.77647769e-02 -1.66423172e-02\\n  2.83834767e-02 -8.38433877e-02  2.77811708e-03  2.75263488e-02\\n -1.21149141e-03  5.58613508e-04  7.57632544e-03  1.04436144e-01\\n -4.88855271e-03 -3.20213474e-02 -5.73587380e-02 -6.49709255e-02\\n  5.15525416e-03 -4.04532105e-02  2.07049269e-02 -7.13493377e-02\\n  1.21851051e-02  1.11863259e-02 -3.04072965e-02 -1.68929454e-02\\n  3.65025997e-02  5.41270301e-02 -3.25955376e-02  3.83700281e-02\\n -5.76919653e-02  1.73518211e-02  4.61096987e-02 -1.61722430e-03\\n  1.18525382e-02  5.15784547e-02  5.59783168e-02  3.24198827e-02\\n  4.83485758e-02  2.56243460e-02  2.50370577e-02  3.44024301e-02\\n -9.17508639e-03 -2.70663071e-02 -9.16771311e-03 -5.31693222e-03\\n -3.50751877e-02  4.83574793e-02 -1.52254722e-03  6.19565547e-02\\n -1.47270653e-02  2.51642279e-02  5.13701700e-02  1.12249270e-01\\n  3.23881693e-02  3.57720740e-02  2.87458822e-02 -1.34563511e-02\\n  2.13675369e-02  1.21310381e-02 -2.80611515e-02  5.10132685e-02\\n -2.89622042e-02  2.11683344e-02  1.47724329e-02  2.71775611e-02\\n -1.73918363e-02 -6.01031035e-02 -2.63380539e-02 -1.24100992e-03\\n -2.29365584e-02 -1.41419759e-02  2.75680963e-02  5.73517056e-03\\n -2.03851461e-02  5.77230044e-02  7.87846465e-03 -8.32209438e-02\\n -1.51774073e-02 -7.47322887e-02  2.25422550e-02  9.50251594e-02\\n  6.77511990e-02 -4.39322107e-02 -3.55161466e-02  3.96748492e-03\\n -1.63240470e-02  6.82270378e-02  3.10284458e-02  3.53554037e-06\\n -4.60623279e-02 -4.23964038e-02 -1.68997273e-02  1.08148213e-02\\n -1.17480811e-02 -1.40591543e-02  2.93016783e-04  1.11729437e-02\\n -3.23307526e-04 -3.20086442e-02 -6.64323289e-03  4.70654108e-02\\n -3.64473872e-02 -2.34498028e-02 -1.08090155e-02 -4.40342398e-03\\n -7.44186295e-03  5.25711551e-02 -4.74583730e-03  4.01880257e-02\\n  7.39463493e-02 -1.68188885e-02 -2.65422016e-02  8.26817937e-03\\n -7.95490388e-03 -7.84849282e-03  5.41330501e-02 -5.23655042e-02\\n  2.12911633e-03 -8.09108606e-04 -1.91439390e-02 -4.96450178e-02\\n -4.29095179e-02 -5.74349053e-02  4.10968550e-02 -4.10891324e-02\\n -3.56482784e-03  3.97371426e-02 -2.11523566e-02 -4.33580838e-02\\n  5.53395301e-02 -9.15340323e-04 -8.70684907e-03  3.48801911e-02\\n  1.23745594e-02 -1.11949176e-03 -7.72825927e-02  1.17164794e-02\\n  1.85241159e-02  1.37965782e-02  6.21929881e-04  1.52288564e-03\\n  1.80571470e-02 -4.98024374e-02 -3.69098932e-02 -9.44217965e-02\\n  1.53807774e-02  2.97173113e-03  1.69963054e-02  2.10866854e-02\\n  8.28659069e-03 -1.13773858e-02 -9.45543312e-03 -7.16505051e-02\\n  1.17038973e-02  3.85171324e-02 -1.06414445e-02  7.73984864e-02\\n  2.99810525e-03 -1.36411898e-02 -4.09596488e-02  1.56340580e-02\\n -2.39753388e-02  1.35838715e-02  2.64132954e-02 -5.27173355e-02\\n -4.81414199e-02  3.80816981e-02 -4.03203182e-02  1.03857918e-02\\n  9.60153248e-03 -1.07381726e-02  3.61765511e-02 -5.32562211e-02\\n  9.44749266e-03  7.79115260e-02  3.30413096e-02  4.10456285e-02\\n  3.31342295e-02 -1.49681000e-02 -6.49787188e-02  1.71278007e-02\\n -2.21182182e-02  1.87863421e-03 -1.64844766e-02 -4.35655527e-02\\n  1.86361130e-02  4.88726608e-02  8.25139973e-03  1.91864576e-02\\n -1.43662747e-02  9.01457667e-03  4.06294651e-02 -1.77237708e-02\\n  1.59764779e-03  3.87383439e-02 -5.54447994e-02  6.59650490e-02\\n  2.11090501e-03 -5.64716645e-02  5.68576753e-02 -4.13878858e-02\\n  4.16091038e-03 -3.67604457e-02 -2.92634517e-02 -4.79721874e-02\\n  5.41460924e-02 -2.94230636e-02  1.79474484e-02 -1.38073806e-02\\n -2.59571392e-02  4.82142763e-03  1.33909741e-02 -8.60109983e-04\\n -6.58196211e-02  6.29415438e-02 -3.46004181e-02  1.25075569e-02\\n -1.96519513e-02 -1.53488247e-02 -4.76271398e-02  6.67214114e-03\\n -2.65968423e-02  1.05135376e-02  2.21490283e-02 -3.08364746e-03\\n -5.98107539e-02 -2.05571409e-02  2.08169352e-02  3.34020326e-04\\n -1.64365172e-02 -1.96765829e-02 -5.09243309e-02  1.80889964e-02\\n  2.53831167e-02  3.61878201e-02  5.25561953e-03  1.33067239e-02\\n  3.48370261e-02  7.33640417e-02 -2.92675942e-03 -1.66693535e-02\\n -5.66023169e-03 -9.32096317e-03  3.43964286e-02 -6.06131852e-02\\n -3.82531574e-03  3.05140633e-02  3.68012562e-02 -3.94158438e-02\\n  3.84012354e-03  1.16750896e-01  3.44380997e-02 -4.50719707e-02\\n -7.97731522e-03 -4.79571410e-02 -1.10840118e-02  1.92941837e-02\\n  2.94260066e-02 -1.14665106e-01 -1.11190826e-02  1.83530375e-02\\n  4.41586524e-02 -2.27676760e-02 -3.33920605e-02 -3.16466601e-03\\n -9.59463343e-02  6.30815774e-02 -4.28980595e-04  1.57368518e-02\\n -5.46374067e-04 -1.94717683e-02 -1.92072261e-02  2.18229294e-02\\n -6.02786615e-03  3.11635174e-02 -2.92427018e-02  3.92556423e-03\\n  2.06948016e-02 -2.65906248e-02  1.42369252e-02 -3.46231945e-02\\n -1.30464081e-02  3.75643112e-02  8.99247378e-02  5.53844795e-02\\n  2.67867837e-03 -2.66825873e-02 -1.55633492e-02  2.27778312e-02\\n  3.34279388e-02 -3.73396985e-02  3.79882157e-02  8.82919936e-04\\n  1.23845681e-03 -2.31872313e-02  3.57862152e-02 -1.86345279e-02\\n -1.70792788e-02  1.14115737e-02  4.53146026e-02 -5.89463748e-02\\n  3.43330670e-03 -4.93084714e-02  2.47560870e-02 -4.45581647e-03\\n  7.45931789e-02  2.25541312e-02  1.95641015e-02 -6.30689189e-02\\n -6.71461225e-02  8.56785220e-04 -3.29048298e-02 -3.87022458e-02\\n  4.08002455e-03 -2.69831195e-02 -1.04609188e-02 -9.47244912e-02\\n -1.95498057e-02  3.09597217e-02  1.34271225e-02  2.49918480e-03\\n  9.61480569e-03  1.02459332e-02  2.32470632e-02 -4.28375900e-02\\n -2.03481875e-02 -2.19832920e-02 -1.83888606e-03 -5.60829192e-02\\n -4.64262106e-02  6.12163767e-02 -4.72023934e-02  1.51760748e-03\\n -1.23779289e-02 -4.56146896e-02 -6.26187101e-02 -2.79126130e-02\\n  4.79650833e-02  2.84394026e-02  1.31936623e-02  1.57483184e-04\\n -2.76742578e-02  4.08411324e-02  9.01612937e-02 -3.43038328e-02\\n  2.29933131e-02  6.49067527e-03  5.51511049e-02  7.42878113e-03\\n  1.07571464e-02 -6.15662150e-02  6.30177837e-03 -3.73963639e-02\\n  2.98980884e-02 -4.08625416e-02 -7.50886351e-02  2.96916049e-02\\n  6.93173334e-02 -1.08900443e-02 -6.84910594e-03  2.02011112e-02\\n  7.16266409e-03  1.49590829e-02 -4.32715341e-02  1.07167347e-04\\n -4.81545143e-02  2.07524616e-02 -3.07160001e-02 -2.10958086e-02\\n  2.95780655e-02  1.12240203e-02 -6.27076328e-02  1.88718196e-02\\n -3.28683890e-02  3.43188383e-02 -1.27275568e-02 -9.45218466e-03\\n  1.94125739e-03 -1.85796171e-02 -1.71839427e-02  2.80057844e-02\\n -2.64212806e-02  3.42432484e-02 -1.17335599e-02  3.16975601e-02\\n  2.77885003e-03  2.20760182e-02  3.39102745e-03  3.60751003e-02\\n -9.34620425e-02 -8.11994150e-02  4.22107260e-04  7.51883686e-02\\n  6.47566766e-02  3.03783230e-02  3.91072556e-02 -3.75186093e-02\\n -1.91358477e-03 -1.09775132e-03  2.75573581e-02  6.08037738e-03\\n  1.26737105e-02  3.59291001e-03  5.60315792e-03 -1.56628191e-02\\n -5.69339395e-02  8.09054524e-02 -3.22829038e-02  2.97453795e-02\\n  1.73927564e-02 -3.48834135e-02 -5.57426326e-02 -5.26351221e-02\\n  3.25995535e-02  6.83596497e-03  1.25823058e-02 -2.80252118e-02\\n -2.58652531e-02 -1.73356906e-02  5.74893393e-02  2.02639885e-02\\n  2.92414008e-03 -4.76982147e-02  5.80731891e-02  5.63404821e-02\\n  3.88599373e-02 -1.95621178e-02 -4.75660414e-02  3.83055434e-02\\n -4.42795381e-02 -2.02666800e-02  2.01389473e-03 -5.79969774e-33\\n -4.34147418e-02 -6.02193251e-02  4.68467660e-02  5.62770553e-02\\n -2.75691263e-02 -3.30470428e-02 -3.67250293e-02 -1.53472628e-02\\n  5.45352651e-03 -1.17963664e-02 -3.88820842e-02 -4.59569693e-03\\n  2.84100734e-02  8.20853468e-03  1.32599138e-02 -3.20911780e-02\\n  1.65160745e-02 -2.15389077e-02 -2.33268202e-03  1.53096085e-02\\n  3.47944014e-02  4.51239906e-02  1.56341959e-02 -5.18654808e-02\\n  3.69273052e-02 -2.37732846e-02  2.97139602e-04  1.61311682e-02\\n -2.45741513e-02  4.19007801e-02 -2.78017167e-02  2.50368603e-02\\n  2.06924882e-02  1.44018792e-02 -2.89645512e-02  5.31896278e-02\\n -6.26226068e-02 -3.24507244e-02  1.61584560e-02 -5.28029073e-03\\n -4.53622304e-02 -7.05727786e-02  5.87692223e-02 -2.21006107e-02\\n -3.67918275e-02  1.34958224e-02  4.71796580e-02 -3.87333632e-02\\n -2.34909765e-02 -6.72350079e-02 -2.92931553e-02  2.61668637e-02\\n  6.89969352e-03 -1.68984197e-02  1.31035903e-02 -1.21129397e-02\\n  6.39017485e-03  3.59614976e-02 -1.49872452e-02  3.80209275e-02\\n  1.01967575e-02  2.90409792e-02  4.40491550e-02 -1.73138715e-02\\n  1.50489463e-02  4.98896241e-02 -1.39259081e-02 -2.58068182e-02\\n -4.44018692e-02  3.00793033e-02 -3.27145047e-02  8.50620568e-02\\n  2.88253948e-02  2.54621971e-02 -8.43326095e-03  6.57560828e-04\\n -5.08781746e-02  6.62795454e-02  2.67612040e-02  1.95164662e-02\\n  2.55057379e-03  4.33338666e-03 -1.75440442e-02  2.16977280e-02\\n  1.01186614e-02 -1.02086328e-02 -9.90124699e-03 -5.20115495e-02\\n  9.93606728e-03 -2.64088786e-03 -6.04087627e-03 -4.15220559e-02\\n -8.46650358e-03 -3.54065187e-02  1.29093044e-02 -1.96832623e-02\\n  7.00978516e-03  6.36103051e-03 -1.83326825e-02 -2.52497979e-02\\n -3.43614221e-02 -2.74005905e-02 -9.18329402e-04 -1.15615632e-02\\n  1.20273912e-02  3.58050019e-02 -2.93616205e-02  8.14027246e-03\\n -4.28436808e-02  3.53504438e-03 -1.75430719e-02  3.99425104e-02\\n  9.79364850e-03  9.06677684e-04 -1.23935158e-03 -2.99694426e-02\\n  6.12456305e-03  4.20444794e-02 -2.21354477e-02  1.06822001e-03\\n -5.06026521e-02 -3.95891704e-02  1.16703706e-02  8.63828603e-03\\n -5.21359034e-02  3.43556330e-02 -5.38016483e-02  5.22710159e-02\\n  2.94631030e-02 -3.99717987e-02 -1.09559800e-02  2.70110164e-02\\n  2.45394347e-07  5.86713143e-02  3.15926336e-02  5.38621992e-02\\n  6.17630705e-02  2.82452572e-02  6.83137029e-03 -1.44477375e-02\\n  2.34006830e-02 -5.73500572e-03 -3.37721296e-02  5.44766849e-03\\n  2.79430971e-02  3.98425795e-02  4.13395315e-02 -1.12780228e-01\\n -2.38353889e-02 -5.52153252e-02 -3.90177555e-02 -4.61434126e-02\\n  6.10494055e-03  7.25614205e-02  3.54371257e-02 -2.30103284e-02\\n -1.58782620e-02 -1.15684379e-05 -5.47217429e-02  4.27221619e-02\\n -2.78354883e-02  7.58009916e-03  2.67746486e-02  6.50505796e-02\\n  7.54065579e-03  1.68828689e-03 -2.93090250e-02 -2.85584293e-03\\n  1.30494013e-02 -4.67584794e-03  8.26340616e-02  1.73650181e-03\\n -4.89601269e-02  2.37004273e-02 -4.82872538e-02  3.51314694e-02\\n -1.91028807e-02  6.45866767e-02  5.38950460e-03  1.17662754e-02\\n -2.50227805e-02  1.73663013e-02 -4.52651381e-02 -2.47149430e-02\\n -4.10272703e-02 -8.32566060e-03  2.14926228e-02 -6.73649274e-03\\n -1.23783238e-02  4.59871051e-04 -6.97720721e-02 -1.63802821e-02\\n -7.75819365e-03 -6.68266267e-02 -6.65518567e-02 -3.80395502e-02\\n  1.64864901e-02 -8.79289955e-03 -3.42018604e-02 -4.12039496e-02\\n  2.38896865e-34  1.26144476e-02 -4.07522265e-03 -2.21080296e-02\\n  1.04649201e-01  2.84471214e-02  2.27679219e-02 -4.99323802e-03\\n  3.51419449e-02  7.31145905e-04 -7.74740428e-02 -2.71714963e-02]'},\n",
       " {'page_number': 2,\n",
       "  'sentence_chunk': 'to summarizing text, translating text into different languages, and more. In other words, you will learn how complex LLM assistants such as ChatGPT work by building one step by step. Stages of building and using LLMs Why should we build our own LLMs? Coding an LLM from the ground up is an excellent exercise to understand its mechanics and limitations. Also, it equips us with the required knowledge for pretraining or fine-tuning existing open source LLM architectures to our own domain-specific datasets or tasks. Research has shown that when it comes to modeling performance, custom-built LLMs—those tailored for specific tasks or domains—can outperform general-purpose LLMs, such as those provided by ChatGPT, which are designed for a wide array of applications. Examples of these include BloombergGPT (specialized for finance) and LLMs tailored for medical question answering. Using custom-built LLMs offers several advantages, particularly regarding data privacy. For instance, companies may prefer not to share sensitive data with third-party LLM providers like OpenAI due to confidentiality concerns. Additionally, developing smaller custom LLMs enables deployment directly on customer devices, such as laptops and smartphones, which is something companies like Apple are currently exploring.',\n",
       "  'sentence_chunk_size': 1301,\n",
       "  'sentence_chunk_word_count': 190,\n",
       "  'sentence_chunk_tokens': 325.25,\n",
       "  'embedding': '[ 2.05597989e-02 -3.87562551e-02 -2.46014893e-02  1.88106000e-02\\n -6.51753396e-02  1.71173494e-02  3.53315733e-02  1.57505006e-03\\n  5.89119382e-02 -1.74048916e-02 -1.73995793e-02  2.27822885e-02\\n  6.94403378e-03  6.98969746e-03  1.80892479e-02 -1.96747798e-02\\n  2.31146738e-02  3.76805034e-03 -2.99083181e-02 -1.44177265e-02\\n -1.14748534e-02 -9.68933478e-03 -1.71655545e-03  5.64225130e-02\\n -4.43164818e-03 -3.17315608e-02 -3.98310907e-02  2.36534085e-02\\n -4.79437364e-03 -3.07112616e-02 -1.03141135e-02 -1.15715265e-02\\n -5.48844412e-03  1.52304759e-02  2.24919904e-06 -3.15197445e-02\\n -4.24454659e-02  1.82019100e-02 -4.50607128e-02 -1.01365708e-02\\n  2.55786292e-02  5.74475080e-02  1.20500410e-02  4.82446738e-02\\n -1.99883897e-02  6.08313568e-02  3.88896838e-02  1.73014514e-02\\n  6.99444190e-02  1.04762554e-01  1.90873686e-02 -4.45215888e-02\\n  2.07835864e-02 -6.54592062e-04  1.65466219e-02 -7.13723153e-02\\n -1.29270395e-02  2.19606627e-02  7.13987947e-02  1.07819298e-02\\n  1.01398677e-02  1.19776074e-02  2.34409776e-02  2.92350864e-03\\n  1.85582414e-02  8.98048840e-03 -3.14712934e-02 -2.86900792e-02\\n -4.82921302e-02  2.57449076e-02  3.40002477e-02  1.08615877e-02\\n  1.89130176e-02 -1.78478509e-02  2.58159135e-02  2.06044316e-03\\n -5.01512885e-02  1.21345446e-02 -9.61898919e-03  4.48572971e-02\\n  2.83866795e-03 -2.88714841e-02 -3.51326214e-03 -1.11905113e-02\\n -4.30147583e-03  5.61088920e-02 -2.68729646e-02 -9.74910147e-03\\n  1.96407665e-03  2.30093878e-02  2.35421509e-02 -5.41065857e-02\\n  1.43242432e-02  7.47703612e-02  1.01322467e-02  1.32717360e-02\\n -3.00313625e-02  3.48018967e-02  1.03680799e-02 -4.73267175e-02\\n  2.57770233e-02  4.53432873e-02 -2.19425131e-02  1.74824540e-02\\n -5.34408428e-02  2.01493260e-02 -1.93914529e-02  6.03434592e-02\\n -7.57485479e-02 -1.29795279e-02 -4.07429561e-02 -4.85484488e-03\\n -3.58202606e-02  1.15092434e-01 -6.56927302e-02 -1.10762380e-02\\n -7.96872303e-02  6.41781911e-02  2.99427584e-02  7.91686866e-03\\n  2.54837628e-02 -5.24610002e-03  4.98012546e-03 -2.68448945e-02\\n -4.40860055e-02 -8.04413483e-02 -1.06366212e-02  1.53268641e-02\\n -5.08949626e-03 -6.11339621e-02  9.40403994e-03  8.55188910e-03\\n -2.77439207e-02  2.93761995e-02  1.83410151e-03  1.24397546e-01\\n  1.04655558e-02 -4.30066772e-02 -9.43843871e-02 -2.27333624e-02\\n  3.46828066e-02 -6.20842278e-02  4.55299579e-03 -4.62847874e-02\\n -7.60534452e-03  7.74667552e-03 -4.67786118e-02  1.84258129e-02\\n  1.94281377e-02  6.27393788e-03 -3.01471520e-02  1.30013619e-02\\n -1.90239660e-02  1.84911005e-02  1.70481410e-02  1.99575927e-02\\n -1.31358420e-02  9.02029350e-02  1.31630003e-02  1.47934500e-02\\n  2.67495960e-02  5.32692485e-03  1.00517152e-02  5.39264902e-02\\n -3.91560644e-02 -1.00980075e-02  1.92036387e-02  1.22363251e-02\\n -4.93749566e-02  1.22355232e-02 -3.87217663e-02  7.76223093e-03\\n -7.73461582e-03  5.24319261e-02  5.50007299e-02  1.02702424e-01\\n  1.10346399e-01  2.11393945e-02 -3.81672233e-02  9.41339508e-03\\n  3.73308584e-02  2.87848394e-02 -1.41144516e-02  5.60905598e-02\\n -3.79511900e-02  2.87923645e-02 -3.90841402e-02  4.41557765e-02\\n -3.33946571e-02 -5.51088378e-02 -3.51632424e-02 -9.61991586e-03\\n -4.75816689e-02 -2.13231817e-02 -3.36245925e-04 -2.38983650e-02\\n -2.99423747e-02  6.60449117e-02 -1.76643003e-02 -4.73101437e-02\\n -2.72898935e-02 -1.06163390e-01  8.01142864e-03  7.30315298e-02\\n  3.36218104e-02 -1.56688988e-02 -3.81382219e-02 -1.00816665e-02\\n -3.01764123e-02  7.72516653e-02  4.11658622e-02  2.47925725e-02\\n -4.24258076e-02 -5.50567433e-02 -3.95313688e-02 -1.05020963e-02\\n -1.77632794e-02 -3.25357891e-03  3.18671539e-02  1.23403710e-03\\n -3.80829014e-02 -2.84637343e-02 -1.64391194e-03  3.16140130e-02\\n -2.86701880e-02 -5.39057441e-02  9.40867420e-03 -2.56000576e-03\\n  7.78950565e-03  5.97591102e-02  2.96888258e-02  8.45679920e-03\\n  3.76354344e-02  3.45842307e-03 -2.28993911e-02  4.45218291e-03\\n -9.37060174e-03  1.64344367e-02  1.72306448e-02 -5.21841692e-03\\n  1.11258421e-02 -4.59326617e-02 -3.76708061e-02 -3.99862081e-02\\n -4.92937081e-02 -3.16366777e-02  6.43537939e-02 -2.41166558e-02\\n  1.42567989e-03  7.37882685e-03 -9.07644746e-04 -2.69460548e-02\\n  3.09339706e-02 -1.13382796e-02 -4.53440920e-02  1.48528432e-02\\n  2.33017113e-02  1.59926396e-02 -2.91980579e-02  3.96146439e-03\\n  2.61066016e-02 -1.14620421e-02  3.81717905e-02 -2.64175143e-02\\n  4.75429259e-02 -4.28920165e-02 -1.31453313e-02 -8.62257183e-02\\n  1.23644853e-02 -5.12227137e-03  2.21562218e-02  2.12649535e-02\\n  6.11761166e-03 -3.33786868e-02 -1.00261867e-02 -5.57048209e-02\\n  3.98571938e-02 -8.35215021e-03 -9.82747227e-03  5.81746474e-02\\n -6.30186498e-03 -3.44670191e-02 -2.55049244e-02  1.68691538e-02\\n  3.12741883e-02  4.77524521e-03  1.25047518e-02 -2.95572467e-02\\n -8.07086900e-02  2.54241563e-02  9.90714598e-03  1.58436280e-02\\n  2.72458475e-02 -2.73042619e-02  3.54043879e-02  1.29104871e-02\\n  2.05105077e-02  5.40026911e-02 -1.69780701e-02  2.18285639e-02\\n  1.63033139e-03 -1.27673550e-02 -7.07016960e-02  2.83829775e-02\\n -1.69749502e-02 -3.76302339e-02 -2.49451976e-02  1.66887796e-04\\n -2.14782842e-02  2.08261851e-02 -1.44374380e-02  6.56959973e-03\\n -1.26474574e-02  1.82497017e-02  4.85617965e-02 -2.39701793e-02\\n  2.58448883e-03  4.22760248e-02 -6.00278676e-02  5.02860509e-02\\n  1.73017886e-02 -6.89713433e-02  1.40196178e-02 -3.46236862e-02\\n  6.59912545e-03 -5.24286926e-02 -4.32258546e-02 -8.63832682e-02\\n  5.05157709e-02 -5.53218946e-02 -3.69849824e-03 -2.48337463e-02\\n -1.45625230e-03  6.82204682e-03 -6.09015673e-03  2.60818396e-02\\n -1.75365303e-02  5.92754744e-02 -6.36033639e-02  4.24167421e-03\\n -2.64884308e-02 -1.21730333e-02 -5.43038845e-02  4.89958376e-02\\n -1.68101862e-02 -3.05823777e-02  1.70919336e-02  2.43464448e-02\\n -3.66188101e-02  1.47620058e-02  3.45068537e-02 -4.62465221e-03\\n -3.26137766e-02  7.23827630e-03 -2.56854706e-02 -3.06510972e-03\\n -4.43394557e-02  3.96803282e-02  1.66730881e-02  1.28492974e-02\\n  5.34607992e-02  3.11462227e-02  2.02444457e-02 -7.03409538e-02\\n -3.41933854e-02 -1.70332473e-02  4.77249809e-02 -8.09210688e-02\\n -2.87099481e-02  5.79743311e-02  5.91761619e-02 -4.14989442e-02\\n  3.88651341e-02  6.18379638e-02 -4.27360414e-03 -3.27151306e-02\\n -2.22153682e-02 -2.51211971e-02 -1.55036142e-02  8.46690546e-06\\n  2.50113774e-02 -1.04238689e-01 -1.22484053e-02  3.00089479e-03\\n  4.19653319e-02 -2.18231007e-02 -1.46391820e-02 -4.28018533e-02\\n -9.47376341e-02  6.96487278e-02 -4.16233018e-03 -3.53463949e-03\\n  1.78508135e-03  9.02986247e-03 -8.99111200e-03  4.39323811e-03\\n  2.77554095e-02  1.39129432e-02 -1.44719128e-02 -7.48676946e-03\\n  2.59199962e-02 -1.20163418e-01 -3.54129495e-03 -3.82745527e-02\\n -1.77622586e-02  2.92499941e-02  5.07619083e-02  9.29855369e-03\\n -1.35194678e-02  3.69034265e-03  4.59787510e-02  4.21618894e-02\\n  6.39119819e-02 -4.04137336e-02  2.99631860e-02  2.84309387e-02\\n -4.23093187e-03  3.67207103e-04  3.11066099e-02 -1.37216710e-02\\n -5.01139928e-03  3.33855674e-02  3.64885963e-02 -5.51046617e-02\\n -1.89403929e-02 -3.84618379e-02 -2.29792277e-04 -2.81503741e-02\\n  4.73596156e-02 -2.58065201e-03 -2.01491732e-02 -2.60321870e-02\\n -2.51663383e-02 -2.86302473e-02 -2.84760445e-02 -2.06688326e-02\\n -6.89060893e-04 -5.45749515e-02  2.26902999e-02 -4.03108560e-02\\n -2.03476325e-02  1.64499469e-02 -3.60388518e-03  1.08904904e-02\\n  1.79292783e-02  1.68796238e-02  4.71340753e-02 -4.35870513e-03\\n -2.58954540e-02 -1.52958352e-02 -1.21199610e-02 -5.06139025e-02\\n -3.23155746e-02  6.29152656e-02 -3.81480120e-02  1.87654104e-02\\n -4.86454517e-02 -3.70134935e-02 -4.71608490e-02  1.33215357e-02\\n  3.49112265e-02  3.61869596e-02  3.87602369e-04  6.42003026e-03\\n -4.51376243e-03  2.68141478e-02  7.15286881e-02 -2.91174669e-02\\n  1.08531518e-02  4.19532210e-02  8.49102736e-02  4.74903034e-03\\n  9.42975748e-03 -2.89284140e-02  3.09272334e-02  1.97679158e-02\\n  8.24134238e-03  2.15118262e-03 -7.98869506e-02  1.14006586e-02\\n  6.33303970e-02  2.35022772e-02 -1.01414751e-02  3.59001942e-02\\n  3.04426532e-03  4.80267666e-02 -1.66821312e-02  2.65912376e-02\\n -2.81851925e-02 -2.06903554e-02 -9.55926487e-04 -1.67908072e-02\\n  1.65173206e-02 -1.77692026e-02 -3.98068018e-02 -2.14804765e-02\\n -2.86542941e-02  9.08551961e-02 -4.66933325e-02 -2.67768744e-02\\n -1.04643507e-02  6.97221002e-03 -3.28773931e-02  3.78631689e-02\\n -2.62352079e-02 -2.27584896e-04  1.44156357e-02  4.81516682e-02\\n -4.33849208e-02  1.92660764e-02  1.56989694e-02  4.44622561e-02\\n -2.75868587e-02 -9.42667425e-02 -1.78531371e-02  1.12825349e-01\\n  4.90453206e-02  4.37310785e-02 -4.47096117e-03 -3.99833992e-02\\n  8.68471526e-03  8.16543773e-03  1.83900148e-02 -3.99722252e-03\\n  4.63578105e-02 -5.12582948e-04 -3.01813614e-03 -4.03406508e-02\\n -9.70587283e-02  7.25530982e-02 -2.45582219e-03  1.11269481e-04\\n -2.93794721e-02 -7.37128360e-03 -5.20708077e-02 -7.69159496e-02\\n  4.05226573e-02  1.70632219e-03  1.13770571e-02 -3.09296753e-02\\n -6.68205619e-02 -2.89393626e-02  2.19200309e-02 -1.74071454e-02\\n  5.65297622e-03 -5.16639426e-02  4.44078930e-02  2.40820069e-02\\n  2.65750010e-02 -3.85215413e-03 -4.70530038e-04 -5.31141972e-03\\n -3.62631306e-02  7.82686751e-03  3.70339416e-02 -6.51042918e-33\\n -3.10361255e-02 -3.91319171e-02  6.57107607e-02  5.99253438e-02\\n  3.57578658e-02 -2.67873034e-02  3.70766292e-03  2.20033824e-02\\n  4.61624470e-04 -3.89732867e-02 -2.07203273e-02 -2.59171855e-02\\n  3.85946184e-02 -1.28222294e-02  1.42859900e-02 -2.64963508e-02\\n  5.81464591e-03 -3.17595489e-02 -2.47866876e-04 -2.10794676e-02\\n -3.36742699e-02  4.20543551e-02  2.80253794e-02 -5.22106290e-02\\n  1.09469043e-02 -2.64976323e-02 -1.47384480e-02 -9.98642296e-03\\n -4.93350253e-02  4.81954738e-02  4.36024449e-04  9.68992524e-03\\n  1.39744403e-02 -7.16628460e-03 -2.02254038e-02  1.86555963e-02\\n -2.03399751e-02 -5.02523035e-02  4.09162678e-02  2.55372934e-02\\n -2.82740425e-02 -7.95388222e-02  4.91177328e-02  9.21636075e-03\\n -7.70139694e-02 -1.05681513e-02  1.10243703e-03 -3.91256250e-02\\n  1.66322070e-03  1.11423829e-03 -1.64953899e-02  3.50149050e-02\\n  3.03946454e-02  2.68515628e-02  2.88592298e-02 -4.18854952e-02\\n -2.95170248e-02 -3.37743238e-02 -1.41758192e-03  3.08046304e-02\\n -1.84147479e-03  6.02665357e-03  6.61897510e-02 -1.11765070e-02\\n -5.46267442e-03  3.52685787e-02 -5.31115662e-03 -2.28297096e-02\\n -3.29811424e-02 -7.42330914e-03 -1.99910458e-02  1.00426793e-01\\n  5.96908592e-02  6.67372160e-03 -1.97512973e-02  1.20536210e-02\\n -2.19507553e-02  3.35073471e-02  4.79765162e-02 -1.01751518e-02\\n  1.84493437e-02  5.44024818e-02 -2.65016444e-02 -1.00821424e-02\\n  1.96663439e-02 -2.05115248e-02 -1.23587307e-02 -5.18771969e-02\\n  4.59413184e-03  2.76479311e-03 -2.92835012e-02 -2.08534710e-02\\n -1.15106320e-02 -1.50215216e-02  5.01571484e-02 -3.76610607e-02\\n -1.26969453e-03  2.17538457e-02  9.71440505e-03 -7.41304923e-03\\n -6.59722090e-02 -1.86880827e-02 -3.69111858e-02 -8.59450083e-03\\n  1.36568481e-02  1.44247832e-02 -2.37501264e-02  1.48997027e-02\\n -6.13608360e-02  2.40336843e-02 -1.58896530e-03  5.78598380e-02\\n  9.55101941e-03 -3.31189632e-02 -3.09544825e-03 -3.78528312e-02\\n -1.61841307e-02  4.56330329e-02 -2.23102327e-02 -2.76477635e-02\\n -5.06423153e-02 -1.02776140e-02 -8.11820384e-03 -5.36103128e-03\\n -1.56600885e-02  5.28904311e-02 -5.06786164e-03  6.01266772e-02\\n  4.31115218e-02 -2.85874046e-02 -2.24709138e-02  4.11221609e-02\\n  3.02253397e-07  4.27624024e-02  4.52017970e-02  4.70231287e-02\\n  8.45397338e-02  2.15793066e-02  2.70450711e-02 -5.93697652e-03\\n  2.19923374e-03 -4.11598431e-03 -6.89944346e-03 -1.04341637e-02\\n  3.45421061e-02  4.73058410e-02  2.87525319e-02 -4.82934751e-02\\n -4.34187874e-02 -2.81728432e-02  9.32389218e-03 -3.05238571e-02\\n  1.99293122e-02  6.10101447e-02  4.67116833e-02  2.05294136e-02\\n -2.19728723e-02 -3.98482643e-02 -3.19503918e-02  4.86903489e-02\\n -5.27059622e-02  1.05469655e-02  4.31101918e-02  5.33283912e-02\\n  7.10555166e-02 -3.98646994e-03  8.89050891e-04 -2.52517313e-03\\n -1.45613430e-02 -1.43083027e-02  1.01439610e-01  3.61033343e-02\\n -5.78018352e-02  5.29357120e-02 -4.98399101e-02  2.48098914e-02\\n  5.48469694e-03  6.44809455e-02 -1.75019503e-02 -8.59292410e-03\\n -2.35647783e-02  6.77402690e-02 -5.72189167e-02 -1.28509710e-02\\n -2.34701075e-02  3.53024900e-02  1.21945990e-02 -3.04168696e-03\\n -4.23610397e-02 -1.27721913e-02 -4.57656533e-02 -2.32339483e-02\\n  1.72558781e-02 -4.67165597e-02 -3.71851958e-02 -3.43862250e-02\\n  2.86391862e-02  1.80895869e-02 -2.59394143e-02 -1.00601129e-02\\n  3.09018662e-34 -3.61633865e-04 -3.42540331e-02 -7.70869758e-03\\n  8.77990872e-02 -8.43739416e-03  2.57307831e-02  3.25923003e-02\\n  2.20437907e-02 -5.63661288e-03 -3.29495445e-02 -1.94521956e-02]'},\n",
       " {'page_number': 2,\n",
       "  'sentence_chunk': 'This local implementation can significantly decrease latency and reduce server-related costs. Furthermore, custom LLMs grant developers complete autonomy, allowing them to control updates and modifications to the model as needed. The general process of creating an LLM includes pretraining and fine-tuning. The “pre” in “pretraining” refers to the initial phase where a model like an LLM is trained on a large, diverse dataset to develop a broad understanding of language. This pretrained model then serves as a foundational resource that can be further refined through fine-tuning, a process where the model is specifically trained on a narrower dataset that is more specific to particular tasks or domains. The first step in creating an LLM is to train it on a large corpus of text data, sometimes referred to as raw text. Here, “raw” refers to the fact that this data is just regular text without any labeling information. ( Filtering may be applied, such as removing formatting characters or documents in unknown languages.) This first training stage of an LLM is also known as pretraining, creating an initial pretrained LLM, often called a base or foundation model. A typical example of such a model is the GPT-3 model (the precursor of the original model offered in ChatGPT).',\n",
       "  'sentence_chunk_size': 1282,\n",
       "  'sentence_chunk_word_count': 207,\n",
       "  'sentence_chunk_tokens': 320.5,\n",
       "  'embedding': '[ 3.29110287e-02 -3.94246727e-02 -2.06209142e-02  1.64344888e-02\\n -7.99099728e-02  3.58435586e-02  2.05033123e-02  3.50391641e-02\\n  2.73551140e-02 -5.09394966e-02  3.06241959e-03 -1.31194331e-02\\n -1.90783124e-02  2.20626388e-02  3.83497812e-02 -3.22348215e-02\\n  3.51742320e-02  6.35287899e-04 -3.97128016e-02 -2.42791651e-03\\n  1.20754307e-02 -4.47414396e-03  1.86494775e-02  2.14333404e-02\\n  1.38272876e-02 -3.27563286e-02 -2.52587199e-02  6.96033090e-02\\n -3.45220827e-02 -3.10177188e-02 -1.10886367e-02  1.33515643e-02\\n  3.86374593e-02  1.13997085e-03  2.25281474e-06 -3.67998891e-02\\n -6.89551309e-02  1.91582199e-02 -2.50520017e-02  1.31871179e-02\\n  2.97689065e-02  4.47810292e-02 -1.32251317e-02  5.78122996e-02\\n -2.90293805e-02  3.97786722e-02  4.53233123e-02  2.35429537e-02\\n  3.63854803e-02  8.18900615e-02 -1.18598202e-02 -4.52036671e-02\\n -3.28405686e-02  2.82813795e-02  1.70079637e-02 -4.46936414e-02\\n  8.34607892e-03  1.62610449e-02  2.12601442e-02  2.22508647e-02\\n -1.01565989e-02 -7.15840608e-03 -1.18699379e-03  1.83385722e-02\\n  3.24110836e-02 -1.10260362e-03 -2.14908831e-02 -3.77569348e-02\\n -5.60390987e-02  1.31015647e-02  1.69462021e-02 -2.39712391e-02\\n  5.00249863e-02 -1.27581637e-02 -9.76763945e-03  1.10204043e-02\\n -2.22747494e-02 -1.14009837e-02  1.16251700e-03  4.70633171e-02\\n -3.04703671e-03 -6.32159710e-02  3.15539539e-03 -2.34547369e-02\\n -2.96089295e-02  7.92858675e-02  2.17657769e-04 -3.24702449e-02\\n -4.70147422e-03  1.86230578e-02  2.26922315e-02 -6.43013716e-02\\n  5.05338758e-02  7.01367185e-02  3.03617697e-02  4.88881068e-03\\n -3.55418772e-02  2.74996869e-02  2.64525358e-02 -4.18628193e-02\\n  6.74668793e-03  3.94363031e-02 -2.91794967e-02  2.02680491e-02\\n -3.37863788e-02  6.29640324e-03 -1.49739189e-02  3.62426862e-02\\n -1.66108757e-02  3.60467169e-03 -4.79736291e-02 -4.46462166e-03\\n -5.35158105e-02  7.67151639e-02 -3.81443799e-02  2.54119039e-02\\n -4.02508676e-02  7.87896737e-02  5.88273816e-02  8.91290326e-03\\n  4.16689366e-02 -3.37409824e-02 -2.98676770e-02  1.01887649e-02\\n -5.19450009e-02 -6.14443123e-02 -7.74909602e-03  5.01383375e-03\\n -1.53720267e-02 -8.32886100e-02  9.66431480e-03  2.52551911e-03\\n -4.92467452e-03  1.82200111e-02  2.02922840e-02  8.72315392e-02\\n  6.25175470e-03 -6.73475116e-03 -9.46310535e-02 -1.30684720e-02\\n  4.64505740e-02 -3.94694954e-02 -2.16888692e-02 -4.84249666e-02\\n -1.29029248e-02  1.78834610e-02 -3.26340534e-02 -2.07639579e-03\\n -2.88097113e-02  5.74473431e-03 -2.07586270e-02  6.86721830e-03\\n -7.60772675e-02  2.98589002e-02  3.05779874e-02  7.33690336e-03\\n  1.64012983e-02  6.14493936e-02  3.55593208e-03  1.42413909e-02\\n  3.06368209e-02 -1.33560216e-02  4.26558964e-03  6.52817711e-02\\n -2.68257800e-02  2.94729248e-02 -4.53652106e-02  6.94816560e-03\\n -3.83940502e-03 -1.80527254e-03 -1.74404252e-02  7.26449070e-03\\n -1.94009207e-02  5.53174466e-02  7.15723261e-02  1.03993855e-01\\n  9.68968198e-02  3.44188474e-02  7.49696558e-03  4.78719734e-02\\n  2.61870530e-02  1.41479950e-02 -2.03405619e-02  1.43324547e-02\\n -3.37854736e-02  1.12594375e-02 -3.56302708e-02  6.89039901e-02\\n -3.02273165e-02 -3.08977515e-02 -3.65361124e-02  4.14654426e-03\\n -5.15658483e-02 -1.54326102e-02  1.95109192e-02 -1.78689044e-02\\n -2.54072137e-02  8.27270970e-02  4.68894467e-03 -4.00317349e-02\\n -1.86839402e-02 -7.12913200e-02 -2.25601718e-02  7.26027638e-02\\n  3.80998366e-02 -1.42114395e-02 -5.94537370e-02 -1.97402202e-02\\n -1.64932739e-02  5.43400124e-02  1.83157027e-02  3.17443945e-02\\n -3.59523743e-02 -4.30929661e-02 -4.39979024e-02  3.76533046e-02\\n -1.38513679e-02  6.52472442e-03 -1.14708534e-02  1.24361739e-03\\n -4.65225242e-02 -3.46432701e-02 -1.53802056e-02  4.82099727e-02\\n -3.47204097e-02 -4.18370739e-02 -4.12533209e-02  1.12535292e-03\\n  2.77244933e-02  2.53229775e-02  2.75424588e-03 -3.56642045e-02\\n  6.12077713e-02  2.64204424e-02 -2.14514323e-02 -5.68751944e-03\\n -8.84897448e-03  3.65046076e-02  3.77619304e-02  2.26568300e-02\\n  4.42046858e-02 -3.91218737e-02 -3.71917672e-02 -4.45864536e-02\\n -4.86259535e-02 -7.99692515e-03  6.27551004e-02 -4.89839688e-02\\n  1.73856579e-02 -1.75136086e-02 -4.63750353e-03 -5.26110120e-02\\n  1.80644244e-02  1.15954159e-02 -4.02020775e-02  6.11791294e-03\\n  1.78914480e-02  1.92460660e-02 -3.41200307e-02  3.45445611e-03\\n  3.76525298e-02 -1.40912468e-02  2.45724134e-02 -1.52054820e-02\\n  5.80222979e-02 -4.85436320e-02 -1.78272910e-02 -9.65650380e-02\\n  2.37708837e-02 -1.97992027e-02  4.37419415e-02  5.13739996e-02\\n -5.97510301e-03 -4.58799265e-02 -2.62847375e-02 -6.22042939e-02\\n  3.32736149e-02 -1.91869366e-03 -5.07640652e-04  5.22213131e-02\\n  9.66298115e-03 -4.97816913e-02 -1.77942459e-02  1.90008674e-02\\n  3.31372842e-02  2.00727731e-02  5.25422255e-03 -6.85550347e-02\\n -7.79778957e-02  3.97664607e-02 -2.80312188e-02  1.85279287e-02\\n  3.15310098e-02 -6.50477931e-02 -6.31739711e-03 -1.28337825e-02\\n  1.92077961e-02  2.07080711e-02 -2.23293770e-02 -1.19017751e-03\\n  1.18754199e-02 -4.74504475e-03 -7.58546218e-02  5.00140851e-03\\n -2.58924644e-02 -4.87713329e-02 -6.91441521e-02  1.17361755e-03\\n -2.75476445e-02  8.45338181e-02 -1.53660867e-02 -6.30081259e-03\\n  1.06511852e-02  7.54835550e-03  3.83898504e-02 -3.06766871e-02\\n -1.54521698e-02  2.04918180e-02 -5.90792075e-02  3.81968170e-02\\n  8.23045522e-03 -7.60558918e-02  1.27641661e-02 -4.34297398e-02\\n -1.12376586e-02 -5.33312894e-02 -5.58521040e-02 -5.20650372e-02\\n  5.22135664e-03 -3.67549504e-03  1.29308039e-02 -1.92915350e-02\\n -2.71763206e-02 -2.76697450e-03  1.40475761e-03  4.20371890e-02\\n -1.42374996e-03  1.96923967e-02 -3.08185779e-02  1.90606862e-02\\n -2.10537892e-02 -1.76736321e-02 -1.41435461e-02  2.66028736e-02\\n -1.22742103e-02 -3.58892307e-02  1.13718184e-02  4.24288167e-03\\n -1.32551929e-03  1.67615619e-02  1.87252108e-02  5.01886979e-02\\n -2.15953533e-02 -2.74390331e-03 -3.62750106e-02 -3.43420245e-02\\n -4.63433117e-02  9.60383005e-03 -2.15919837e-02  6.65332377e-03\\n  1.94428544e-02  2.96877883e-02  2.08365601e-02 -8.42690915e-02\\n -4.13158834e-02  1.33282626e-02  7.19479620e-02 -7.80480877e-02\\n -6.17473722e-02  6.74838051e-02  4.62200791e-02 -5.42969778e-02\\n  4.25310945e-03  8.83922502e-02  7.26829004e-03 -6.47775978e-02\\n -3.54200639e-02 -3.36461179e-02  2.68654115e-02  4.14396599e-02\\n  3.34279165e-02 -7.38408566e-02 -1.01978751e-02  1.54414931e-02\\n  4.71069198e-03 -1.94257703e-02 -1.86761767e-02 -2.31489651e-02\\n -2.33346876e-02  4.83828560e-02  1.92791447e-02 -3.78454179e-02\\n  4.64502722e-02  6.85537048e-03 -3.22009884e-02  2.46333815e-02\\n  6.59028022e-03  1.71445254e-02 -2.47515365e-02  4.06548660e-03\\n  4.07284610e-02 -9.07194838e-02 -4.37590331e-02 -4.08201255e-02\\n -4.78580780e-02  5.62016405e-02  6.43882155e-02 -3.07795964e-03\\n -1.54732326e-02  9.53321997e-03  8.02136064e-02  4.98449169e-02\\n  4.31584865e-02 -6.19750768e-02  1.33385230e-02  8.10853392e-03\\n  1.59666147e-02  1.07046934e-02 -8.10280349e-03  1.12630567e-03\\n -1.02897324e-02  6.45434391e-03  6.44589663e-02 -5.59918061e-02\\n  1.57246348e-02  8.14591721e-03 -3.45475879e-03 -1.07320456e-03\\n  3.27665880e-02  1.17052700e-02 -3.92358042e-02 -6.11702502e-02\\n -3.10186390e-02 -3.41668166e-02  1.25266602e-02 -4.37704213e-02\\n -7.57078035e-03 -6.44243732e-02  1.13119846e-02 -3.33309546e-02\\n  6.34176482e-04  8.29882571e-04  3.58828418e-02  2.79070847e-02\\n  4.22852524e-02  6.09066279e-04  2.45941933e-02 -3.55892107e-02\\n  1.64926928e-02 -3.50635760e-02 -3.38560566e-02 -6.85608834e-02\\n -3.88882570e-02  6.15662150e-02 -4.52301949e-02 -8.31770245e-03\\n -7.42577314e-02 -2.74769608e-02 -4.09935154e-02  2.09413692e-02\\n  3.50759365e-02  3.96716818e-02  1.84007771e-02  8.81643593e-03\\n -6.88745826e-02  3.44491042e-02  4.19349521e-02 -4.90257964e-02\\n  3.39267887e-02  3.62698995e-02  1.48867462e-02  5.35559049e-03\\n -1.34365605e-02 -3.39215323e-02  4.12888676e-02  2.16742232e-02\\n -2.68981513e-02  5.51321497e-03 -3.89030203e-02 -2.41193431e-03\\n  7.14480653e-02  6.20399714e-02  7.74582848e-04  1.75806286e-03\\n -6.77564181e-03  4.62196395e-02  1.07967982e-03  2.51228102e-02\\n -3.94510962e-02  6.02350011e-03 -1.26469124e-03  4.69403341e-03\\n -1.69164629e-03  5.21193957e-03 -3.50470953e-02  1.07186695e-03\\n -9.55251325e-03  5.62498011e-02 -1.54387522e-02 -2.57189944e-02\\n  2.61471774e-02  2.16075834e-02 -1.50846727e-02  3.04814465e-02\\n -1.89653859e-02  1.96482986e-03 -2.45898534e-02  5.39654568e-02\\n -1.51907438e-02  4.39539477e-02 -1.61075487e-03  5.19332699e-02\\n -4.08751890e-02 -5.22127450e-02 -2.37019043e-02  5.75879440e-02\\n  7.21632615e-02  4.84032109e-02  3.08227371e-02 -1.94886476e-02\\n  4.60083410e-03 -1.24604767e-03  2.22019404e-02 -2.24998780e-02\\n  1.65416766e-02 -5.27633121e-03 -1.57808997e-02 -5.48407324e-02\\n -3.68190780e-02  4.89504412e-02  5.10632917e-02  2.39231437e-02\\n -2.91230511e-02 -3.32390405e-02 -2.40879282e-02 -2.12228931e-02\\n  5.48450761e-02 -4.24763747e-03 -2.75024655e-03 -3.56270671e-02\\n -4.98846285e-02 -1.62081402e-02  4.20515835e-02 -5.66568749e-04\\n  4.50515235e-03 -5.57145588e-02  5.74176945e-02  5.44311702e-02\\n  4.07953421e-03 -1.34892669e-02 -2.08797120e-02 -1.03659136e-03\\n  2.84731463e-02  7.57258106e-03  3.90969217e-02 -6.59877786e-33\\n -2.65756939e-02 -6.38250858e-02  5.50731309e-02  4.08147983e-02\\n  7.09267054e-03 -4.08467874e-02  2.85580177e-02 -2.05570254e-02\\n  2.91979574e-02 -4.02391069e-02 -3.21469381e-02  1.07886586e-02\\n  4.32623215e-02 -1.31169874e-02 -2.27358546e-02 -2.12706793e-02\\n  2.55447216e-02 -4.52009551e-02  1.91690084e-02 -2.64850771e-03\\n -1.78596471e-02  7.35528469e-02  6.95757940e-02 -3.85724753e-02\\n -5.01928665e-03 -3.34252864e-02 -5.31036966e-03 -1.28850376e-03\\n -3.01481001e-02  4.66033891e-02  1.41332252e-03 -6.32481463e-03\\n  1.24281775e-02 -3.46616916e-02 -2.23669000e-02  3.31012392e-03\\n -2.21192893e-02 -3.62049639e-02  2.76800282e-02  3.39296535e-02\\n -1.06427511e-02 -3.69716324e-02  3.60308401e-02  4.05587861e-03\\n -9.83757004e-02  3.00866384e-02  5.54411393e-03 -2.96152625e-02\\n -4.05692533e-02 -3.96964140e-02 -1.39197018e-02  2.45125741e-02\\n  4.25915271e-02  8.56738091e-02  4.98240329e-02 -4.20280620e-02\\n -1.54454736e-02 -2.54280888e-03 -1.66975167e-02 -5.46692580e-04\\n  4.18835208e-02  2.35872101e-02  5.12588359e-02 -2.23129289e-03\\n -4.38698055e-03  3.93393859e-02  1.66753903e-02 -9.08527616e-03\\n -2.66602095e-02  1.44239366e-02  1.99912414e-02  6.21820763e-02\\n  2.15473529e-02  2.50924733e-02  1.06780780e-02  2.69634952e-03\\n -5.15444241e-02  1.25274360e-02  1.20097455e-02 -9.78594832e-03\\n  1.59493182e-02  5.62643856e-02  2.82269740e-03 -2.00315956e-02\\n -2.12467574e-02 -3.50614414e-02 -1.32965865e-02 -4.67247143e-02\\n  3.03129032e-02 -1.21794296e-02  1.48128916e-03 -1.14028761e-02\\n  1.53404390e-02 -9.92230326e-03  1.90969240e-02 -2.04872694e-02\\n -1.50098288e-02  2.33719833e-02  1.46288180e-03  1.08337058e-02\\n -4.49947491e-02 -4.69617434e-02 -3.54159027e-02 -2.42545698e-02\\n  2.37937979e-02  7.09822914e-03 -8.48790724e-03  3.78598571e-02\\n -8.93739536e-02  5.07451706e-02 -7.40220863e-03  3.93542014e-02\\n  1.49336765e-02 -1.67666702e-03  8.33631121e-03 -5.48343733e-02\\n  1.56091657e-02  1.03745855e-01  6.49931375e-03 -5.43597378e-02\\n -4.32232209e-02 -2.66002547e-02  9.02906060e-03  2.69968100e-02\\n -2.35243514e-02  4.64959815e-02 -1.90399569e-02  4.99030910e-02\\n  6.43126965e-02 -2.19000578e-02 -4.27972712e-03  1.24288555e-02\\n  2.96840511e-07  2.61068363e-02  8.71931538e-02  2.90694814e-02\\n  1.85429361e-02  3.74327973e-02  6.55369973e-03 -1.42229721e-02\\n  6.08544191e-03  3.02542634e-02 -2.47895811e-02  1.25089809e-02\\n  3.38697508e-02  3.59965526e-02  1.59933604e-02 -6.74388260e-02\\n -1.18664252e-02 -4.32475731e-02  1.80547982e-02 -3.86063419e-02\\n  1.09386789e-02  8.70588981e-03  5.45435958e-02  2.13488061e-02\\n -2.10531913e-02 -5.87369688e-03 -3.13612148e-02  3.87253538e-02\\n -6.22149669e-02  8.22538510e-03  5.08349463e-02  8.79082978e-02\\n  4.72967736e-02 -1.08549208e-03 -1.89077610e-03 -2.67729647e-02\\n -2.54936405e-02 -3.89839001e-02  1.14102803e-01 -1.71490461e-02\\n  1.16994018e-02  3.24696787e-02 -6.12494908e-02 -3.93553637e-03\\n -6.03646878e-03  6.25553802e-02  3.50359129e-03  2.67389859e-03\\n -1.51596183e-03  3.90669964e-02 -1.02783196e-01 -1.06347622e-02\\n -3.34006399e-02  3.13146263e-02  2.23104060e-02  7.53645459e-03\\n -2.71534808e-02 -3.89177389e-02 -5.80855720e-02 -1.45027845e-03\\n  2.70369295e-02 -2.51224861e-02 -5.17472923e-02 -3.23208272e-02\\n  3.49914506e-02  1.91676337e-02 -4.55688871e-02 -1.13133471e-02\\n  3.58784018e-34 -1.17790503e-02 -2.55379155e-02  2.54237875e-02\\n  8.90140459e-02 -1.78291798e-02  1.07789505e-03  4.54045869e-02\\n  1.94276832e-02 -2.21787151e-02 -5.75178005e-02  5.65578183e-03]'},\n",
       " {'page_number': 2,\n",
       "  'sentence_chunk': 'This model is capable of text completion—that is, finishing a half-written sentence provided by a user. It also has limited few-shot capabilities, which means it can learn to perform new tasks based on only a few examples instead of needing extensive training data. After obtaining a pretrained LLM from training on large text datasets, where the LLM is trained to predict the next word in the text, we can further train the LLM on labeled data, also known as fine-tuning. The two most popular categories of fine-tuning LLMs are instruction fine-tuning and classification fine-tuning. In instruction fine-tuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text. In classification fine-tuning, the labeled dataset consists of texts and associated class labels—for example, emails associated with “spam” and “not spam” labels. Introduction to Transformers Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper “Attention Is All You Need” (https://arxiv.org/abs/1706. 03762). To understand LLMs, we must understand the original transformer, which was developed for machine translation, translating English texts to German and French. The transformer architecture consists of two submodules: an encoder and a decoder.',\n",
       "  'sentence_chunk_size': 1382,\n",
       "  'sentence_chunk_word_count': 206,\n",
       "  'sentence_chunk_tokens': 345.5,\n",
       "  'embedding': '[ 5.91579825e-02 -7.17367744e-03 -6.20600022e-03  4.78102416e-02\\n -2.36184411e-02  2.86071636e-02 -1.89670771e-02  1.31543707e-02\\n  8.39817294e-05 -4.14367095e-02 -3.25129963e-02 -2.37449314e-02\\n -1.81516297e-02 -1.47613222e-02  4.05076630e-02 -3.78847420e-02\\n  3.65521722e-02  6.82320329e-04 -5.50381467e-02 -3.69923911e-03\\n -1.15476726e-02 -7.83188175e-03  1.69905312e-02  2.12320201e-02\\n  1.73349809e-02  4.09736205e-03 -1.57167651e-02  3.26926820e-02\\n -2.23901514e-02  1.81376282e-02  3.83994095e-02  2.33926177e-02\\n -9.14927758e-03  1.87360495e-02  1.95886287e-06 -5.28901778e-02\\n -1.70528349e-02 -2.82543208e-02 -1.38147613e-02 -4.51711677e-02\\n  3.82417664e-02 -3.15605290e-02 -3.90068442e-03  2.85979472e-02\\n -1.82906259e-02  4.27973308e-02  5.38496859e-02  8.69391114e-02\\n  3.79873365e-02  9.97704566e-02  1.93424663e-03 -8.27679932e-02\\n  4.94209155e-02 -3.16967890e-02  5.71130849e-02 -4.99964021e-02\\n  2.86929924e-02 -8.03649649e-02 -2.66193449e-02  6.72970936e-02\\n -1.91716664e-02  5.52920625e-02  1.38656702e-02 -2.19124812e-03\\n -5.97624667e-03 -3.19368090e-03 -5.24648651e-03 -5.34149036e-02\\n -1.83370132e-02  1.21882288e-02 -2.51644757e-02  8.21545534e-03\\n  5.33428229e-03  7.71878846e-03  1.35571444e-02 -2.86522787e-02\\n -3.03280838e-02 -3.60964215e-03  1.00050345e-02  2.03989930e-02\\n  6.22254983e-03 -1.53309517e-02  7.49074947e-03  1.70261115e-02\\n -3.05782799e-02  6.02787100e-02 -1.44050010e-02 -2.56828982e-02\\n -1.66190080e-02  3.16859111e-02  5.22958711e-02 -6.00235499e-02\\n  5.62676713e-02  7.70841390e-02  1.05791111e-02  8.40790570e-03\\n -4.97645028e-02 -3.33173424e-02  3.12954821e-02 -3.09333187e-02\\n  4.06808220e-03  3.40665430e-02 -1.15089761e-02  2.85099484e-02\\n -2.99404971e-02  3.39345075e-02 -5.28715253e-02  6.88708872e-02\\n -6.81799576e-02 -2.15910096e-02 -2.11152826e-02 -1.91169027e-02\\n -4.14305702e-02  3.16544175e-02 -9.73444618e-03  8.67922232e-03\\n -6.15021437e-02  3.18284854e-02  3.83309722e-02  1.67893097e-02\\n  2.63628215e-02 -1.40061527e-02 -4.54145297e-02 -1.59793021e-03\\n -5.69196790e-02  2.64483020e-02 -1.68031342e-02  6.30707573e-03\\n  2.75140330e-02 -2.96205040e-02 -7.21197063e-03  1.34876315e-02\\n  3.25003220e-03 -3.64747345e-02  4.89197820e-02  5.54760620e-02\\n -4.07268107e-02 -5.35335056e-02 -5.82896732e-02  1.32382084e-02\\n -2.00664420e-02 -5.71137145e-02  1.49372302e-03 -5.03997393e-02\\n  2.20407788e-02  3.38669829e-02 -1.12029687e-02 -1.03260018e-02\\n  9.61427926e-04  5.34068458e-02 -5.57228364e-02  4.76517826e-02\\n  4.17967606e-03 -3.13399802e-03  4.26461212e-02 -1.22893183e-02\\n  2.71181967e-02  3.00144237e-02 -1.01742353e-02  7.75758699e-02\\n  2.44679898e-02 -4.15716739e-03  2.70564668e-02  4.65623587e-02\\n -8.34001321e-03 -6.38456736e-03 -1.76897347e-02 -2.91329119e-02\\n -3.85557152e-02  1.38899488e-02 -1.10827303e-02  4.90708016e-02\\n -9.65271518e-03  5.20993210e-03  8.09887797e-02  3.27111222e-02\\n  6.87971115e-02  5.88341504e-02  1.10117476e-02  2.18659043e-02\\n  6.88448781e-03  3.91682871e-02 -5.31568043e-02  5.40167019e-02\\n -3.22127603e-02 -2.25954689e-03 -5.49520599e-03  2.85894610e-02\\n -5.07646501e-02 -7.42829442e-02  5.16473688e-03 -2.35060379e-02\\n -1.05265379e-02 -1.21095153e-02  5.95392846e-02  5.72676435e-02\\n -6.95201457e-02  1.03352048e-01 -2.05622334e-02 -4.28114290e-04\\n  1.07698087e-02 -4.99603413e-02 -2.30300501e-02  6.15994222e-02\\n  3.26024890e-02 -2.87758149e-02 -4.73806560e-02 -4.68500629e-02\\n  8.18314869e-03  5.97565062e-02  4.41101147e-03  4.68029752e-02\\n -1.03640687e-02 -3.85001376e-02 -3.08432151e-02  1.26092229e-02\\n -6.18690718e-03 -2.57662591e-02 -1.68368313e-02 -7.57330237e-03\\n  4.56445338e-03 -1.60488505e-02 -2.02191006e-02  4.45965305e-02\\n -4.96651325e-03 -3.84110101e-02  9.24472511e-03  3.45768742e-02\\n  1.03651080e-03  1.76278446e-02  2.76533514e-02 -1.15569057e-02\\n  5.87051325e-02 -3.56884971e-02 -2.87300963e-02 -2.37064101e-02\\n -1.69276856e-02  3.36665027e-02  3.36459726e-02 -2.99098417e-02\\n  8.77889711e-03  4.51542847e-02 -3.76779139e-02 -4.49726917e-02\\n -4.01268229e-02 -1.81724690e-02  1.03645762e-02 -5.52799739e-02\\n  4.00160626e-02 -7.99688231e-03  1.38769476e-02 -3.96269858e-02\\n  4.62416746e-02 -2.35519409e-02  2.82121152e-02 -9.42521263e-03\\n -1.07093528e-02  5.94074884e-03  1.91091467e-03 -6.56647906e-02\\n  2.23177653e-02 -3.61605324e-02  2.67892852e-02  7.81264342e-03\\n  3.99135612e-02 -3.45928408e-02 -7.59290233e-02 -3.16317938e-02\\n  1.55075621e-02 -1.81463454e-02  3.36200893e-02  2.22787391e-02\\n  6.18067291e-03  1.14161782e-02 -2.53043529e-02 -4.38521206e-02\\n  3.54679227e-02  1.44036207e-02 -1.45248100e-02  7.75321722e-02\\n -9.76233371e-03 -1.52476076e-02 -3.60811949e-02 -1.17183914e-02\\n  2.24555098e-02  2.46865284e-02  1.39941983e-02 -3.36805321e-02\\n -7.47406259e-02  2.00528465e-02  4.11379337e-03  2.37011407e-02\\n  3.77985695e-03 -1.14393987e-01 -9.41597670e-03  3.91592388e-04\\n  1.80047806e-02  6.44861162e-02  3.68321389e-02 -8.89973994e-03\\n -3.41563555e-03  3.68757136e-02 -3.58470865e-02  2.74843331e-02\\n -7.14275986e-02 -4.72264178e-02 -8.89869109e-02 -7.62153836e-03\\n  3.44949216e-03  9.35954377e-02 -1.70243718e-02  1.13247633e-02\\n -2.09949668e-02 -2.27905959e-02  3.75897884e-02 -1.23153105e-02\\n  4.38747508e-03  3.21871117e-02 -5.26585355e-02  2.29489580e-02\\n -2.15857178e-02 -4.55401503e-02  5.91540560e-02 -2.39825342e-02\\n -1.41587295e-02 -2.53754612e-02 -6.05496438e-03 -1.42327454e-02\\n  2.20869891e-02  1.32352775e-02  2.33146586e-02 -7.46175041e-03\\n -5.32323308e-02 -3.10071651e-02  4.88562835e-03 -1.66404452e-02\\n -2.33855341e-02  6.27201498e-02  1.92720871e-02 -1.09469118e-02\\n -1.85688287e-02 -6.80679083e-03 -4.98458445e-02 -8.63594003e-03\\n -4.41033542e-02 -1.57031585e-02  3.19217257e-02 -1.62515212e-02\\n -1.00005887e-01 -3.63146290e-02  3.05397343e-02  4.89004441e-02\\n -3.52355167e-02 -8.82897805e-03 -6.65889457e-02  3.84520018e-03\\n  7.17536453e-03  2.39415038e-02 -7.59154372e-03 -3.13886106e-02\\n  2.62147877e-02  5.53418659e-02 -9.83880833e-03 -2.90683936e-02\\n  5.97846555e-03 -2.94721071e-02  7.72288665e-02 -5.08578122e-02\\n -3.37611698e-02  1.27155967e-02  5.42458482e-02 -3.33251506e-02\\n -1.74638107e-02  1.03394434e-01  3.20663815e-03 -6.43540248e-02\\n -1.99561547e-02  3.84295755e-03 -6.49855065e-04  4.35765758e-02\\n  5.19412421e-02 -8.52091983e-02 -3.34297642e-02  1.70024615e-02\\n  6.80367602e-03  8.73066578e-03  1.06000388e-02 -1.07408327e-03\\n -4.16315794e-02  1.62414964e-02  2.54197372e-03 -3.60066295e-02\\n -3.43773440e-02  1.48189645e-02 -2.78585088e-02  2.73458231e-02\\n  3.18241902e-02  5.68584539e-03  3.71313235e-03 -5.00686802e-02\\n  1.17390212e-02 -5.15478605e-04 -4.74381745e-02 -2.58118939e-02\\n -5.30532524e-02  2.14420557e-02  8.15836713e-02  2.97920872e-02\\n -3.43417861e-02 -3.44067141e-02  3.67788449e-02 -3.17813195e-02\\n  3.41261514e-02 -3.03012170e-02  9.52703506e-02 -2.00030562e-02\\n -4.56628809e-03 -4.17047320e-03  1.71299670e-02 -3.45328189e-02\\n  2.85206072e-04 -6.58550533e-03  7.25651905e-02 -1.97945014e-02\\n -2.70232465e-03  4.10627574e-03 -1.46059431e-02 -1.07470900e-02\\n  5.31796217e-02  2.28871945e-02 -5.87900802e-02 -4.69269715e-02\\n -4.53583077e-02  3.95529112e-03 -6.59944564e-02 -2.48500183e-02\\n  3.23941596e-02 -3.61162350e-02 -2.74455547e-02 -4.16413248e-02\\n -2.41926908e-02  2.46926919e-02 -1.84846353e-02  5.02936356e-03\\n  4.48022597e-03  5.67346299e-03 -2.10266430e-02 -4.22437340e-02\\n -3.40126455e-03 -3.89763736e-03  3.59525643e-02 -1.14408657e-01\\n -7.77969509e-03  6.19627014e-02 -6.34614825e-02  1.52174337e-03\\n -4.19909284e-02 -1.09373711e-01 -2.84187607e-02  6.44996809e-03\\n  5.80561198e-02  2.11055372e-02  2.93015298e-02 -4.21754457e-02\\n -1.08749747e-01  3.17367129e-02  5.59197702e-02 -4.02115583e-02\\n  7.06467871e-03  5.14159277e-02 -1.70576889e-02 -8.48587230e-03\\n  3.31320651e-02 -2.14378312e-02  3.59067954e-02  2.92121824e-02\\n -8.90890881e-03 -5.11454903e-02 -5.56260869e-02  2.72779074e-02\\n -7.00915465e-03  5.66901416e-02  9.32399277e-03  2.07803193e-02\\n -8.08072370e-03  4.51832116e-02  3.00026350e-02  3.94235249e-04\\n -3.04907188e-02 -1.45564238e-02  3.30717973e-02 -1.92639101e-02\\n  1.59937311e-02  3.90163413e-03 -3.39812376e-02  1.75653491e-02\\n -2.72728950e-02  2.14887690e-02 -1.77069195e-02  2.52318615e-03\\n  1.70422476e-02  4.04039882e-02 -9.15828999e-03  2.41689309e-02\\n  2.63609365e-02  3.28020975e-02 -5.15371282e-03  1.74953528e-02\\n  6.25237357e-03  3.58814895e-02 -3.08922138e-02  3.99993211e-02\\n -5.12981042e-02  2.24432233e-03 -2.51586623e-02  2.86139287e-02\\n  6.36019856e-02  1.05515029e-03  3.98781039e-02  2.06970088e-02\\n -2.69131083e-02 -1.31760687e-02  2.16791984e-02 -3.70693132e-02\\n  6.27961522e-03 -5.52184705e-04 -8.28764215e-03 -3.23767401e-03\\n -2.74256840e-02  4.67184037e-02 -3.07893381e-02  1.48715610e-02\\n  1.01598417e-02 -2.36356594e-02 -3.08372565e-02 -7.65801966e-03\\n  6.71193050e-03  5.44266868e-03 -1.05370004e-02 -7.53101520e-03\\n  1.13464715e-02 -3.98401506e-02  5.29898554e-02 -9.13817249e-03\\n  3.28569766e-03 -5.44759631e-02  2.56417934e-02  7.73103610e-02\\n  4.35072492e-04 -3.14767174e-02 -4.45097424e-02  5.30536612e-03\\n  6.38942420e-02 -1.36628915e-02  1.61965117e-02 -6.43256737e-33\\n -1.16753643e-02 -5.23865856e-02  2.29035057e-02  4.90833633e-02\\n -1.97874885e-02 -2.47635283e-02  5.04485890e-03 -1.97474062e-02\\n -2.79029850e-02 -1.17539465e-02 -2.98426505e-02 -4.88128653e-03\\n  2.40920335e-02  5.13135977e-02  1.92146981e-03 -3.14315595e-02\\n  5.89491874e-02 -7.02596875e-03  7.43999379e-03  6.77204551e-03\\n  9.71507560e-03  6.92908987e-02  6.95241243e-02 -2.06491426e-02\\n  2.26749163e-02 -3.35522294e-02 -3.21369469e-02  3.16972062e-02\\n -2.42600422e-02  6.70909137e-02 -5.27907023e-03  2.95471568e-02\\n  3.60327810e-02 -3.65351029e-02 -2.76201293e-02  4.26472649e-02\\n -9.62447450e-02 -3.70045826e-02  7.76007888e-04  5.31250089e-02\\n -4.30065505e-02 -6.99242726e-02  9.58269835e-02 -2.02626120e-02\\n -4.22232896e-02 -1.75826717e-03  3.62903513e-02 -4.51556034e-02\\n -3.05322297e-02 -5.30838333e-02 -2.31382251e-02 -5.70443831e-03\\n  3.91283408e-02 -3.00936941e-02  2.41696481e-02  6.55498803e-02\\n -7.24032149e-03  5.99551313e-02 -7.91231692e-02  8.56431480e-03\\n  1.40500711e-02  6.47307560e-02  4.80027758e-02  3.12678069e-02\\n -4.46352288e-02  1.72204431e-02  2.05615293e-02  8.19897652e-03\\n -2.41847262e-02  1.32795365e-03  3.91338281e-02  4.80687357e-02\\n  5.44787273e-02  2.48042624e-02  2.21863221e-02 -5.09201139e-02\\n -2.03597881e-02  2.67385039e-02 -2.31915247e-03  4.32189740e-02\\n  2.72864606e-02  1.05217323e-02  1.06011964e-02 -1.98007245e-02\\n -3.86273824e-02 -9.89808608e-03 -3.36192586e-02 -1.85261946e-02\\n  8.71562213e-03  3.16258818e-02 -1.83229074e-02 -2.21822020e-02\\n  2.32343897e-02 -3.91911082e-02  2.42987033e-02 -5.85783459e-03\\n  3.31263873e-03  2.53670998e-02  1.23656879e-03  1.43761868e-02\\n -2.81210504e-02 -4.85019907e-02 -1.92062818e-02 -2.95935068e-02\\n -2.02700347e-02  4.47292160e-03  3.99277173e-02  3.70846763e-02\\n -6.86343610e-02  1.89429577e-02 -7.84753263e-03  2.96674892e-02\\n  8.74442793e-03  2.20518950e-02 -6.54929224e-03 -1.14057530e-02\\n -1.81119703e-02  8.92233253e-02 -1.10430119e-03 -2.05476452e-02\\n -3.14121805e-02 -1.43230325e-02  1.92335751e-02  5.01527749e-02\\n -3.99010889e-02  2.03029681e-02 -5.58518954e-02  4.11735587e-02\\n  3.26254442e-02 -7.80665651e-02  1.18243035e-04  2.88732108e-02\\n  2.73198452e-07  6.53672069e-02  5.72186746e-02  2.80552916e-02\\n  2.84145717e-02  1.25013161e-02  4.01750393e-02 -2.01444468e-03\\n  2.32812297e-02  3.63022238e-02 -6.45781867e-03  2.28816718e-02\\n  1.26889618e-02  4.75690477e-02 -1.33672329e-02 -8.99832025e-02\\n  9.87471314e-04 -5.18150069e-02  3.47939357e-02 -2.58255564e-02\\n  7.67585309e-03  4.09958474e-02  1.05154507e-01  2.43559647e-02\\n -2.18028910e-02 -2.37249564e-02 -4.10996042e-02  5.25561459e-02\\n -3.63695808e-02 -9.90004092e-03  2.90671904e-02  6.56410307e-02\\n  1.55792646e-02  1.17071345e-02  7.08519854e-03 -2.87850909e-02\\n  1.62813179e-02 -6.10014133e-04  7.50790164e-02  8.23548250e-03\\n -1.32448040e-02  2.40810458e-02 -2.24326756e-02 -3.35104088e-03\\n -4.45833653e-02  5.93466796e-02 -8.31748843e-02  3.02672596e-03\\n  1.82966962e-02 -2.76490278e-03 -2.69934479e-02  1.55247441e-02\\n -2.37154905e-02 -1.74983628e-02  1.65101094e-03  5.44492193e-02\\n -2.22782674e-03 -2.40102317e-03 -5.62754683e-02  5.96727710e-03\\n -8.11816659e-03 -4.57415543e-02 -9.40111373e-03 -5.89984357e-02\\n -3.39248478e-02  6.60219137e-03 -2.34041065e-02 -3.04825343e-02\\n  2.49100087e-34  1.79822147e-02 -2.61741374e-02  1.01067200e-02\\n  3.71688679e-02  2.63925246e-03  8.70612916e-03  2.39226539e-02\\n  3.06855142e-02  1.53996947e-03 -7.22387061e-02 -1.73561741e-02]'},\n",
       " {'page_number': 2,\n",
       "  'sentence_chunk': 'The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input. Then, the decoder module takes these encoded vectors and generates the',\n",
       "  'sentence_chunk_size': 237,\n",
       "  'sentence_chunk_word_count': 37,\n",
       "  'sentence_chunk_tokens': 59.25,\n",
       "  'embedding': '[ 5.16783679e-03 -5.62438555e-02 -5.98708121e-03  5.64484522e-02\\n -3.50154117e-02  6.94627464e-02  2.08276510e-03  3.23588029e-02\\n -6.87020570e-02 -4.36560884e-02  3.37347426e-02  5.32841906e-02\\n  4.80433069e-02  6.80239424e-02  3.27081159e-02 -6.01467416e-02\\n  1.77845315e-04  1.05595784e-02 -5.00151254e-02  5.92016475e-03\\n  1.91014204e-02  2.65355076e-04  5.91821782e-03  4.38405015e-02\\n -1.26594836e-02  1.69811323e-02 -1.44991204e-02  1.69445723e-02\\n -3.82228568e-02 -3.37796509e-02 -8.29323009e-03  4.48098872e-03\\n  7.00839758e-02 -2.39640661e-02  1.45203626e-06 -1.19751096e-02\\n -4.62636091e-02 -1.43173980e-02 -2.75105406e-02  2.39771768e-03\\n  2.49007437e-02 -3.07534337e-02  6.47713523e-03  3.40743773e-02\\n  1.87599361e-02 -4.39999392e-03  4.88139354e-02  4.41592885e-03\\n  5.87424152e-02  5.05233444e-02 -1.12746088e-02 -5.90841286e-02\\n  5.76334773e-03  4.13706824e-02  8.02514888e-03 -1.48464926e-02\\n  4.28616293e-02  5.32568619e-03 -1.43437972e-02  6.52738139e-02\\n -5.87669052e-02  2.56916732e-02  1.67957284e-02 -2.54830602e-03\\n  4.11676988e-02  1.18613439e-02  6.31809756e-02 -7.85949156e-02\\n -1.93388984e-02 -3.05274129e-02  3.02966349e-02 -2.52506249e-02\\n -1.39134442e-02  3.14481221e-02 -1.21051371e-02 -2.37548463e-02\\n -2.87437178e-02 -2.97443308e-02 -2.36275923e-02  3.71118598e-02\\n  1.41302990e-02  5.72917648e-02 -5.08601852e-02 -7.76676014e-02\\n -2.75801308e-02  1.83098335e-02 -1.47178816e-03 -2.55260821e-02\\n -3.83745097e-02  1.83823742e-02  6.24813475e-02 -4.75342758e-02\\n  7.09054098e-02  3.58322784e-02  4.93654348e-02 -3.05163441e-03\\n -3.12484968e-02 -5.54578342e-02 -2.39938200e-02 -9.44834501e-02\\n -3.55532542e-02  4.80377749e-02  3.80437970e-02  9.47202090e-03\\n  1.83516182e-02  4.94155772e-02 -8.36983975e-03  4.28742878e-02\\n -3.94366123e-02 -2.12810412e-02 -1.24790240e-02 -3.78881544e-02\\n -5.27445264e-02  4.69443575e-02 -5.76613192e-03 -1.25396913e-02\\n -2.73138769e-02  2.12961975e-02  2.66288370e-02  2.94299293e-02\\n -9.13499668e-03 -1.84491575e-02  1.53655438e-02  2.31900197e-02\\n -7.10632950e-02  3.11948713e-02 -3.51544917e-02 -5.00281081e-02\\n -1.99073670e-03 -3.76342200e-02  1.99509086e-03  1.04419803e-02\\n  4.54774722e-02 -8.51363223e-03 -2.57312786e-02  7.51946680e-03\\n -2.10006703e-02  2.84762252e-02  8.35769577e-04 -1.30337989e-02\\n  4.07617725e-02  1.94478054e-02 -8.05085152e-03 -2.16542762e-02\\n  3.86269279e-02  7.04704300e-02 -2.50302553e-02 -5.06883673e-02\\n  7.08691543e-03  3.77491117e-02 -3.19500268e-02  6.12188391e-02\\n -1.43502569e-02 -2.33206805e-02  7.06815273e-02 -6.51804032e-03\\n  7.61482567e-02  2.34567821e-02 -1.02967827e-03  1.68419667e-02\\n -1.12486468e-03 -3.40572633e-02  6.10796213e-02  1.37498463e-02\\n -1.12723187e-02 -1.78110506e-02 -4.41067740e-02  4.37389500e-03\\n  4.20465767e-02  1.25166634e-03  3.25263999e-02  6.01146091e-03\\n -5.59665225e-02  3.74666564e-02  8.73284228e-03  8.56111124e-02\\n  9.69903693e-02  1.65972533e-03  3.92401293e-02  2.51609124e-02\\n  6.52664900e-03  3.21257338e-02  2.48918701e-02  5.71303405e-02\\n -3.56819145e-02 -3.58219072e-02  5.52579621e-03  2.96025295e-02\\n -6.37712982e-03 -3.71386111e-02  1.22520868e-02  1.46550927e-02\\n  4.18827236e-02 -3.58566642e-02  1.73398666e-02  7.15269847e-03\\n -6.66075125e-02  8.31512641e-03 -3.26412916e-02 -3.10962666e-02\\n  5.89158125e-02  1.53235073e-04 -1.98074747e-02  9.56711993e-02\\n  6.09580949e-02  1.59168094e-02  5.72644956e-02 -2.90086772e-02\\n -1.00308210e-01 -7.76551384e-03 -4.72401455e-02 -9.48230457e-03\\n -5.03358096e-02 -5.35785109e-02 -5.46525605e-02  3.35945785e-02\\n  7.29303807e-03  1.30757699e-02 -4.77487110e-02  1.80820376e-02\\n -1.31537225e-02  6.93830848e-02  1.50718112e-02 -7.19628064e-03\\n -1.82607919e-02 -4.14616317e-02 -4.01404426e-02  2.06811652e-02\\n -3.42243016e-02  2.87192012e-03  3.98779614e-03  3.18820328e-02\\n  3.80107984e-02 -1.58803933e-03  8.53295345e-03  8.54954589e-03\\n -4.06785496e-02 -1.76872090e-02  8.35289247e-03 -6.98068142e-02\\n  4.45821397e-02 -7.49530364e-03  6.51909113e-02 -2.10167002e-02\\n  2.22490262e-02 -3.36436518e-02  3.33083011e-02 -3.64372060e-02\\n  2.54180320e-02 -4.02273014e-02 -9.03317612e-03  3.18703540e-02\\n  2.94044912e-02  5.96994068e-03  1.32018412e-02  1.35078048e-02\\n  9.14831832e-03  2.72117485e-03 -1.52905351e-02  3.87358293e-02\\n  3.66933830e-02 -2.17818525e-02  1.65305994e-02  1.24097494e-02\\n -5.11548929e-02  2.52339896e-02 -4.06128056e-02  1.06550287e-02\\n -1.73076894e-02 -1.91092864e-02  1.90959852e-02  2.89567243e-02\\n -7.29095144e-03  2.39087306e-02  2.51150336e-02 -1.88350864e-02\\n  2.13355204e-04 -2.40447447e-02 -4.51066233e-02 -2.44649919e-03\\n -3.70281911e-03 -2.24746298e-03 -3.42744635e-03  8.50198558e-04\\n  1.62127404e-03  8.83809105e-02  8.66645481e-03 -6.73893318e-02\\n -8.72154236e-02 -1.44595439e-02  3.17310579e-02 -5.92885120e-03\\n  2.77195983e-02 -6.76188767e-02  8.98184534e-03 -1.78875085e-02\\n  2.39781197e-03 -6.66100252e-03  9.43725370e-03  1.25946820e-01\\n -2.02182238e-03 -7.68939266e-04 -3.76927815e-02 -8.61292798e-03\\n -2.82727908e-02  4.07908261e-02 -6.05759025e-02 -4.45373356e-02\\n  6.55296259e-04  7.16077089e-02  1.85689908e-02  4.80008014e-02\\n -6.56850561e-02 -7.35545531e-03  1.00051574e-02 -2.19847895e-02\\n -8.09029937e-02 -6.22575991e-02 -6.15041070e-02  5.68999536e-02\\n -1.97574161e-02 -3.76748922e-03  1.47410762e-02 -3.15964818e-02\\n -8.32697470e-03 -5.61577566e-02  1.85290352e-02  1.12159634e-02\\n  4.81696287e-03  2.94532906e-02  1.17788287e-02 -7.55596557e-04\\n -5.02245128e-02  8.19862261e-02  6.64243326e-02  3.79380472e-02\\n -6.89129680e-02 -1.41923232e-02  6.90730102e-03 -4.12738398e-02\\n -1.85303632e-02  3.52938846e-02 -7.11020839e-04  3.48442607e-02\\n  3.03643886e-02  6.19820086e-03  5.56896590e-02  2.26685926e-02\\n -3.07892524e-02  4.02629981e-03  5.41802160e-02  2.98993085e-02\\n  1.38825253e-02  1.59338079e-02 -6.99910074e-02  3.20813768e-02\\n -3.12888883e-02  7.16247177e-03 -5.29535562e-02 -2.40043607e-02\\n  5.09106256e-02  9.86135844e-03 -6.38291165e-02 -9.26290639e-03\\n -1.40161882e-03  7.57665792e-03  5.51238284e-02  6.70989882e-03\\n  3.48744281e-02  6.02389649e-02  1.11535145e-02  4.84619377e-04\\n -7.96598475e-03  3.98014225e-02  2.33954974e-02 -7.14472011e-02\\n -1.21554052e-02 -1.74853392e-02 -2.39638779e-02  4.95169088e-02\\n -5.92899788e-03 -5.81027791e-02  3.89929079e-02  1.13792508e-03\\n -7.83179712e-04 -6.45256741e-03  4.68971347e-03  1.48774898e-02\\n -4.20141891e-02  4.12728973e-02  2.09522340e-02 -6.05194308e-02\\n -1.05822586e-01 -1.81891751e-02 -2.50027757e-02  4.26604562e-02\\n -3.86200473e-03  5.93636520e-02  2.13761739e-02  3.80810946e-02\\n  5.71728349e-02  1.10523077e-02 -3.98442745e-02 -7.01903254e-02\\n -1.99957695e-02  3.07169724e-02  6.03113957e-02  1.74572952e-02\\n -1.38260089e-02  3.79913091e-03 -6.36227876e-02 -9.07595307e-02\\n  1.32321632e-02 -3.85554694e-02  3.72576006e-02  1.32509293e-02\\n -3.69893992e-03 -1.76353250e-02  3.23316343e-02 -3.26488842e-03\\n -4.13995993e-04 -2.81479508e-02  2.82102413e-02 -4.99812029e-02\\n  1.97062735e-02 -3.37612233e-03 -3.19224346e-04  2.75232624e-02\\n  9.43030044e-03  5.51134385e-02  1.96358655e-03 -7.28680799e-03\\n -9.23948810e-02  7.46774767e-03 -5.90445437e-02 -6.71162009e-02\\n -2.02565212e-02  9.41126794e-03 -4.68471609e-02 -4.12966944e-02\\n  1.79738887e-02  4.41878028e-02  1.62410121e-02  4.56191786e-03\\n -1.02127688e-02  4.62144837e-02 -3.82768065e-02 -4.35415283e-02\\n -7.59731159e-02  1.38030136e-02  2.08778232e-02  3.12887840e-02\\n -3.00355051e-02  2.51656491e-02 -1.50095951e-02 -2.68811639e-02\\n -5.52911535e-02 -1.44165866e-02 -2.30091549e-02  1.41833555e-02\\n  2.78698523e-02  2.72593331e-02  3.25347520e-02 -2.05427445e-02\\n -2.00789794e-02  1.38734281e-03 -1.58245508e-02 -5.31162731e-02\\n  7.21360743e-03 -2.84280423e-02  4.72833563e-06  2.28502904e-03\\n -1.28913205e-02  4.75912029e-03 -1.89215839e-02 -1.88692883e-02\\n -3.83679904e-02 -2.63435394e-02 -5.76115921e-02  5.19229621e-02\\n -8.63342639e-03 -4.05410826e-02  2.29362156e-02 -7.37691578e-03\\n  1.24184554e-02  2.33183708e-02 -4.08389047e-02 -8.78225174e-03\\n -3.40163931e-02  2.93889036e-03  1.48672769e-02  1.02591363e-03\\n  2.64781304e-02  4.05447260e-02  3.15657370e-02 -2.09739655e-02\\n  1.71220228e-02  6.10901639e-02  2.68969424e-02  2.21142806e-02\\n -7.01370742e-03  1.08386474e-02 -1.04354380e-03  2.70007095e-05\\n  2.54193624e-03  2.69858278e-02 -5.51524349e-02  3.50906365e-02\\n -2.14883150e-03  4.99129184e-02  2.76725423e-02  1.23196212e-03\\n -2.05921307e-02  3.80357280e-02 -1.34656457e-02 -8.67043063e-03\\n  2.11537629e-02  3.22273821e-02  2.75689270e-02  1.64890755e-03\\n  2.51050498e-02 -1.93317737e-02  2.42985487e-02 -3.86066660e-02\\n  1.42070353e-02 -1.10702468e-02 -1.46712484e-02 -1.02807404e-02\\n -8.92954320e-02  3.94902416e-02 -4.92239930e-02  3.02572101e-02\\n -2.04095035e-03 -4.23806682e-02 -1.29982205e-02 -5.23614325e-03\\n  4.96031642e-02 -7.04117294e-04 -3.14180516e-02  1.16423965e-02\\n  3.70303579e-02 -1.61024481e-02 -1.70379411e-02 -4.12573136e-04\\n  4.77529205e-02 -4.43530232e-02  2.78406963e-02 -1.04256934e-02\\n -3.98394726e-02 -1.35572348e-02  1.81898717e-02  2.35182419e-02\\n -3.90078016e-02 -7.65798055e-03  6.19381899e-03 -5.59465112e-33\\n -5.82801029e-02 -3.36967707e-02  2.04136819e-02  1.76954083e-02\\n -2.77197547e-03 -4.77713980e-02  3.28782364e-03 -2.26747841e-02\\n  2.31388323e-02 -2.68093497e-02  4.03426550e-02 -2.05642683e-03\\n  1.33560700e-02  1.85013115e-02 -1.29158022e-02 -1.50960246e-02\\n  3.30035202e-02 -2.91557070e-02 -3.30690760e-03 -6.26147812e-05\\n  3.61472666e-02  7.01706856e-03  5.53080514e-02 -4.59425226e-02\\n -3.30949156e-03 -1.40724769e-02 -1.92094035e-02 -1.76287442e-02\\n  3.86193320e-02  2.63115875e-02 -1.61786005e-02  1.91520620e-02\\n  1.53007787e-02 -7.41959736e-02 -3.65525694e-03  8.72826502e-02\\n -9.33672339e-02 -1.03347132e-03 -1.30320759e-02  5.05268164e-02\\n -4.86010946e-02 -1.06016532e-01  4.22841497e-02 -3.76916607e-03\\n -5.16434312e-02  1.54680274e-02 -1.05155900e-03 -1.74643937e-02\\n -6.92364154e-03 -1.08738644e-02 -3.31273861e-02  1.31749026e-02\\n -2.19652671e-02  4.19267006e-02  6.20809160e-02  2.77822763e-02\\n  2.56900396e-03 -2.98667476e-02 -2.03187391e-02  2.08903886e-02\\n  4.87784222e-02  2.61577647e-02 -8.32476746e-03  7.68474583e-03\\n -5.93979530e-05  1.19761610e-02  1.81616861e-02 -9.54116881e-02\\n -5.60068265e-02  4.71173339e-02 -2.10990384e-02  3.32125016e-02\\n  6.76663816e-02  1.01369256e-02  4.43625413e-02 -7.79752135e-02\\n  9.52969491e-03  6.26006499e-02 -3.12469229e-02 -1.98589433e-02\\n  3.58051173e-02  1.50874853e-02  3.05546168e-02 -3.72306406e-02\\n -4.06884812e-02 -9.14477278e-03 -7.27760512e-03 -6.85325563e-02\\n  3.45739420e-03  1.25111518e-02 -3.01339962e-02  4.22420502e-02\\n  2.33534276e-02 -3.37008461e-02  4.57541198e-02 -4.60221479e-03\\n  2.68349964e-02  4.34131958e-02  2.75910506e-03 -1.19052624e-04\\n  3.41912918e-02  5.08825853e-03  4.73828763e-02 -5.86119369e-02\\n -9.01134335e-04  1.21731008e-03 -5.70750684e-02  2.81066187e-02\\n -3.63761932e-02 -5.63777983e-03 -1.22406129e-02 -8.35468411e-04\\n -4.81151394e-04  1.73855051e-02 -2.39436813e-02  8.47496674e-04\\n -1.68968532e-02 -6.25008121e-02 -5.71270939e-03 -3.47765237e-02\\n  6.02194248e-03 -3.63308042e-02  3.11880908e-03  4.73894142e-02\\n -5.28293774e-02  1.33852409e-02 -7.23143741e-02 -6.17329730e-03\\n  7.54140019e-02  1.64087024e-02  7.73024687e-04  4.26106937e-02\\n  2.11436344e-07  3.49805057e-02  7.30065182e-02  5.49626388e-02\\n -6.38859300e-03  6.28139153e-02 -2.07525101e-02 -1.45933218e-02\\n  6.15424849e-02  6.21936657e-02 -5.87157048e-02  4.23549354e-04\\n -4.35648346e-03  1.55354524e-02  7.44085154e-03 -2.17042528e-02\\n -2.70856749e-02  2.72378384e-05 -2.02305075e-02 -4.65155393e-03\\n  7.38671795e-02  1.17868997e-01  7.47321248e-02 -4.26088125e-02\\n  2.07448695e-02  3.51824909e-02  1.31571200e-02  2.19882466e-02\\n  2.37058420e-02  5.05191386e-02 -1.25811063e-02 -7.59294629e-02\\n  1.49735752e-02  6.32687099e-03  9.80281178e-03 -3.24328840e-02\\n -1.16028544e-02  7.51261367e-03  7.74862617e-02  7.22064311e-03\\n -6.69223722e-03  1.39870988e-02 -1.11868009e-02 -4.21047956e-02\\n -3.33373435e-02  2.56479159e-02  1.02869514e-02 -2.42639259e-02\\n -9.92286857e-03  4.10839729e-03  1.08032208e-02 -6.89818561e-02\\n  1.78754656e-03  1.74119999e-03 -2.95270537e-03  3.95066850e-02\\n  1.72477029e-02  4.80997656e-03 -4.31682682e-03  9.53693874e-03\\n -3.78526784e-02 -5.47442064e-02 -4.83260304e-02 -4.52979431e-02\\n  3.76216369e-04  3.72538380e-02 -4.19180468e-02  1.79806852e-03\\n  2.02008589e-34  1.60103105e-02 -1.43349096e-02 -7.79724820e-03\\n  5.26595637e-02  7.18835182e-03 -2.63914857e-02  4.59500682e-03\\n  2.12606378e-02  7.68815307e-03 -6.93273768e-02 -4.56463769e-02]'},\n",
       " {'page_number': 3,\n",
       "  'sentence_chunk': 'output text. In a translation task, for example, the encoder would encode the text from the source language into vectors, and the decoder would decode these vectors to generate text in the target language. Both the encoder and decoder consist of many layers connected by a so-called self- attention mechanism. You may have many questions regarding how the inputs are preprocessed and encoded. These will be addressed in a step-by-step implementation in subsequent chapters. A key component of transformers and LLMs is the selfattention mechanism (not shown), which allows the model to weigh the importance of different words or tokens in a sequence relative to each other. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output. Later variants of the transformer architecture, such as BERT (short for bidirectional encoder representations from transformers) and the various GPT models (short for generative pretrained transformers), built on this concept to adapt this architecture for different tasks. BERT, which is built upon the original transformer’s encoder submodule, differs in its training approach from GPT. While GPT is designed for generative tasks, BERT and its variants specialize in masked word prediction, where the model predicts masked or hidden words in a given sentence.',\n",
       "  'sentence_chunk_size': 1431,\n",
       "  'sentence_chunk_word_count': 215,\n",
       "  'sentence_chunk_tokens': 357.75,\n",
       "  'embedding': '[ 1.70865115e-02 -5.70335053e-02 -1.65333133e-02  4.70145904e-02\\n -3.05545535e-02  3.19243744e-02  1.48926908e-02  1.07399672e-02\\n -4.06434722e-02 -4.76173423e-02 -2.64052544e-02 -1.39088416e-03\\n  2.58639790e-02 -8.19109194e-03  7.04912245e-02 -4.73712906e-02\\n  2.09932141e-02 -6.89550582e-03 -6.89124689e-02 -2.70773843e-02\\n -5.70377801e-03 -1.35042900e-02  1.19999880e-02  3.60806435e-02\\n -1.92601737e-02 -9.30392276e-03 -3.97107657e-03  7.73646729e-03\\n -6.65891357e-03 -5.13421232e-03  4.51581459e-03  2.25376226e-02\\n  2.53251493e-02  2.21460145e-02  2.06787126e-06 -4.98379208e-02\\n -2.28484329e-02 -1.52527718e-02  3.55036254e-03 -2.98683979e-02\\n -2.48128287e-02 -2.40985900e-02 -1.79771315e-02  2.59224232e-02\\n -1.40925089e-03 -4.05952940e-03  9.55746323e-02  7.07143247e-02\\n  4.71982956e-02  7.75600225e-02 -6.01951079e-03 -9.07098502e-02\\n  1.08343968e-02 -1.64752193e-02  1.93765927e-02 -6.98012672e-03\\n  3.08964550e-02 -7.09849447e-02 -5.92641570e-02  1.77082382e-02\\n -4.22099568e-02  1.58200245e-02  1.43365534e-02  2.34035905e-02\\n  2.80883703e-02  3.77136059e-02 -1.42514510e-02 -6.91395327e-02\\n -1.61703825e-02 -9.96723212e-03 -2.77356468e-02 -1.95439272e-02\\n -1.45240165e-02  7.54358666e-03 -1.02098025e-02  2.24010441e-02\\n -1.46535486e-02  1.81445610e-02  1.56615041e-02  5.87547161e-02\\n  3.90050095e-03  4.97245463e-03 -1.64481215e-02 -4.97331508e-02\\n -3.81173268e-02  4.53954339e-02  2.26041004e-02 -5.10919839e-02\\n -4.83863465e-02  2.91648470e-02  3.00455783e-02 -5.87076284e-02\\n  5.84024377e-02  6.29632771e-02  6.12937845e-02  4.20715846e-03\\n -1.85901262e-02 -4.48050462e-02 -1.56281411e-03 -6.91494271e-02\\n -1.72828287e-02  3.52106355e-02 -3.81939486e-02 -1.92485854e-03\\n -5.86103164e-02  9.89404619e-02 -6.43925741e-02  8.04367438e-02\\n -6.24761730e-02 -1.05494102e-02  2.04326469e-03 -5.00508137e-02\\n -7.83309937e-02  7.38434568e-02  1.87609601e-03  7.04426493e-04\\n -4.38465029e-02  3.54852714e-02  3.58210690e-02 -1.38377650e-02\\n  3.53477560e-02 -2.82850061e-02 -6.89960197e-02  1.73991523e-03\\n -4.52746302e-02  3.08844056e-02  4.39895550e-03 -2.63082366e-02\\n -1.05033135e-02 -2.05854774e-02 -1.65552162e-02  1.40813179e-02\\n  4.59266966e-03 -2.12071352e-02 -1.66634042e-02  8.99560377e-02\\n -1.16650192e-02 -2.83040758e-02 -4.70799655e-02  4.76887710e-02\\n  1.82322711e-02 -2.77259536e-02  4.12367694e-02 -1.24862632e-02\\n -7.14148022e-03  5.41584119e-02 -2.15400439e-02  1.10111188e-03\\n  1.68325566e-02  1.07207205e-02 -3.57849919e-03  4.79802638e-02\\n  3.53279412e-02 -1.61337033e-02  6.13976866e-02  7.27597019e-03\\n  3.74942236e-02  2.43326314e-02  1.35141611e-02  3.87493446e-02\\n -9.90074966e-03 -3.19009759e-02  4.42235656e-02  3.27964835e-02\\n  5.41333994e-03 -2.30902005e-02 -1.13284234e-02 -3.13733034e-02\\n  9.55669209e-03  1.86115894e-02  2.69446746e-02  3.74402106e-02\\n -1.68571956e-02  1.77110303e-02  6.73872530e-02  6.26039058e-02\\n  6.07990772e-02  8.97608176e-02  1.61204860e-02  1.43916951e-02\\n  1.46645466e-02  6.42601401e-02 -1.33109894e-02  6.67356402e-02\\n -1.27322758e-02 -6.42946362e-03 -1.54731963e-02  4.80690822e-02\\n -2.53464077e-02 -3.42675485e-02 -1.39572145e-02  7.16631953e-03\\n -1.53265172e-03 -5.80989681e-02  1.04557192e-02  4.33750115e-02\\n -9.81596112e-02  1.17214575e-01 -5.12411185e-02 -3.49556170e-02\\n  1.43999243e-02 -4.27538566e-02 -4.10049148e-02  2.29285788e-02\\n  1.78067256e-02 -6.33225730e-03 -3.35217677e-02 -1.18817259e-02\\n -7.70516470e-02  8.33454430e-02 -4.02659960e-02  4.37815003e-02\\n -4.36916277e-02 -3.45347971e-02 -3.24207060e-02  2.71331742e-02\\n -1.73300784e-02  3.49362083e-02 -4.29201499e-02 -1.03049381e-02\\n -9.25905537e-03 -1.00990543e-02  5.37917241e-02  2.69494075e-02\\n -8.83940887e-03 -6.67505637e-02 -1.47818485e-02  7.24653527e-03\\n  5.92723116e-03 -1.04841788e-03 -1.55079085e-03  4.13125493e-02\\n  3.24094296e-02 -3.28211002e-02 -4.06762436e-02  2.55513769e-02\\n -7.42253289e-02  1.29655132e-03  1.18537685e-02 -4.67878431e-02\\n  3.05733532e-02  2.42178235e-02  2.93279756e-02 -5.60703613e-02\\n  5.19860629e-03 -6.99228793e-02  6.82241917e-02 -4.46254797e-02\\n  2.87203677e-02 -8.58016894e-04  8.99478886e-03 -1.26498379e-02\\n  4.84602749e-02 -1.04785943e-02 -3.82491946e-03 -1.06886430e-02\\n  3.31838056e-02  2.57988330e-02 -1.32012730e-02 -4.68796631e-03\\n  5.33203185e-02 -1.72608905e-02  1.04280347e-02  3.92516926e-02\\n -2.65376512e-02 -2.62940559e-03 -6.76667914e-02 -3.88000868e-02\\n -1.46485949e-02 -7.27805833e-04  5.05587645e-02  1.85325332e-02\\n -2.01384593e-02 -1.15672406e-03  1.99309341e-03 -3.04407962e-02\\n  1.10937143e-03 -2.63004098e-02 -1.21014174e-02  5.80456629e-02\\n -4.06090636e-03 -8.07604101e-03 -2.19967458e-02 -2.09753010e-02\\n -7.61284755e-05  6.61358461e-02  1.40360696e-02 -3.52999493e-02\\n -8.40845779e-02 -4.39952798e-02  2.41891295e-02  3.67648192e-02\\n  1.39522199e-02 -1.01836734e-01  1.83713529e-02  1.80456620e-02\\n  9.97708440e-02  1.71882380e-02  5.07094674e-02  4.69681323e-02\\n -1.24277582e-03 -8.40196479e-03 -2.36970522e-02  8.72299261e-03\\n -7.15220347e-02 -1.31615149e-02 -5.62160015e-02 -6.10794080e-03\\n  2.62266294e-05  1.03433073e-01  4.03847732e-02  8.38496606e-04\\n -8.92615542e-02 -7.11396849e-03  2.40296721e-02 -9.62723512e-03\\n  3.25244591e-02 -4.56896238e-02 -7.32288361e-02 -3.94370407e-04\\n  1.83838431e-03 -1.66534185e-02  4.54601459e-02 -5.04112467e-02\\n -4.68186568e-03 -4.76325080e-02  5.46121523e-02 -9.83390305e-03\\n  7.17493938e-03  2.47491784e-02  1.57188382e-02  8.49971920e-03\\n -4.41557840e-02  6.38818881e-03  2.80774347e-02  3.03861964e-02\\n -2.27129832e-02  6.01315312e-03 -1.37116091e-04 -2.62808241e-02\\n -2.08233632e-02 -1.85992084e-02 -2.33542938e-02  4.38354947e-02\\n  2.99314298e-02  7.81393517e-03  4.81084995e-02 -5.08095697e-03\\n -6.24554902e-02 -4.73539159e-03  4.94432412e-02  5.05383313e-02\\n -1.25148185e-02  3.15668248e-03 -5.74960299e-02  1.96600184e-02\\n -3.68929692e-02 -2.38309591e-03  1.09220184e-02 -1.48197077e-02\\n  1.40246330e-02  3.90987901e-04 -2.39430759e-02 -1.85160730e-02\\n -2.98465788e-02  2.64665089e-03  7.39944130e-02 -4.38833237e-02\\n -2.31166109e-02  2.15481967e-02  4.24975678e-02 -2.06735209e-02\\n  3.53189278e-03  1.04395740e-01 -1.79589707e-02 -4.74846438e-02\\n -6.49178075e-03 -4.75871488e-02 -2.85027307e-02  2.95771267e-02\\n  1.67038701e-02 -5.41331694e-02  2.34542973e-02 -5.76107157e-03\\n -7.42035080e-03  1.51148369e-03 -7.50699639e-03  2.62763426e-02\\n -4.83573675e-02  3.76620516e-02 -9.98958107e-03 -1.23177283e-02\\n -5.15753292e-02 -6.09409250e-03 -2.39380915e-02  2.08033416e-02\\n  2.80876132e-03  9.17234924e-03 -3.30995047e-03 -9.62241739e-03\\n  4.94305901e-02 -1.45863555e-02  7.36806076e-03 -2.98580304e-02\\n -1.48791261e-02 -9.14054341e-04  1.01013318e-01  2.10773908e-02\\n -5.62081747e-02 -6.63144886e-02  3.34637575e-02 -3.91841494e-02\\n  2.26643737e-02 -2.09384076e-02  3.74238081e-02 -1.03867520e-02\\n -1.63796470e-02 -1.80419218e-02  9.16969776e-03  1.95683371e-02\\n -2.34447960e-02 -4.12132517e-02  5.81996255e-02 -3.25331725e-02\\n -1.04973549e-02  7.12796766e-03 -1.82611290e-02 -2.33400632e-02\\n  2.69368552e-02  3.50790098e-02 -2.63244193e-03 -3.94453295e-02\\n -2.63916478e-02  5.55364117e-02 -4.53201383e-02 -4.66517396e-02\\n -5.63341193e-03 -3.01700868e-02 -6.03726320e-02 -5.64196520e-02\\n -4.78911027e-03 -1.91248115e-03 -6.90746168e-03  3.52216028e-02\\n -1.87797882e-02  4.19120938e-02 -1.76792983e-02 -3.31540778e-02\\n -3.21282400e-03  3.86154465e-02 -2.20222641e-02 -3.73784304e-02\\n -4.43864129e-02  5.17408773e-02 -4.65812162e-02 -1.38634602e-02\\n -4.05532718e-02 -4.01680209e-02 -3.19179371e-02  3.69814634e-02\\n  4.61189114e-02  1.72242336e-02  1.51483072e-02  7.43560726e-03\\n -3.80944386e-02  4.85261939e-02  1.89592317e-02 -9.90374666e-03\\n  2.30103061e-02  3.80982682e-02 -1.37308538e-02 -8.37502349e-03\\n -3.76875419e-03  6.48135203e-04  4.33412232e-02 -1.09808687e-02\\n  1.47433113e-03  1.39953783e-02 -5.16499244e-02  5.07142022e-02\\n  1.47449609e-03  8.44923779e-02 -3.40290107e-02  5.43919764e-02\\n -2.12969724e-02  4.54663634e-02 -1.85573623e-02  3.29951569e-02\\n -3.57736908e-02 -2.66543101e-03  3.82262096e-02  6.80266460e-03\\n  8.95609800e-03  3.06075178e-02 -4.48031686e-02 -2.11644806e-02\\n -2.71808691e-02  7.24587515e-02 -3.47351804e-02  4.10265150e-03\\n -1.13086915e-02  2.54495423e-02 -6.87723700e-03 -1.11523010e-02\\n -4.13734000e-03  1.27511227e-03 -1.06155975e-02  5.11431955e-02\\n  2.03233436e-02  5.93299083e-02  1.05029335e-02  3.64178531e-02\\n  7.74747867e-04  7.08762277e-03 -4.54599895e-02  1.53275048e-02\\n  2.13488117e-02  4.48408816e-03  2.56224927e-02 -1.01170046e-02\\n -4.52236040e-03  1.65127609e-02  9.55337752e-03 -3.10448091e-02\\n  3.28183733e-02  1.44965965e-02 -1.57935210e-02  1.46635910e-02\\n -7.59211108e-02  5.58348596e-02 -1.96529571e-02  8.24325066e-03\\n  5.68204466e-03 -2.98868567e-02 -1.53783429e-02 -3.88038792e-02\\n  3.15645635e-02 -3.40053961e-02 -4.41516022e-04 -5.19168843e-03\\n -6.79635385e-04 -2.13114992e-02  3.09282281e-02  1.91639792e-02\\n  1.65989157e-02 -6.84312731e-02  3.86996642e-02  4.41476181e-02\\n -2.45303288e-02 -6.53504133e-02 -1.47170818e-03  4.56963368e-02\\n  1.53379673e-02  2.16482244e-02  3.29885222e-02 -6.56966013e-33\\n -5.87670393e-02 -5.07629700e-02  4.56792600e-02  3.11705917e-02\\n -3.88575159e-02 -1.66562367e-02 -9.22512729e-03 -2.83142943e-02\\n  2.32714079e-02 -2.33870605e-03 -6.47325255e-03 -2.32580025e-02\\n  1.16217034e-02  2.75730900e-02  9.98972729e-03 -3.54184695e-02\\n  3.63239273e-02 -2.71988586e-02 -1.12130418e-02 -3.25326342e-03\\n -7.70513061e-03  3.72875370e-02  1.13400519e-01 -8.40449985e-03\\n  1.44739496e-02 -5.56668080e-02 -1.16337277e-02  2.56125955e-03\\n -9.06033441e-03  3.42385657e-02 -4.06429134e-02  7.20012793e-03\\n  5.90594998e-03 -1.08928643e-01 -2.78871525e-02  8.26728344e-02\\n -5.81556186e-02 -4.33444083e-02  2.31799204e-02  4.30620275e-02\\n -6.40074685e-02 -6.77734688e-02  4.32980843e-02  2.37300945e-03\\n -6.46473914e-02 -3.60726053e-03  1.01760188e-02 -9.93977115e-03\\n  3.11089051e-03 -2.50207037e-02 -4.38801609e-02 -1.04532996e-03\\n -4.26504994e-03 -1.27109680e-02  5.30617982e-02  1.85740143e-02\\n -1.21831372e-02  2.19687149e-02 -5.05071990e-02  1.04960066e-03\\n  3.24040912e-02  7.37947300e-02  3.58575843e-02  2.92532556e-02\\n  6.49247132e-03  8.79959762e-03  6.54441491e-03 -2.27690935e-02\\n -4.67606001e-02 -1.60277393e-02 -2.82147573e-03  4.56025861e-02\\n  7.82899112e-02 -4.42533381e-02  1.75752249e-02 -2.49160710e-03\\n -1.14360303e-02  3.31196301e-02 -5.17176241e-02  1.99749563e-02\\n  3.55464071e-02  2.83823889e-02  3.24097462e-02 -6.52866671e-04\\n -1.75392982e-02 -2.74244491e-02 -1.36475079e-03 -2.04300582e-02\\n -8.52444675e-03 -1.38491858e-03 -7.61290360e-03  4.98880111e-02\\n  1.26620037e-02  3.83548351e-04  3.82792279e-02 -1.12398565e-02\\n -2.47640233e-03 -2.92432006e-03 -2.70295213e-03  3.93706150e-02\\n -2.43251007e-02 -2.66864058e-02  2.10598279e-02  9.50510614e-03\\n  3.41492146e-03  3.31427041e-03 -1.65489521e-02  5.51425703e-02\\n -5.04066497e-02  4.24800301e-03 -3.24906558e-02  8.18186440e-03\\n -2.13464871e-02 -1.20512387e-02 -4.26491611e-02 -1.72086374e-03\\n -1.65586490e-02  7.34544313e-03 -1.40206481e-03 -1.38061428e-02\\n -2.39160601e-02  5.88831753e-02  1.02744782e-02  3.48647162e-02\\n -3.19348611e-02  2.38978583e-02 -5.00386059e-02  8.43985379e-02\\n  4.77324687e-02 -4.46972586e-02 -2.99969520e-02  4.15546894e-02\\n  2.86275167e-07  6.83784634e-02  6.06271438e-02  1.01432867e-01\\n  2.65408736e-02  3.44598703e-02  1.75216030e-02 -9.34142433e-03\\n  7.44862184e-02  5.24606407e-02 -1.58870909e-02  1.35140689e-02\\n  1.93724148e-02 -1.72233637e-02  8.99224367e-04 -4.02651019e-02\\n -3.31957731e-03 -4.62905914e-02  2.24236976e-02 -1.50562646e-02\\n  4.62331809e-02  4.78721820e-02  7.15199783e-02 -1.96713619e-02\\n  1.25741241e-02 -3.67091633e-02  7.28267571e-03  4.79280837e-02\\n -7.18667265e-03  3.76288295e-02  7.57398875e-03  2.65653152e-02\\n  8.54378846e-03 -2.58520851e-03  1.90142859e-02 -3.78182568e-02\\n  1.36615697e-03  2.38147806e-02  4.43528928e-02 -7.24252407e-03\\n -5.54559752e-02  4.48785722e-02  1.33405775e-02 -1.64881181e-02\\n -5.07040210e-02  1.09075522e-02 -7.34505728e-02 -2.66933106e-02\\n  3.37259807e-02  4.58019134e-03  1.06599769e-02 -1.25664109e-02\\n -1.54649355e-02 -2.07746923e-02 -3.72929848e-03  3.36186513e-02\\n  1.81246046e-02 -5.41588850e-03 -5.26066422e-02 -2.50954982e-02\\n -1.52481440e-02 -8.41878578e-02 -2.13370491e-02 -8.07422176e-02\\n -3.63362730e-02  3.70869972e-02 -3.13640125e-02 -2.29528956e-02\\n  2.60061687e-34 -1.02439839e-02 -3.25603113e-02 -2.30113287e-02\\n  5.81447184e-02 -9.21835750e-03 -2.90476419e-02  2.84961648e-02\\n  4.60369848e-02 -2.58398727e-02 -4.97643612e-02 -4.50102352e-02]'},\n",
       " {'page_number': 3,\n",
       "  'sentence_chunk': 'This unique training strategy equips BERT with strengths in text classification tasks, including sentiment prediction and document categorization. As an application of its capabilities, as of this writing, X (formerly Twitter) uses BERT to detect toxic content. GPT, on the other hand, focuses on the decoder portion of the original transformer architecture and is designed for tasks that require generating texts. This includes machine translation, text summarization, fiction writing, writing computer code, and more. GPT models, primarily designed and trained to perform text completion tasks, also show remarkable versatility in their capabilities. These models are adept at executing both zeroshot and few-shot learning tasks. Zero-shot learning refers to the ability to generalize to completely unseen tasks without any prior specific examples. On the other hand, fewshot learning involves learning from a minimal number of examples the user provides as input. Transformers vs. LLM Today’s LLMs are based on the transformer architecture. Hence, transformers and LLMs are terms that are often used synonymously in the literature.',\n",
       "  'sentence_chunk_size': 1134,\n",
       "  'sentence_chunk_word_count': 165,\n",
       "  'sentence_chunk_tokens': 283.5,\n",
       "  'embedding': '[ 2.65649948e-02 -2.65517384e-02  1.14763097e-03  9.63288266e-03\\n -3.47449593e-02  3.30520682e-02  8.12941510e-03  5.76709285e-02\\n  7.66744744e-03 -7.55176172e-02 -2.69381385e-02 -2.39136023e-03\\n -2.93036345e-02 -4.52899896e-02  4.20493111e-02 -6.01026192e-02\\n  2.49150880e-02  3.85630839e-02  2.86152633e-03 -1.54307000e-02\\n  2.03789156e-02  1.43068004e-02  1.92527901e-02  2.65696291e-02\\n  2.49333610e-03 -5.40220295e-04  7.59987999e-03  1.79854874e-02\\n  2.26951074e-02  5.02387574e-03  1.33474255e-02  3.11421044e-03\\n  2.63778437e-02  7.23551661e-02  1.96773749e-06 -2.55965032e-02\\n -2.40315590e-02  2.52561271e-03  2.91138776e-02 -4.83653992e-02\\n  1.80913955e-02 -2.68180724e-02 -9.41982586e-03  4.47627045e-02\\n -1.94744337e-02  1.63669756e-03  8.20445940e-02  1.01561740e-01\\n  4.25154753e-02  9.19926241e-02 -2.14618947e-02 -4.75519039e-02\\n  3.97573486e-02 -4.42013377e-03  6.92162216e-02  1.36047893e-03\\n  1.86193436e-02 -6.70650005e-02 -7.08422111e-03  5.41799143e-03\\n -2.10311767e-02  9.49769653e-03 -7.77071342e-03  2.39011608e-02\\n -4.02916782e-03  3.60317193e-02 -5.49467690e-02 -2.55571250e-02\\n -2.10972019e-02  1.64262112e-02 -1.64601579e-02 -6.02100836e-03\\n  5.80614469e-05  8.72946437e-03 -1.76463891e-02  2.73506041e-03\\n -3.12187113e-02  1.93447061e-02 -3.42955031e-02  4.65317518e-02\\n -3.86394821e-02 -3.70832346e-02  1.37363076e-02  2.14860355e-03\\n -3.21486853e-02  6.05209395e-02  5.34118153e-03 -5.21513298e-02\\n -8.62090383e-03  3.18779014e-02  3.99291515e-03 -3.55409570e-02\\n  1.22928433e-02  5.44789284e-02  4.51970883e-02  8.60168971e-03\\n -2.89597511e-02 -2.52489988e-02  2.95126382e-02 -6.13056123e-02\\n  1.18532628e-02  3.42538729e-02 -1.89932212e-02  1.41648129e-02\\n -5.53600788e-02  5.13089746e-02 -2.88465060e-02  6.98939487e-02\\n -3.62943523e-02 -7.04424875e-03 -7.02838004e-02 -2.25608218e-02\\n -3.60624567e-02  8.63774940e-02  3.86412963e-02  1.23853972e-02\\n -6.49432540e-02  5.20197861e-02  1.88182779e-02 -2.20466452e-03\\n  1.80802718e-02 -6.91349152e-03 -3.68192196e-02  1.66251119e-02\\n -2.67767254e-02  6.24524280e-02  1.11926883e-03  4.58485854e-04\\n -2.58354340e-02 -2.20973007e-02 -6.82283612e-03  2.47589424e-02\\n -6.72628870e-03 -2.00929958e-02  3.16823646e-02  9.70392302e-02\\n -1.19032124e-02 -5.42782582e-02 -9.71392989e-02  2.95328479e-02\\n -8.63163359e-03 -5.74309602e-02  2.15790384e-02 -3.22368294e-02\\n -2.29307078e-02  4.00471166e-02 -4.50105220e-03  2.49188244e-02\\n  1.59631129e-02  3.24375033e-02 -4.15489562e-02  3.93099785e-02\\n  4.82543511e-03 -2.09072605e-02  4.88909669e-02  1.42897712e-02\\n -9.16706026e-03  7.13275298e-02  6.71114260e-03  5.25974259e-02\\n  1.01781776e-02 -2.20030695e-02  3.06094531e-02  4.02007625e-02\\n -6.92990795e-03  1.42016876e-02 -2.25134175e-02 -2.70777494e-02\\n -3.82948555e-02 -5.62976999e-03 -7.13370368e-03  6.59239069e-02\\n -2.89377533e-02  3.00944652e-02  9.78809744e-02  6.70005530e-02\\n  2.62120105e-02  6.79002255e-02  4.43471270e-03  2.28767972e-02\\n  8.41640215e-03  3.17631140e-02 -3.74008045e-02  4.30038832e-02\\n -5.02571538e-02 -1.16292043e-02 -2.83086319e-02  5.43273473e-03\\n -2.98472755e-02 -2.02790387e-02 -2.59022042e-02 -2.67449394e-02\\n -1.92316121e-03 -3.72970924e-02  4.58446778e-02  9.44923144e-03\\n -8.60164240e-02  1.03402570e-01 -2.80405860e-02 -4.40063067e-02\\n -2.04116497e-02 -7.49845579e-02  1.49904573e-02  5.17149754e-02\\n  2.53439490e-02 -2.51380149e-02 -7.32690915e-02 -6.19553067e-02\\n -4.05742191e-02  5.79995997e-02 -2.43349038e-02  4.01584134e-02\\n -9.33515560e-03 -2.44391896e-02 -1.24022411e-02  1.11548631e-02\\n  9.86916199e-03  2.92766746e-03 -1.43493628e-02  2.92912759e-02\\n -2.14940496e-02 -4.85092029e-02  1.13324616e-02  2.91851759e-02\\n -3.47640947e-04 -6.90241531e-02 -1.54088745e-02  1.07853934e-02\\n -2.62088305e-03  1.70133952e-02 -2.92438567e-02 -3.30835618e-02\\n  3.05070989e-02 -4.68604686e-03 -6.36563152e-02  1.03279306e-02\\n -1.34057980e-02  5.58632100e-03  2.81851143e-02 -4.68699038e-02\\n  9.00075864e-03  5.18001094e-02 -3.31294052e-02 -8.88815336e-03\\n -1.69393159e-02 -5.86058386e-02  7.11277947e-02 -5.98124787e-02\\n  7.43063400e-03 -4.29463722e-02 -1.82690006e-02 -1.55503284e-02\\n  5.53660281e-02 -1.70707062e-03  5.08367317e-03 -1.18802721e-02\\n -2.54393090e-02  4.47613560e-02  6.12638006e-03 -5.68822809e-02\\n  4.58590239e-02  7.66893895e-03  1.88880302e-02  4.46889289e-02\\n  1.58130541e-03 -3.02479453e-02 -5.21149449e-02 -9.68563855e-02\\n  1.75401010e-02 -5.33931749e-03  4.13940363e-02  1.24857025e-02\\n -1.79707948e-02 -4.25320445e-03  8.62742309e-03 -3.31244171e-02\\n -1.47977995e-03 -9.89363715e-03  1.40392445e-02  6.46739453e-02\\n -1.19788535e-02 -1.32074673e-02  8.47422890e-03  7.88626075e-03\\n -2.09813006e-02  4.00604829e-02  4.74262564e-03 -5.32899536e-02\\n -3.22711132e-02  7.95408152e-03 -1.65008493e-02  4.45762137e-03\\n  1.12047764e-02 -9.73370895e-02  9.45872720e-03 -6.68053236e-03\\n  3.94480079e-02  3.59547399e-02  4.53321226e-02  4.45454791e-02\\n -2.64398698e-02  3.29140089e-02 -1.99038070e-02  8.44214484e-03\\n -3.86906825e-02 -4.81340066e-02 -5.04710153e-02 -1.04247974e-02\\n  1.12915281e-02  6.95476979e-02  4.46701684e-04 -7.35503389e-03\\n -5.53378500e-02 -1.75406393e-02  1.48020759e-02 -5.95662394e-04\\n  2.69020982e-02 -2.61004027e-02 -2.67588440e-02 -4.39190567e-02\\n -1.11511862e-03 -6.03704751e-02  6.42771870e-02 -9.76800360e-03\\n -1.22439058e-03 -4.24936786e-02 -1.68712586e-02 -1.67996492e-02\\n  7.85969105e-03  1.14016449e-02  3.23023051e-02  1.72213633e-02\\n -3.70531306e-02  1.83318998e-03 -6.55623106e-03  2.91421562e-02\\n -1.80416182e-02  4.57041450e-02  2.66888421e-02 -8.62779934e-03\\n -3.84039246e-02 -3.40014808e-02 -1.03249382e-02 -8.33325647e-03\\n -5.11256093e-03  1.14056515e-02  1.87520646e-02 -4.56057079e-02\\n -7.88817331e-02 -4.06893902e-03  3.11651248e-02  1.01861674e-02\\n -2.86098104e-02 -2.91819405e-02 -1.90844145e-02  1.43120298e-02\\n -2.18071993e-02 -5.14989533e-02 -1.06108747e-02 -4.31780796e-03\\n  1.22967660e-02  5.56265600e-02  1.43315308e-02 -3.24398763e-02\\n -3.58484723e-02 -1.30503280e-02  4.70220335e-02 -6.05134964e-02\\n -4.81787063e-02 -1.79472007e-03  2.79720724e-02 -2.56448779e-02\\n  3.75989936e-02  1.06712811e-01  1.22042028e-02 -6.55803457e-02\\n -3.84945087e-02 -3.43313417e-03 -1.24855218e-02  2.28110701e-02\\n  9.06721130e-03 -8.05370808e-02 -8.39071348e-03 -4.10188455e-03\\n  3.52575257e-02 -2.75436640e-02 -1.98969208e-02  8.17328319e-03\\n -6.21053688e-02  3.81791815e-02 -8.95924401e-03 -2.04047412e-02\\n -2.35485844e-02 -2.39805114e-02 -1.69055387e-02  6.04775501e-03\\n  1.60387866e-02 -1.96421668e-02 -3.12425252e-02 -1.97269041e-02\\n  3.74448113e-02 -3.13442424e-02 -1.57673974e-02 -1.12899719e-02\\n -3.27903032e-02 -3.36086005e-03  9.12661105e-02  4.13658880e-02\\n -7.02201948e-02 -4.10350151e-02  1.17764296e-02 -1.88544951e-02\\n  6.36619190e-03 -3.09537891e-02  7.47921169e-02 -3.72519009e-02\\n  1.96016440e-03 -2.13321690e-02  1.54837454e-02  1.30435042e-02\\n -1.10857037e-03 -5.61386645e-02  8.38770121e-02 -3.10127921e-02\\n -3.57552506e-02 -2.67939866e-02 -1.82581544e-02  6.44768216e-03\\n  5.58120906e-02  2.25112308e-03 -4.18959409e-02 -7.66465589e-02\\n -3.72738913e-02  1.04791149e-02 -5.09739593e-02 -2.66909543e-02\\n -1.80094093e-02 -5.25558814e-02 -6.85223117e-02 -5.57900257e-02\\n  7.60503300e-03  7.75146624e-03  2.60052620e-03 -2.70055397e-03\\n -9.96733829e-03 -2.40913662e-03  9.52609908e-03 -4.75677848e-02\\n -1.75479986e-02  3.93691473e-02 -7.84410688e-04 -6.61794916e-02\\n -1.86306387e-02  7.40577802e-02 -1.93662271e-02 -1.16385501e-02\\n -5.08638509e-02 -8.41896832e-02 -4.59015183e-02  5.63140539e-03\\n  5.68183884e-02  2.47235186e-02 -9.53699835e-03  1.91980321e-02\\n -3.30253355e-02  8.70390385e-02  1.11934267e-01  1.09088998e-02\\n  1.57145914e-02  5.03440686e-02  2.99348645e-02 -6.64548203e-03\\n  1.05816908e-02 -1.97853353e-02  3.49078812e-02 -7.40812672e-03\\n -3.27439755e-02 -2.11603958e-02  2.70691160e-02  2.98465770e-02\\n  3.81933129e-03  6.87250048e-02 -2.29556318e-02  1.45795839e-02\\n  9.70711466e-03  2.68556122e-02  5.06964419e-03  2.41807345e-02\\n -2.20360272e-02 -3.67144570e-02  4.42176126e-02 -1.62143055e-02\\n -3.77884284e-02  4.64623421e-02 -4.44133691e-02  4.99536917e-02\\n -7.39590898e-02  6.84086978e-02  2.01824843e-03 -3.36688757e-02\\n -3.99826840e-03  1.06933350e-02 -3.00232880e-02 -2.68474426e-02\\n  1.86932087e-02 -4.51310910e-03 -2.50392817e-02  5.27841896e-02\\n  2.66711060e-02  5.47839105e-02  2.39125732e-02  2.78624203e-02\\n -5.71959391e-02 -1.71986185e-02 -5.09527139e-02  4.21795808e-02\\n  4.54241484e-02  1.67555865e-02  3.45969833e-02 -2.76372153e-02\\n -2.14974284e-02  4.82319808e-03 -9.22209583e-04 -2.55658943e-02\\n  3.88200255e-03  1.12925414e-02  1.28203561e-03  3.89048504e-03\\n -3.41875292e-02  7.02099502e-02 -1.31536815e-02  4.74061035e-02\\n  1.02625659e-03 -3.60753611e-02 -1.94956083e-02 -2.18305048e-02\\n  7.29682297e-02 -4.82063815e-02  4.16010693e-02 -1.16071366e-02\\n -2.78777778e-02 -1.85699966e-02  7.53777698e-02 -3.70974801e-02\\n  2.65876036e-02 -7.61488304e-02  2.97039319e-02  6.93277568e-02\\n  1.10317124e-02 -2.98774857e-02 -2.81533636e-02  1.63163394e-02\\n  2.37173028e-02  1.48372296e-02  5.01094200e-03 -6.44429660e-33\\n -3.70535464e-03 -4.88068722e-02  7.73906801e-03  4.47506830e-02\\n -4.29376140e-02 -3.34479734e-02  2.43960731e-02 -6.01021806e-03\\n  4.41836286e-03  1.56612005e-02 -1.20474445e-02  2.22615171e-02\\n  6.14678860e-03  2.15015784e-02 -1.32397562e-02 -3.13117504e-02\\n  4.04706225e-02 -2.86929961e-02 -4.23406437e-03  2.08631065e-02\\n  2.08971146e-02  4.90366295e-02  9.28290486e-02 -2.88770739e-02\\n -3.25335679e-03 -3.08996104e-02 -3.30239050e-02 -7.48672616e-03\\n  8.85562412e-03  4.70749252e-02  1.27777632e-03  5.63247614e-02\\n  3.84004377e-02 -5.33586480e-02  1.51941879e-02  5.91062754e-02\\n -3.65311578e-02 -2.22352855e-02  1.37126092e-02  8.48472957e-03\\n -7.45213730e-03 -3.49752754e-02  4.34616022e-02 -2.03999151e-02\\n -4.85409573e-02 -1.60454120e-02  1.75158605e-02 -2.52665859e-02\\n -1.05683031e-02 -7.58224428e-02 -2.66153235e-02  1.24236774e-02\\n  1.28116608e-02  3.50037217e-03  2.78141741e-02  4.26782742e-02\\n -2.55420047e-04  8.81500989e-02 -5.13266623e-02 -2.33208900e-03\\n  3.88203301e-02  3.45839038e-02  3.03565189e-02  5.78712719e-03\\n -2.19272263e-02  4.81556952e-02  9.32520255e-03 -2.12702109e-03\\n -4.17258069e-02 -1.41313178e-02  1.41037861e-02  5.22753447e-02\\n  3.21339443e-02  2.16659177e-02  2.69945208e-02 -3.94922458e-02\\n -4.42675464e-02  4.95421030e-02  1.39444713e-02  2.92403176e-02\\n  4.60005216e-02  1.60748418e-02  6.04707235e-03 -8.84444825e-03\\n  4.22700716e-04 -3.32590677e-02 -1.84129030e-02 -3.21573354e-02\\n -1.85256712e-02 -1.49105478e-03  5.86141017e-04  3.19445017e-03\\n  2.69284490e-02 -3.01770736e-02  2.02164222e-02  1.78758614e-02\\n -3.93228419e-02  3.29919383e-02  4.21261555e-03  1.75841171e-02\\n -4.05322798e-02 -2.73239966e-02  9.60061885e-03  1.98185295e-02\\n  5.35256229e-03 -3.98363266e-03  5.50421104e-02  7.01828897e-02\\n -8.01791549e-02  1.44994063e-02 -1.69203784e-02  4.45938036e-02\\n -7.70390360e-03  3.17695402e-02 -1.90139748e-02 -1.88399535e-02\\n  1.29116653e-02  2.57065929e-02  1.20730500e-03  1.06795952e-02\\n -2.24408507e-02  3.37684527e-02 -2.84778234e-02  9.13226139e-03\\n -2.76055187e-02  1.43691581e-02 -5.38528804e-03  6.65950403e-02\\n  5.13203070e-03 -1.07023962e-01 -1.41748749e-02  2.41063908e-02\\n  2.81192456e-07  4.76095006e-02  7.93887377e-02  3.67969126e-02\\n  1.30744511e-02  3.45318168e-02  4.69718166e-02 -5.58486069e-03\\n  6.68469593e-02  3.00296526e-02  2.42315847e-02  6.26831595e-03\\n -5.91721619e-04  1.05016641e-02  3.15765887e-02 -8.23656768e-02\\n -2.60426805e-05 -5.79035245e-02  2.21319664e-02 -3.79329249e-02\\n  4.09391560e-02  6.66573197e-02  6.71702996e-02 -3.00183636e-03\\n -2.62550116e-02  2.31805667e-02 -4.84928191e-02  8.54169056e-02\\n -4.40677702e-02  3.07341758e-02  2.35375427e-02  4.69883680e-02\\n  3.62537913e-02  2.01745704e-02 -8.99103656e-03 -1.40831079e-02\\n -3.35280667e-03 -2.08601523e-02  4.81403023e-02 -1.00927893e-02\\n -5.79212233e-02  2.97506917e-02 -1.80213097e-02  1.94088686e-02\\n -3.49662192e-02  5.91794662e-02 -5.75147197e-02 -2.85859127e-02\\n  2.30510049e-02 -4.13468294e-02 -4.18000668e-02  2.36270912e-02\\n -4.43704091e-02 -2.16516107e-02  2.08823197e-02  8.88756756e-03\\n -9.71202459e-03 -2.13685669e-02 -6.03658371e-02  5.87350596e-03\\n -1.05393538e-02 -4.91825007e-02 -5.43006957e-02 -3.91781069e-02\\n -1.71389170e-02  3.14654447e-02 -4.91534881e-02 -7.53348367e-03\\n  2.57049758e-34  2.72603743e-02 -6.53284639e-02 -1.71235185e-02\\n  4.37767282e-02 -1.67766772e-02  1.98622346e-02  2.83914823e-02\\n -2.96983728e-03 -3.66488006e-03 -3.77251953e-02  3.98588338e-04]'},\n",
       " {'page_number': 3,\n",
       "  'sentence_chunk': 'However, note that not all transformers are LLMs since transformers can also be used for computer vision. Also, not all LLMs are transformers, as there are LLMs based on recurrent and convolutional architectures. The main motivation behind these alternative approaches is to improve the computational efficiency of LLMs. Whether these alternative LLM architectures can compete with the capabilities of transformer-based LLMs and whether they are going to be adopted in practice remains to be seen. For simplicity, I use the term “LLM” to refer to transformer-based LLMs similar to GPT. Utilizing large datasets The large training datasets for popular GPT- and BERT-like models represent diverse and comprehensive text corpora encompassing billions of words, which include a vast array of topics and natural and computer languages. To provide a concrete example, table 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base model for the first version of ChatGPT. Dataset Name Dataset Description Number of tokens Proportion in training data Common Crawl    (filtered) Web crawl data 410 billion 60% WebText2 Web crawl data 19 billion 22%',\n",
       "  'sentence_chunk_size': 1162,\n",
       "  'sentence_chunk_word_count': 178,\n",
       "  'sentence_chunk_tokens': 290.5,\n",
       "  'embedding': '[ 2.03693900e-02  5.50633157e-03 -2.52645463e-03  2.85861734e-02\\n -3.44969556e-02  6.25207974e-03 -3.77321541e-02  6.89941570e-02\\n -6.95064384e-03 -6.49932474e-02 -5.87447509e-02 -1.86997578e-02\\n -1.37873413e-02 -4.91614603e-02  4.56253178e-02 -3.35829183e-02\\n  1.00350548e-02  1.33709935e-02 -7.54028037e-02  2.57161632e-03\\n -6.51154201e-04 -1.90534219e-02  1.05572969e-03  3.39455232e-02\\n -6.21960266e-03  1.85011178e-02 -1.21578434e-02  1.41977286e-03\\n  2.52064150e-02 -8.54813610e-04 -5.38655929e-03 -1.31845400e-02\\n  4.31844778e-02  5.10633066e-02  1.79973642e-06 -1.61647294e-02\\n  3.67362960e-03  1.38501171e-02 -1.12843066e-02 -1.18026547e-02\\n -1.14784641e-02 -1.13733886e-02 -1.00991903e-02  1.95869468e-02\\n -1.50699541e-02  3.06774955e-02  7.61523619e-02  7.84961879e-02\\n  1.71916131e-02  8.80945176e-02  1.51500711e-03 -6.65703043e-02\\n  8.22148845e-02 -2.53396865e-04  4.71525267e-03  1.62837580e-02\\n -1.86175143e-03 -5.71540222e-02  1.96190942e-02  1.80524271e-02\\n -3.49789709e-02  1.86340082e-02 -3.30022280e-03 -8.98314361e-03\\n -3.93353850e-02  2.46435069e-02 -7.66066462e-02 -1.91674754e-02\\n -4.01122719e-02  6.55911537e-03 -6.07102923e-03 -1.51947197e-02\\n -1.04310112e-02 -1.65598672e-02 -1.80414114e-02  4.47434559e-02\\n -3.76847833e-02  5.88126853e-02 -1.20825730e-02  7.48707876e-02\\n -1.93963666e-02 -5.59653826e-02  4.37757261e-02 -1.35593601e-02\\n -3.39910723e-02  6.05522282e-02  2.28028633e-02 -3.82923931e-02\\n  1.01322494e-02  4.56551649e-02  4.53035254e-03 -6.92743622e-03\\n  5.22873644e-03  5.11328764e-02  1.73751488e-02  1.34305237e-02\\n -1.81052852e-02  1.22732138e-02  2.62716822e-02  7.83652253e-03\\n  1.83201768e-02  3.50639783e-02 -4.31020632e-02 -4.24933061e-03\\n -3.74422260e-02  5.38504981e-02 -8.48739669e-02  7.41100758e-02\\n -1.76676065e-02 -4.73096967e-02 -2.33321935e-02 -2.20403094e-02\\n -1.97721068e-02  6.96496740e-02  1.16524147e-02  5.58072934e-03\\n -5.42764030e-02  7.40413442e-02  5.20683676e-02 -4.63426933e-02\\n  3.17363851e-02 -1.35290427e-02 -2.90620606e-02  2.81515308e-02\\n -2.25686040e-02  1.64379235e-02  6.62711263e-02 -1.15097724e-02\\n -1.15084881e-02 -6.62114024e-02  1.00848787e-02  3.82151082e-02\\n -7.33660534e-03 -2.96823382e-02  2.14415733e-02  8.85158330e-02\\n -1.67729426e-02 -6.08557574e-02 -7.01986924e-02  6.41823485e-02\\n  5.21969330e-03 -2.48273853e-02  5.45314886e-02 -1.82737522e-02\\n -2.25652270e-02  2.02482119e-02  1.74064573e-03  2.93351598e-02\\n  2.32654978e-02  1.60182677e-02 -2.64537223e-02  4.72103208e-02\\n  2.17832439e-02  6.33275975e-03  1.41569721e-02 -8.55677575e-03\\n -1.65906027e-02  4.74830866e-02  2.47476604e-02  1.02632781e-02\\n  1.12630818e-02 -2.59451121e-02  2.52208076e-02  7.56968781e-02\\n -3.42070982e-02  3.05303149e-02  3.23490500e-02 -4.00281101e-02\\n -1.80851482e-02  3.75415571e-02 -1.30051952e-02  9.13056359e-02\\n -5.48547134e-03 -1.21463267e-02  8.43118727e-02  8.06121528e-02\\n  6.84723929e-02  6.62838370e-02  1.42625254e-02  1.57227945e-02\\n  7.93208405e-02  4.42760810e-02 -1.63327884e-02  1.34789003e-02\\n -3.47406492e-02 -1.91525032e-03 -4.01439890e-02  2.42653452e-02\\n -2.07778979e-02 -1.22022787e-02 -3.07436157e-02 -1.50409564e-02\\n -2.13932917e-02 -2.31983848e-02  5.01844287e-02 -8.75303894e-03\\n -4.74217497e-02  1.40016526e-01 -1.02927322e-02 -4.27786931e-02\\n -2.83809211e-02 -8.11503679e-02 -4.95874882e-02 -7.86905549e-03\\n  4.07771766e-02 -5.07244766e-02 -6.84989914e-02 -2.71878969e-02\\n -3.42665836e-02  8.68819281e-02 -3.79793197e-02  5.72886243e-02\\n -1.22024752e-02 -1.75859928e-02  7.10172392e-03  2.34773457e-02\\n -1.30991461e-02 -4.37581679e-03  2.45797914e-02 -3.69407027e-03\\n -2.23454703e-02 -4.62341793e-02  3.66019830e-02  4.01948877e-02\\n -2.02364195e-02  2.36456543e-02 -2.84934952e-03 -2.18169065e-03\\n  2.68390849e-02  2.40729824e-02 -1.40298661e-02 -1.36721758e-02\\n  4.33316343e-02 -1.87343322e-02 -4.19386514e-02  8.51339661e-03\\n -1.46110030e-02  8.51924252e-03  4.22256477e-02 -4.59590666e-02\\n  2.36380547e-02  3.71784382e-02 -1.52795780e-02 -2.38576531e-02\\n -7.71208555e-02 -7.63357729e-02  8.70228112e-02 -4.89753038e-02\\n  2.89453007e-02 -3.31473374e-03  1.58828367e-02 -3.42594497e-02\\n  9.18235257e-03 -2.38373857e-02  1.99199677e-03  1.64392926e-02\\n  2.28546970e-02  2.07867138e-02 -3.75630111e-02 -5.51362969e-02\\n  6.29784688e-02  8.39263725e-04  1.75055880e-02  2.66863406e-02\\n  5.87179279e-03 -3.59675102e-02 -6.25534430e-02 -5.98380901e-02\\n -2.23815236e-02 -1.50442282e-02  4.11756150e-02  1.53985545e-02\\n -1.84164383e-02 -1.12779699e-02  3.90441827e-02 -3.88192534e-02\\n  7.93030951e-03  4.07972839e-03  9.89796687e-03  7.22381324e-02\\n -3.34112532e-02 -2.65021157e-02 -1.27810920e-02 -1.95166636e-02\\n  8.31926998e-04  3.10316086e-02  8.76499573e-04 -2.36116573e-02\\n -6.55404404e-02  2.01573111e-02  2.44110990e-02  2.53892038e-02\\n -2.20666621e-02 -6.28346205e-02  1.32537428e-02  3.32055986e-03\\n  5.32535911e-02  1.08750910e-03  3.94823477e-02  3.57226236e-03\\n -3.05399895e-02 -2.99924822e-03 -1.98024400e-02  2.91590579e-04\\n -4.18026149e-02 -8.44334438e-02 -7.34171569e-02 -9.14067309e-03\\n  2.31556334e-02  3.93196270e-02 -3.31807211e-02 -2.91314209e-03\\n  1.90784980e-03 -5.67530766e-02  3.49016786e-02 -1.20485108e-02\\n  3.86296436e-02  1.49250031e-03 -5.79341501e-02 -1.76734608e-02\\n  2.38726567e-02 -3.30236293e-02  1.65463891e-02 -5.44594526e-02\\n -2.22628303e-02 -9.44731350e-04 -1.24010090e-02 -5.96504770e-02\\n  7.09766196e-03  1.95483640e-02  8.80820863e-03  1.00007299e-02\\n -2.27441899e-02 -3.22346389e-02 -9.98401898e-04  1.13724461e-02\\n -1.36017296e-02  1.03366315e-01  8.84743687e-03  2.93562505e-02\\n -3.53839770e-02 -5.20982780e-02 -3.40318382e-02  2.79640709e-03\\n  1.41220530e-02  9.62294452e-03  4.67722304e-02 -5.16277999e-02\\n -8.64759907e-02  2.67074276e-02  6.27758168e-03  2.78096218e-02\\n -9.30551253e-03 -1.24598667e-02 -2.01109499e-02  6.45897612e-02\\n -4.58149649e-02  3.16851512e-02  1.94375776e-02  1.83471777e-02\\n -1.69610716e-02  4.05108966e-02  9.24013276e-03 -7.26859719e-02\\n -3.59159149e-02 -2.51617306e-03  6.74108714e-02 -5.60389124e-02\\n -4.08957489e-02  7.77723175e-03  4.55453433e-03 -3.85717228e-02\\n  2.15435661e-02  7.70176798e-02 -1.79252997e-02 -5.47361188e-02\\n -1.68334525e-02  1.42948711e-02 -5.86682186e-03  4.75917058e-03\\n -3.86789296e-04 -6.12674765e-02  2.17833323e-03  1.97530445e-02\\n  1.49437254e-02 -1.70984790e-02 -2.65328940e-02  5.58500271e-03\\n -4.71638441e-02  4.24156003e-02 -2.05870420e-02  9.82805714e-03\\n -4.88348491e-03  5.53851901e-03 -7.87306018e-03  1.64753534e-02\\n  1.10440468e-02  2.62562791e-03 -3.71247046e-02 -4.46414463e-02\\n  3.97177972e-02 -7.13365674e-02  4.64284822e-05 -2.23473795e-02\\n -3.18525136e-02 -2.97036003e-02  4.92802933e-02  1.63049120e-02\\n -4.00019921e-02 -7.64876530e-02  3.49394865e-02  3.11775273e-03\\n  3.39124240e-02 -2.45613307e-02  1.71766952e-02 -2.70078816e-02\\n -1.33950007e-03 -1.83139760e-02  4.11330489e-03 -4.69446508e-03\\n -6.47070073e-03 -4.58700210e-02  7.31116757e-02 -6.30061999e-02\\n -1.71622299e-02 -1.56084048e-02 -6.36545569e-02 -4.13173400e-02\\n  1.61663312e-02 -1.71155911e-02 -8.98257364e-03 -3.58180739e-02\\n -1.14152720e-02  2.47451738e-02  2.44347248e-02 -5.21704406e-02\\n  8.20943806e-03 -3.46624330e-02 -4.72255200e-02 -3.67203876e-02\\n  1.17190164e-02 -2.16400623e-02 -1.85989961e-02  3.31754461e-02\\n -7.89317396e-03  3.40595208e-02  1.57205462e-02 -3.01065147e-02\\n  6.94142887e-03 -1.37946736e-02 -1.60047412e-02 -8.72807652e-02\\n  3.77389719e-04  8.03106800e-02 -2.09588390e-02  1.72440596e-02\\n -7.04130754e-02 -8.76489356e-02 -3.91433947e-02  2.20164787e-02\\n  1.22842100e-02  1.27727510e-02 -2.53227539e-02  3.15742828e-02\\n -2.86694355e-02  5.83088882e-02  7.78649077e-02  5.88892680e-03\\n  2.14503836e-02  4.92688604e-02  5.22985756e-02 -8.60929117e-03\\n  1.24240154e-03 -2.21633259e-02 -1.01224808e-02  3.74234468e-03\\n  3.11124939e-02  3.09342612e-03 -8.94286204e-03  2.92887539e-02\\n  5.26778810e-02  8.40192586e-02 -3.35883610e-02  5.45370094e-02\\n -8.93255416e-03  2.85455678e-02 -1.58540288e-03  4.56708297e-02\\n -3.42019200e-02  1.31032830e-02  1.93746965e-02 -2.55544446e-02\\n -2.01004688e-02  4.42586653e-02 -1.39162866e-02  3.69049124e-02\\n -2.35363171e-02  8.76528546e-02 -2.46648248e-02 -5.51334992e-02\\n  1.27128670e-02  9.43730306e-03  4.67837835e-03 -3.62942331e-02\\n  2.71096341e-02  9.41426773e-03  2.27805302e-02  4.37722132e-02\\n  2.11542267e-02  2.17445567e-02  4.35599918e-03  4.30027694e-02\\n -3.80085893e-02 -3.48074175e-02 -1.12817604e-02  6.57439306e-02\\n  7.72529840e-02  3.62208448e-02 -4.46645450e-03 -5.28027639e-02\\n -2.29046308e-02  9.53250006e-03  3.73020628e-03 -3.67147592e-03\\n  4.88550738e-02  3.30133475e-02 -1.71753317e-02 -1.29960738e-02\\n -1.22505585e-02  3.02924272e-02  1.69251710e-02  2.79976446e-02\\n -8.11060518e-03 -8.72674119e-03 -2.09069084e-02 -5.86891696e-02\\n  9.97003727e-03 -5.76011129e-02  5.22069260e-02 -3.18414606e-02\\n -4.09792662e-02 -9.47576540e-04  4.05229963e-02 -4.26883921e-02\\n -3.35499346e-02 -1.00982308e-01  2.30559725e-02  5.32944389e-02\\n  3.36836204e-02 -6.22612098e-03  5.09064132e-03  1.14042740e-02\\n  2.07774229e-02  1.95989548e-03  7.32936524e-03 -5.80975887e-33\\n -4.51016793e-04 -4.83132899e-02  4.35813442e-02  3.05712614e-02\\n -6.10204749e-02 -1.33150797e-02  2.21508835e-02 -3.30606736e-02\\n -2.29725381e-03 -3.01116779e-02 -5.71658164e-02 -1.38963191e-02\\n  1.04562324e-02  3.12910043e-02 -3.28554176e-02 -5.05554341e-02\\n  2.08257679e-02 -2.77814567e-02 -2.11694576e-02 -1.57674141e-02\\n -1.28350547e-02  4.33266684e-02  8.17774162e-02 -9.16548166e-03\\n -1.14143873e-02 -1.38494577e-02  2.10968824e-03  1.49661452e-02\\n -9.62932501e-03  5.01436144e-02 -1.75995138e-02  5.08090965e-02\\n  2.49273777e-02 -6.57433793e-02 -6.04352588e-03  3.62420380e-02\\n -2.80572940e-02  1.71424840e-02  3.49908881e-03  2.84363031e-02\\n  2.59165117e-03 -2.13120617e-02  5.00729568e-02  5.81538910e-03\\n -5.73148504e-02  2.28480007e-02 -6.85922522e-03 -2.25266200e-02\\n -5.60985692e-03 -5.31672612e-02  3.20437597e-03  7.87844416e-03\\n  3.25877443e-02  3.23422626e-02  2.84491088e-02  9.33239516e-03\\n -1.20625179e-02  9.84931439e-02 -3.92146148e-02  7.74875842e-03\\n  5.41529432e-02  3.38787809e-02  4.47802953e-02  1.79259907e-02\\n  2.53692316e-03  3.63190658e-02  5.19001251e-03 -3.32640074e-02\\n -3.01345736e-02 -4.55077365e-02  5.75644290e-03  6.84955940e-02\\n  3.22384909e-02 -1.91484112e-02 -2.13742703e-02  1.27731739e-02\\n -1.60840657e-02  2.02954672e-02 -2.26330440e-02  4.70910072e-02\\n  2.32273731e-02  4.32172418e-02  2.38927212e-02 -1.42485518e-02\\n -3.58327036e-03  5.07573560e-02  1.72923431e-02  4.49405191e-03\\n -1.00251576e-02 -4.68261028e-03  1.06584039e-02  1.67477364e-03\\n  1.13994293e-02 -2.13802774e-02  9.33377445e-03  8.97340197e-03\\n  3.23360087e-03  2.10047942e-02 -1.84110459e-02 -4.31578606e-03\\n -4.78613414e-02 -4.23107259e-02 -4.71159350e-03  8.00188910e-03\\n -1.47252344e-02 -1.62778087e-02 -6.06231531e-03  5.90009801e-02\\n -7.22139254e-02  2.46207770e-02 -2.97189951e-02  5.71247004e-03\\n -2.08287910e-02 -1.61224604e-03  1.08889467e-03 -3.84248719e-02\\n  1.03742220e-02  3.19922492e-02 -1.42880650e-02 -3.62627651e-03\\n -2.30401196e-02  1.22193173e-02  1.15045730e-03  2.04192158e-02\\n -8.59266799e-03  5.31130396e-02  7.51986110e-04  6.27711266e-02\\n  1.69612877e-02 -5.89669868e-02  2.46458827e-03  2.60797758e-02\\n  2.56371663e-07  6.74509853e-02  5.25773130e-02  7.11444169e-02\\n  1.70159731e-02  3.64067405e-02  1.12297321e-02  1.73677523e-02\\n -1.46682636e-04  8.70142039e-03  7.13679055e-03  1.73088145e-02\\n  1.20917577e-02  1.48177454e-02  6.70212088e-04 -9.07583684e-02\\n  1.00422883e-03 -7.28744864e-02 -1.97398849e-02 -2.54057068e-02\\n  1.65736731e-02  2.43423525e-02  7.96745270e-02  8.56502447e-04\\n -4.48315889e-02 -1.93793140e-02 -3.66850272e-02  6.02807403e-02\\n -6.59398884e-02  6.58008223e-03  3.15663181e-02  6.86226860e-02\\n  5.02354093e-02  5.73258009e-03 -2.40868554e-02 -3.66033823e-03\\n  1.89462435e-02 -1.57354940e-02 -3.37066734e-03  6.63856044e-03\\n -7.11436942e-02  8.63947999e-03 -2.99162976e-02 -1.01206852e-02\\n -1.38510216e-03  2.57435963e-02 -4.78996262e-02 -2.50619277e-02\\n  8.67280737e-03 -5.42367110e-03 -6.35024831e-02 -8.26596748e-03\\n -5.42679280e-02  2.60595651e-03  5.84552204e-03 -1.62897464e-02\\n -2.00917907e-02 -3.85481212e-03 -5.36247008e-02  1.05455788e-02\\n -1.03156874e-02 -5.81396930e-02 -7.50569850e-02 -4.88635749e-02\\n -4.02915338e-03  1.26688080e-02 -8.81710835e-03 -4.05197330e-02\\n  2.51669185e-34  9.27590858e-03 -1.50261456e-02 -2.47306586e-03\\n  9.24574733e-02 -3.03211045e-02  2.54324656e-02  3.64453346e-02\\n  3.16415285e-03 -4.28479686e-02 -8.77480656e-02 -1.19646788e-02]'},\n",
       " {'page_number': 4,\n",
       "  'sentence_chunk': 'Dataset Name Dataset Description Number of tokens Proportion in training data Books1 Internet-based book corpus 12 billion 8% Books2 Internet-based book corpus 55 billion 8% Wikipedia High-quality text 3 billion 3% The table above, reports the number of tokens, where a token is a unit of text that a model reads and the number of tokens in a dataset is roughly equivalent to the number of words and punctuation characters in the text. The main takeaway is that the scale and diversity of this training dataset allow these models to perform well on diverse tasks, including language syntax, semantics, and context—even some requiring general knowledge. The pretrained nature of these models makes them incredibly versatile for further fine-tuning on downstream tasks, which is why they are also known as base or foundation models. Pretraining LLMs requires access to significant resources and is very expensive. For example, the GPT-3 pretraining cost is estimated to be 4.6 million USD in terms of cloud computing credits (https://mng.bz/VxEW). The good news is that many pretrained LLMs, available as open source models, can be used as general-purpose tools to write, extract, and edit texts that were not part of the training data. Also, LLMs can be fine-tuned on specific tasks with relatively smaller datasets, reducing the computational resources needed and improving performance. We will implement the code for pretraining and use it to pretrain an LLM for educational purposes. All computations are executable on consumer hardware. After implementing the pretraining code, we will learn how to reuse openly available model weights and load them into the architecture we will implement, allowing us to skip the expensive pretraining stage when we fine-tune our LLM.',\n",
       "  'sentence_chunk_size': 1772,\n",
       "  'sentence_chunk_word_count': 278,\n",
       "  'sentence_chunk_tokens': 443.0,\n",
       "  'embedding': '[ 5.13226986e-02  1.08912063e-03 -2.15304513e-02  4.56842370e-02\\n -5.24877943e-02  3.89237888e-02  1.58566944e-02  2.42728330e-02\\n -2.92511983e-03 -9.67485905e-02 -3.46115269e-02 -1.52139449e-02\\n -2.79493053e-02  1.72198303e-02  3.95716541e-02 -4.15125974e-02\\n  5.12022376e-02 -2.01950911e-02 -2.42794325e-04 -7.34880799e-03\\n  2.31676288e-02  1.42842140e-02 -6.22285483e-03  1.78263914e-02\\n -2.50283163e-02 -3.11691482e-02 -6.65299315e-03  3.06858141e-02\\n -1.01802428e-03 -1.26540130e-02  2.62787472e-02  1.79442782e-02\\n  1.33168790e-02  1.66460332e-02  2.11639781e-06 -7.58491829e-02\\n -4.56875488e-02  2.63029640e-03 -1.06429113e-02  1.78836752e-02\\n  6.54525980e-02  2.00256072e-02 -1.36192460e-02  1.74134374e-02\\n -2.17576120e-02  3.49862464e-02  2.22569462e-02  5.61586767e-02\\n  4.52914387e-02  1.00522019e-01  1.13689881e-02 -5.37392795e-02\\n  3.72619741e-02  4.77940515e-02  6.74182549e-03  3.06060351e-03\\n  2.38688253e-02  8.10626335e-03 -2.83741988e-02  1.33714490e-02\\n -2.62019839e-02  1.65989008e-02  3.11776384e-04 -3.78761790e-03\\n  3.49501669e-02 -2.86621857e-03 -8.33444744e-02 -1.91462915e-02\\n -3.54368947e-02  3.54598165e-02  1.89166665e-02 -5.20024262e-02\\n  2.22484004e-02  3.83537752e-03 -1.40722515e-02 -4.44184383e-03\\n -5.14071807e-02  2.08905134e-02 -3.93014075e-03  5.12577966e-02\\n  3.80427130e-02 -5.15469648e-02  1.49154551e-02  7.86549598e-03\\n -2.78089195e-02  4.55484800e-02  6.59226719e-03 -3.52820009e-02\\n  1.92688685e-02 -1.36263913e-03 -2.90176943e-02 -5.38014658e-02\\n  3.23961265e-02  2.96799783e-02  1.14352331e-02  2.50304006e-02\\n -5.97069971e-02 -6.08044490e-02  3.35691571e-02 -1.34673668e-02\\n  2.16886913e-03  6.88072341e-03 -3.49243321e-02  3.39851379e-02\\n -2.83185691e-02 -1.70789622e-02  8.49303417e-03  5.15634120e-02\\n -2.72575673e-02  7.20626581e-03 -3.96465994e-02 -2.08006194e-03\\n -6.32667467e-02  7.44804367e-02 -1.33015504e-02  3.20947217e-03\\n -7.42097422e-02  1.24642625e-02  3.35734375e-02  1.38623677e-02\\n  2.86876559e-02 -1.95784811e-02 -1.87574103e-02  2.35689641e-03\\n -2.65568048e-02 -2.51142178e-02 -8.37795530e-03  3.35300826e-02\\n -2.30017081e-02 -7.48952925e-02  1.80260520e-02  2.16346513e-02\\n  1.88936926e-02 -2.10436042e-02  3.62188146e-02  7.29177147e-02\\n  6.30653137e-03 -3.57619636e-02 -9.74211171e-02 -2.32372582e-02\\n  1.70144178e-02 -6.72462806e-02 -2.12553162e-02 -7.69009069e-02\\n -1.19577320e-02  3.15965377e-02  1.49024138e-02  8.28617625e-03\\n  7.24314712e-04  8.41545966e-03 -3.54798473e-02  7.71384016e-02\\n -3.10602561e-02  4.10833443e-03  3.11082173e-02 -2.79564057e-02\\n -2.55350042e-02  6.76994547e-02  4.18602340e-02 -4.13497305e-03\\n  7.79899806e-02  7.35337054e-03 -3.44484523e-02  5.07530719e-02\\n -1.03535000e-02  4.23559593e-03 -1.68040451e-02  1.36229750e-02\\n -2.29719165e-03  2.57015135e-02 -5.78348665e-03  6.72092587e-02\\n -2.09971853e-02  3.27675566e-02  7.62747005e-02  1.13609739e-01\\n  4.58104946e-02  3.97430807e-02  3.01469285e-02  1.50970127e-02\\n  5.39407581e-02 -1.07261725e-03 -2.81524193e-02 -2.07112487e-02\\n -2.05464773e-02  2.72082351e-02  1.62200592e-02  2.01031864e-02\\n  8.73450562e-03 -6.60786256e-02 -3.17321867e-02 -2.22607758e-02\\n -5.35738692e-02 -3.50441225e-02  8.26041475e-02 -9.27106757e-03\\n -4.39957604e-02  7.50272349e-02  1.73463635e-02 -7.58166462e-02\\n  3.08889896e-02 -3.67038175e-02  1.69568546e-02  6.50354847e-02\\n  3.57914940e-02 -9.14952084e-02 -1.25799337e-02 -1.83529481e-02\\n  4.44203056e-02  2.91262642e-02  9.79202427e-03  6.20020728e-04\\n -5.67855919e-03 -6.40038252e-02  3.45417554e-03 -7.31060188e-03\\n  8.38991348e-03  8.46834388e-04  1.09602592e-03  2.27183197e-02\\n -1.42222112e-02 -6.29617795e-02 -3.90161127e-02  7.07714930e-02\\n -3.42218205e-02  7.64938677e-03 -3.10053788e-02 -1.69825386e-02\\n  3.38725522e-02  4.66073975e-02 -1.77909937e-02  9.90561768e-03\\n  4.58018333e-02 -3.31733413e-02 -8.29609577e-03 -1.45418774e-02\\n -1.39398566e-02  1.36699798e-02  8.26986581e-02 -3.43581885e-02\\n  9.56234988e-03  1.17951324e-02 -4.03263941e-02 -2.89915111e-02\\n -3.51404957e-02 -1.94194224e-02  8.03700984e-02 -2.91658752e-02\\n  2.58257445e-02  6.76770462e-04 -3.60030606e-02 -3.46232206e-02\\n  3.55588533e-02  2.19345652e-02  2.82680485e-02 -5.27480617e-03\\n -1.86976641e-02  3.09142098e-02 -2.72413809e-02 -2.27956958e-02\\n -3.02897915e-02 -2.62631457e-02  8.57630000e-03 -1.13288853e-02\\n  2.23289132e-02 -8.42364952e-02 -1.51216825e-02 -1.01074666e-01\\n  6.86973752e-03 -2.88226791e-02  2.92414315e-02  1.43042132e-02\\n -2.19754633e-02 -3.14009376e-02 -3.63043090e-03 -2.48987302e-02\\n  8.03100970e-03 -2.23053880e-02  5.11877146e-03  5.42893559e-02\\n -9.40803438e-03 -1.72708221e-02 -4.07706723e-02 -4.43197368e-03\\n -7.26112211e-03  3.00554875e-02  3.15049998e-02 -1.32560153e-02\\n -4.50462140e-02  4.76544201e-02  2.22688471e-03  2.36715749e-02\\n  4.31557978e-03 -4.53118235e-02  8.12182017e-03 -1.56630520e-02\\n  1.75558534e-02  4.31885831e-02  1.39116459e-02 -1.51066007e-02\\n  5.34352334e-03 -2.00464297e-02 -6.17301427e-02  3.71392746e-03\\n  6.88173342e-04 -2.86216661e-03 -6.47401437e-02 -1.83699047e-03\\n  1.23776598e-02  3.38812992e-02  9.71482310e-04 -3.42685021e-02\\n -1.79821649e-03  6.18482055e-03 -9.65664722e-03 -4.15741317e-02\\n -2.09401175e-02 -9.32626054e-03 -3.24028321e-02  4.94652577e-02\\n  3.18982080e-03 -3.16827856e-02  4.41624746e-02 -5.11596613e-02\\n  2.19222773e-02 -3.95638496e-02 -2.03184690e-02 -7.20617399e-02\\n  1.39547391e-02  8.53181002e-04 -4.27731429e-04  8.18809215e-03\\n -3.77503596e-02 -3.22582163e-02  2.37553753e-02  4.11769412e-02\\n -3.32485139e-02  6.73974380e-02 -3.91607657e-02  1.73735525e-02\\n -1.67556070e-02 -1.44062508e-02 -2.40233280e-02  7.97497982e-04\\n -2.25280020e-02 -1.84170660e-02  1.76800415e-02 -3.16940527e-03\\n -2.82640811e-02 -1.20208682e-02  7.65822176e-03  5.50846979e-02\\n -1.24585424e-02 -5.96155645e-03 -1.96530204e-02  1.42880864e-02\\n  2.94576362e-02  1.05876382e-02  5.05843246e-03 -4.49324213e-02\\n -4.11445508e-03  4.61175255e-02  1.92806125e-02 -4.79148775e-02\\n -3.54969688e-02  1.74781177e-02  4.83808406e-02 -1.17282942e-01\\n -2.04982888e-02  1.41223548e-02  6.32155165e-02 -5.00950515e-02\\n  4.98281268e-04  1.29692435e-01  1.53355617e-02 -4.54568081e-02\\n -7.30733061e-03 -3.09147500e-02 -2.48959735e-02  1.96161084e-02\\n  4.47429338e-04 -1.17565736e-01 -1.87702943e-02  1.00445356e-02\\n  2.23373026e-02 -1.24729602e-02 -4.15437631e-02  9.93456226e-04\\n -8.48849043e-02  9.11250245e-03  1.92217343e-02 -2.01305356e-02\\n -2.81752727e-04 -3.10123693e-02 -3.36437859e-02 -3.77539583e-02\\n  2.02326737e-02  5.78243565e-03 -4.67247888e-02  1.70126688e-02\\n  1.85866002e-02 -3.27115096e-02 -7.36600626e-03 -1.55968238e-02\\n -6.14633076e-02  1.96420252e-02  2.33279504e-02  2.92789452e-02\\n -4.49679382e-02  4.43226937e-03  6.36531487e-02 -5.99058205e-03\\n  4.88498015e-03 -4.42961156e-02  6.18083924e-02 -3.52868065e-02\\n  2.11719684e-02 -7.62846507e-03  4.54509594e-02 -3.08587961e-02\\n -2.69791130e-02  4.40525897e-02  5.75889088e-02  9.29700769e-03\\n  4.54980507e-02 -4.54059467e-02 -2.39989851e-02  7.50020961e-04\\n  6.58009574e-02  2.11795736e-02 -4.41963151e-02 -3.37439142e-02\\n -2.90156715e-02  3.84801207e-03 -6.44525886e-02  1.41350674e-02\\n -1.68113988e-02 -5.88888377e-02  9.24545340e-03 -7.88819268e-02\\n  5.67895477e-04  2.74323840e-02 -2.62049073e-03  3.23072709e-02\\n  3.14225182e-02  2.63242442e-02  7.58952554e-03 -4.74944599e-02\\n  2.27424502e-02 -3.47438715e-02 -1.00563951e-02 -1.26870871e-01\\n -1.60137247e-02  3.35097834e-02 -1.44373281e-02 -2.04957332e-02\\n -2.68022567e-02 -5.03032245e-02 -7.10721090e-02  2.75620073e-02\\n  5.57447746e-02  2.36002393e-02  2.67383549e-02  2.26090904e-02\\n -4.89708893e-02  4.64066416e-02  5.82351536e-02 -7.50449002e-02\\n  1.11104622e-02  2.85912286e-02  1.57337971e-02 -6.37494493e-03\\n  2.60331649e-02 -5.48410900e-02  8.14798288e-03  3.43602560e-02\\n  3.22095910e-03 -2.88075153e-02 -1.48936585e-02  7.37033691e-03\\n  4.47541513e-02  6.97377473e-02 -1.51212811e-02 -8.40734877e-03\\n -4.66017015e-02  5.71577623e-02 -2.76774727e-02 -3.69514991e-03\\n -2.07830146e-02  6.93528773e-03  1.39068924e-02 -2.07688212e-02\\n -3.98447476e-02 -4.13740007e-03 -1.58011951e-02  5.59779778e-02\\n -3.56061496e-02  6.44592494e-02 -1.52727580e-02  1.17285121e-02\\n  3.50996666e-02 -2.31338013e-02 -1.50860734e-02 -2.20836010e-02\\n -1.65012665e-02  3.15234885e-02 -2.41103326e-03  2.38571595e-03\\n  8.68610293e-03  3.08822617e-02 -3.39790503e-03  2.83393841e-02\\n -3.95349376e-02  1.24231279e-02 -2.19402667e-02  1.99285336e-02\\n  5.99596538e-02 -1.94016844e-02  3.88492532e-02 -1.83681585e-02\\n -2.37430856e-02 -2.64966208e-02  6.28396496e-02 -1.68946118e-03\\n  3.93603221e-02  5.45149948e-03  1.03923697e-02 -1.50730694e-02\\n  1.81448255e-02  7.57083520e-02  1.96892899e-02  3.48110795e-02\\n  2.53173877e-02 -3.48156393e-02 -3.25716734e-02  2.25789356e-03\\n  3.27760074e-03  2.11134255e-02  1.13443425e-02 -1.67491604e-02\\n -4.71801050e-02 -4.64398190e-02  4.16329168e-02  1.71359796e-02\\n -9.54758190e-03 -2.19657831e-02  4.97321896e-02  6.21516071e-02\\n  1.13482624e-02 -1.29391011e-02 -3.13549563e-02  3.22375000e-02\\n  2.49637142e-02 -4.90997434e-02  4.01634816e-03 -6.13314986e-33\\n -9.37847234e-03 -2.89415661e-02  2.36527734e-02  3.63676064e-02\\n -2.58480273e-02  1.84783398e-03  1.85551438e-02 -2.92965155e-02\\n  8.19095131e-03 -8.19714274e-03 -3.22281234e-02 -2.87104631e-04\\n  2.95866113e-02  2.21882816e-02  6.62082806e-03  8.86767451e-03\\n  3.79732214e-02 -3.58906239e-02 -3.19718290e-03 -1.33057982e-02\\n  3.34391333e-02  7.18578547e-02  7.56055638e-02 -2.28211377e-02\\n  1.22583164e-02 -2.45781224e-02  1.15169128e-02 -2.17277659e-04\\n -1.96515154e-02  5.22415675e-02 -2.90394132e-03  5.24969250e-02\\n  2.34039947e-02 -2.34949738e-02 -1.17844623e-02  1.14204893e-02\\n -7.38580674e-02 -4.80491333e-02  3.96791361e-02  3.10880411e-02\\n  5.70168532e-03 -4.36234884e-02  6.73735812e-02  1.83321107e-02\\n -3.81236002e-02  2.83753034e-03  6.94851875e-02 -2.89540384e-02\\n -4.46242606e-03 -3.97048667e-02 -5.73271252e-02  3.65263149e-02\\n -4.91832523e-03  5.95289879e-02  5.30096143e-02  9.96278320e-03\\n  2.68334313e-03  3.54220904e-02 -5.59866391e-02  2.13384200e-02\\n  4.76465523e-02  6.79527819e-02  4.25357111e-02  1.60201092e-03\\n  3.74863180e-03  5.44152558e-02  1.58730615e-02 -2.66469531e-02\\n  4.76255547e-03  2.29752753e-02  3.86373550e-02  4.26265150e-02\\n  1.59811676e-02  4.72137034e-02  2.80903801e-02  1.29462257e-02\\n -8.34451690e-02  6.48375321e-03  6.10710646e-04  2.79964339e-02\\n -1.26704350e-02  1.67210083e-02 -4.50758031e-03 -6.01244392e-03\\n -3.42417732e-02 -8.22801515e-03  2.99759954e-03 -1.97998006e-02\\n  6.14202814e-03 -1.56916361e-02 -1.32153844e-02 -1.94366295e-02\\n -5.70173608e-03 -2.94796117e-02 -4.93577905e-02 -5.81434462e-03\\n  3.39910351e-02  2.71184724e-02 -2.06041839e-02  1.88524816e-02\\n -7.75037110e-02 -4.99141961e-02 -3.14122997e-03 -2.83523761e-02\\n  4.91486341e-02 -2.43898388e-02  4.20430824e-02  3.36413831e-02\\n -7.49697536e-02  1.80988032e-02 -5.97539824e-03 -2.30128039e-03\\n  1.93774141e-02  5.52829802e-02  1.23880152e-02  7.65117351e-03\\n  1.78164672e-02  4.42092158e-02  7.12906150e-03 -4.65304963e-02\\n -5.15051447e-02 -3.99704166e-02  3.53292271e-04  2.46019736e-02\\n -5.17650619e-02 -7.09362933e-03 -2.59662326e-02  4.01885957e-02\\n  4.32478338e-02 -5.44416308e-02 -9.44240298e-03  3.21689174e-02\\n  2.77332816e-07  6.81458041e-02  5.75099103e-02  5.62400781e-02\\n -7.69252982e-03  6.95802551e-03  2.52218693e-02 -2.24667042e-02\\n  2.72171237e-02  1.03383837e-02 -4.40886021e-02  9.87669732e-03\\n  1.37040075e-02  3.40248831e-02  1.45328613e-02 -1.28099248e-01\\n -9.56625585e-03 -5.02380505e-02  1.56133873e-02 -5.85818104e-02\\n  3.12264673e-02  3.58090624e-02  7.08330050e-02  3.63231152e-02\\n -2.65595131e-02 -3.72707807e-02 -3.79211344e-02  2.98696533e-02\\n -6.46482259e-02 -1.86501164e-02  4.11570854e-02  4.50519621e-02\\n  3.70036513e-02  3.81073123e-03  2.32844856e-02 -4.11726423e-02\\n  1.53587535e-02 -1.48140118e-02  5.16037159e-02 -9.39355418e-03\\n -2.31964840e-03  2.34501753e-02 -1.84130874e-02 -1.71029158e-02\\n -3.12250257e-02  7.72908702e-02  3.00062890e-03  1.08528947e-02\\n -1.14977136e-02 -2.43046526e-02 -5.13902605e-02 -1.01681612e-02\\n -3.37843113e-02  3.53849446e-03  4.04541977e-02  2.47388501e-02\\n -1.80162415e-02 -1.52091412e-02 -7.70753995e-02 -7.59204710e-03\\n  9.04997718e-03 -3.05255000e-02 -6.52561039e-02 -2.43119132e-02\\n  3.59546207e-02  1.42637054e-02 -7.32469782e-02  3.22557078e-03\\n  2.99135785e-34  4.32891250e-02 -5.20659015e-02  2.39998121e-02\\n  8.31308514e-02  1.06669106e-02 -2.30748346e-03 -8.80930293e-03\\n  1.52076781e-02 -3.22638005e-02 -6.92712218e-02  4.59320331e-03]'},\n",
       " {'page_number': 4,\n",
       "  'sentence_chunk': 'GPT-3 Dataset Details The Table above displays the dataset used for GPT-3. The proportions column in the table sums up to 100% of the sampled data, adjusted for rounding errors. Although the subsets in the Number of Tokens column total 499 billion, the model was trained on only 300 billion tokens. The authors of the GPT-3 paper did not specify why the model was not trained on all 499 billion tokens. For context, consider the size of the CommonCrawl dataset, which alone consists of 410 billion tokens and requires about 570 GB of storage. In comparison, later iterations of models like GPT-3, such as Meta’s LLaMA, have expanded their training scope to include additional data sources like Arxiv research papers (92 GB) and StackExchange’s code-related Q&As (78 GB). The authors of the GPT-3 paper did not share the training dataset, but a comparable dataset that is publicly available is Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research by Soldaini et al. 2024 (https://arxiv.org/abs/2402.00159). However, the collection may contain copyrighted works, and the exact usage terms may depend on the intended use case and country. A closer look at the GPT architecture GPT was originally introduced in the paper “Improving Language Understanding by Generative Pre- Training” (https://mng.bz/x2qg) by Radford et al.',\n",
       "  'sentence_chunk_size': 1342,\n",
       "  'sentence_chunk_word_count': 214,\n",
       "  'sentence_chunk_tokens': 335.5,\n",
       "  'embedding': '[ 4.13343310e-02  4.84744422e-02 -3.49333393e-03  3.17897014e-02\\n -3.75360884e-02  3.87368016e-02  7.54414359e-03  2.41784342e-02\\n -5.08466586e-02  5.35272574e-03 -4.29287702e-02 -2.36979425e-02\\n -1.40014794e-02  3.12621780e-02  1.25170061e-02 -3.34611945e-02\\n  1.81149095e-02  1.42324334e-02 -1.52436178e-02 -2.53590997e-02\\n  1.29688568e-02  3.12580131e-02  3.67641710e-02  2.76437644e-02\\n -1.96283329e-02  1.46122440e-03  1.00310979e-04 -3.64239961e-02\\n -2.07244270e-02 -3.82467099e-02 -3.19169648e-02  1.25097623e-03\\n  5.35612628e-02  5.14293574e-02  2.19540880e-06 -4.69170213e-02\\n -5.04784696e-02  4.23384085e-02  2.20555812e-03  5.26136085e-02\\n  1.68083012e-02  2.63286177e-02 -3.64024751e-03 -8.21493473e-03\\n -1.37974452e-02  1.55200716e-02  4.30293940e-02  7.66480938e-02\\n -1.43621361e-03  4.16154265e-02 -4.18820465e-03 -1.25568518e-02\\n  6.20371141e-02 -7.63629423e-03  4.93712351e-02 -4.19015400e-02\\n -3.90259773e-02  2.54855026e-02  2.99146194e-02  1.56952848e-03\\n  1.98913342e-03  1.05450591e-02  4.79598083e-02  5.63382264e-03\\n -2.79291570e-02  4.25431952e-02 -1.20242625e-01 -2.71747224e-02\\n -1.41001120e-02 -1.00313798e-02  3.39464061e-02 -2.53610723e-02\\n -8.54784809e-03 -3.28155309e-02  9.47930478e-03 -2.35732924e-02\\n -3.73454690e-02  1.58593024e-03 -3.48740146e-02  8.60292614e-02\\n  2.15018075e-03 -2.57869791e-02  3.74729224e-02 -5.99587560e-02\\n -4.08313796e-02  5.82596511e-02 -1.71812472e-03 -2.92992126e-02\\n  1.42917167e-02  2.92362291e-02 -7.40833133e-02  1.50597664e-02\\n -5.10547357e-03 -3.71848680e-02  1.54445414e-02  4.53949533e-03\\n -4.63589393e-02 -2.44704317e-02  4.48700450e-02  3.52765322e-02\\n -2.84300093e-02  3.50550562e-02  2.23246682e-03  2.37862431e-02\\n  1.70012582e-02  2.72209160e-02 -8.22032616e-02  2.70018224e-02\\n -1.55991185e-02 -5.69279678e-02 -4.09096293e-02 -2.27958206e-02\\n -8.03337321e-02  7.54778162e-02 -8.90549924e-03  6.38516108e-03\\n -4.27213758e-02  3.11646089e-02  3.89252254e-03 -3.06545850e-02\\n  2.55436283e-02  6.84244186e-03 -5.63467816e-02 -1.93414539e-02\\n  4.58750688e-03 -1.93653069e-02 -2.12833211e-02 -2.27532652e-03\\n -7.62251345e-03 -8.19219127e-02 -1.21109383e-02 -5.81682893e-03\\n -5.57329564e-04 -5.66369779e-02  1.53902275e-02  2.06826366e-02\\n  1.78347137e-02 -5.96053191e-02 -2.51873303e-02  8.09305813e-03\\n  8.90261959e-03 -5.89636043e-02  6.95840456e-03 -3.95343872e-03\\n -1.28671508e-02  3.26380581e-02 -4.26666392e-03  4.02323268e-02\\n -4.20689397e-02  1.61577072e-02 -4.76915538e-02  3.59333940e-02\\n -5.98824024e-02 -8.92264582e-03 -1.32869296e-02 -1.40467687e-02\\n -2.27180254e-02  3.79198007e-02  2.35485826e-02 -1.35866618e-02\\n  1.66984741e-02 -2.79771239e-02 -2.64034271e-02  3.19124642e-03\\n  5.10480329e-02  4.26232778e-02  5.36590479e-02  4.81936596e-02\\n  2.41196323e-02  2.92322133e-02  5.76051790e-03  8.43789503e-02\\n -2.30338387e-02 -1.97438728e-02  7.55391666e-04  1.11149669e-01\\n -1.04119340e-02  5.62517047e-02 -9.43111442e-03  2.82829423e-02\\n  6.89461082e-02  1.95375131e-03 -2.88095213e-02 -3.43242288e-02\\n  1.09885680e-02  4.52213408e-03  5.24024777e-02  2.15375312e-02\\n -1.21719409e-02  2.00598873e-03 -1.02800243e-02  1.54405572e-02\\n -7.51611963e-02 -3.81444693e-02  6.74676746e-02 -3.86720598e-02\\n -1.18801445e-02  5.20685762e-02 -2.74553197e-03 -5.65691479e-02\\n  2.34430027e-03 -6.74937367e-02  1.12559469e-02  3.93341947e-03\\n  1.94836948e-02 -5.16018309e-02 -3.48338741e-03 -4.85589309e-03\\n  2.31270143e-03  1.64997182e-03  1.36185931e-02  4.19507027e-02\\n  1.10118510e-02 -1.85597334e-02 -1.08323051e-02 -5.13559021e-02\\n -1.28429318e-02  3.06321103e-02  3.00016925e-02  1.00028552e-02\\n -1.34349605e-02 -7.18127787e-02  2.66990270e-02  1.45027842e-02\\n -2.45161615e-02  9.33946762e-03 -2.82920077e-02 -2.90721487e-02\\n  9.58910510e-02  3.68098840e-02 -3.61214913e-02 -2.20572632e-02\\n  2.80166883e-02  1.68830119e-02  1.11455806e-02 -2.52494514e-02\\n -4.70001996e-02  6.11764416e-02  3.98260914e-02 -1.96277630e-02\\n  5.38920471e-03 -2.86142137e-02 -3.63584533e-02 -4.12422828e-02\\n -6.29389985e-03 -3.26008052e-02  1.18498757e-01 -5.44705801e-02\\n  9.72955429e-04 -2.72770301e-02  2.06424147e-02  7.37389876e-03\\n  1.60942357e-02  3.27202259e-03 -4.90263849e-03  2.17206590e-02\\n  2.41906140e-02  1.10758223e-01 -5.56444675e-02  2.84288893e-03\\n  3.02807838e-02  3.43380054e-03  4.82638739e-03  3.49874236e-02\\n  3.60810496e-02 -4.22445238e-02 -1.71069037e-02 -3.84198241e-02\\n -5.27140312e-03 -2.24287100e-02  6.97281286e-02  1.67110879e-02\\n -3.61917689e-02 -6.11239225e-02 -1.56133622e-03 -3.33242640e-02\\n -6.06495747e-03  4.87040589e-03  8.84723704e-05  4.41851392e-02\\n  1.02643308e-03 -6.99501997e-03 -8.95200763e-03  8.05643573e-03\\n  6.82961335e-03 -1.07100876e-02  6.31556958e-02  1.07107973e-02\\n  7.59185851e-03  2.80339886e-02  3.24019417e-02  2.92927083e-02\\n -1.24088051e-02 -2.99130995e-02  5.12804799e-02 -2.41260808e-02\\n  1.93981733e-02 -4.19570431e-02 -1.78537704e-02  3.18482853e-02\\n -3.56528014e-02 -2.61999597e-03 -1.55929178e-02  1.61180273e-02\\n -8.20630137e-03 -6.56003952e-02 -4.04492728e-02 -1.66023092e-04\\n  1.62513666e-02 -8.39184225e-03 -1.42516531e-02 -1.63913630e-02\\n -5.42413071e-02  9.25722066e-03  1.37621788e-02  1.57498333e-04\\n -8.04432901e-04  1.49382493e-02 -6.01088367e-02 -1.75279807e-02\\n  2.36398242e-02 -7.85731822e-02  2.52717570e-03 -6.94836080e-02\\n -2.08988655e-02 -5.65758720e-02 -2.50404119e-03 -4.80905324e-02\\n  4.65459637e-02  1.05012385e-02  2.34484393e-02  1.27269244e-02\\n -1.38903968e-02 -3.12509947e-02 -4.83758003e-03  6.80367928e-03\\n  2.00386271e-02  1.14401337e-02  8.73564463e-03  2.02769767e-02\\n -2.20389552e-02 -3.35530154e-02  2.04976257e-02  2.30533313e-02\\n  1.50454883e-02 -9.12129134e-03  3.02382335e-02 -3.97830568e-02\\n  1.06700277e-02  2.78268871e-03  1.73491966e-02  6.32041022e-02\\n -5.84813347e-03 -3.72162685e-02 -6.84955763e-03 -3.27409967e-03\\n  3.14469379e-03 -3.93442065e-03  1.55563839e-02  1.13992784e-02\\n -5.05157523e-02  2.05750223e-02  8.59542191e-02 -1.42733343e-02\\n  1.24187488e-02  1.70780383e-02  1.22319534e-02 -1.64780822e-02\\n -2.83038653e-02  3.49100642e-02  6.36920407e-02 -1.90348849e-02\\n  2.21756082e-02  7.27457479e-02  5.74657286e-04 -7.62546659e-02\\n -8.53045937e-03  2.48893518e-02 -1.20730111e-02  2.00640578e-02\\n -8.72971769e-03 -7.10676387e-02  1.32208047e-02  7.89451227e-03\\n  2.77237222e-02 -3.05258986e-02 -2.28016041e-02 -1.79905389e-02\\n -7.04205856e-02  7.15689734e-02 -1.93404909e-02 -4.80268858e-02\\n -1.96160842e-02 -3.68255377e-03 -3.96493822e-02 -1.11919679e-02\\n  5.34685105e-02 -1.91812329e-02 -3.48874219e-02  2.57425066e-02\\n  1.87020116e-02 -5.74550359e-03 -2.78273933e-02  3.14826295e-02\\n -6.05624262e-03  1.20418305e-02  3.20607424e-02  2.44563445e-02\\n -4.59524766e-02 -2.79979929e-02  4.28288318e-02 -1.86245516e-02\\n -3.26939784e-02 -1.34787653e-04  4.51930054e-02  3.26613337e-02\\n -3.94739257e-03 -8.20507668e-03  4.27806824e-02 -4.24811468e-02\\n  4.63833241e-03  4.45554592e-03  8.01019743e-02 -2.21817400e-02\\n -1.93343069e-02 -2.64693126e-02 -8.27676654e-02 -4.42993455e-02\\n  2.61324421e-02  1.71695706e-02  1.69283915e-02  5.13591664e-03\\n -1.25544732e-02  2.53650118e-02 -3.30658117e-03  6.25560731e-02\\n  2.30708048e-02  2.81553343e-03  5.47456555e-03  3.08868736e-02\\n -6.94057345e-03 -8.79658666e-03  3.21009159e-02  1.65749174e-02\\n  3.49699035e-02  1.88859820e-03  1.84467882e-02  4.50526830e-03\\n  5.34193777e-02 -1.40081970e-02 -3.71745117e-02 -9.62304398e-02\\n -3.51246633e-02 -3.51448194e-03  1.10419653e-02 -2.78998706e-02\\n -3.45826894e-02 -4.18338925e-02 -1.14680137e-02 -1.31916357e-02\\n  1.16614960e-02  3.45619358e-02 -2.25220807e-02  2.33866610e-02\\n -2.08843257e-02  4.08891626e-02  8.74523595e-02 -1.28246313e-02\\n -1.96165536e-02  6.89770421e-03  1.13000805e-02 -2.12095622e-02\\n  4.05486021e-03 -2.15095393e-02 -3.01614925e-02  2.08774954e-02\\n -5.32340072e-02  6.28410280e-02  8.77524391e-02  3.26612629e-02\\n  8.26075226e-02  4.60154414e-02 -1.07782381e-02  7.30167031e-02\\n -3.48253436e-02  1.81131903e-02 -5.66559285e-03  5.29871434e-02\\n -5.97260632e-02 -1.42102027e-02  2.63874456e-02 -7.99958706e-02\\n -7.16229603e-02 -2.13142950e-02  1.91826199e-03  1.71258245e-02\\n -5.52854948e-02  2.64980234e-02 -5.03559643e-03 -4.42688353e-03\\n  6.31310344e-02 -4.56915349e-02  1.76713727e-02 -5.11759929e-02\\n -4.56723310e-02  1.84095725e-02 -1.82811171e-04  3.53447124e-02\\n  9.92849749e-03  9.85435327e-04  5.95884360e-02  6.19520843e-02\\n -6.12048805e-02 -5.52890599e-02 -1.84897520e-02  2.44726352e-02\\n  2.84623578e-02  4.02297592e-03  1.61010735e-02 -6.08489066e-02\\n -1.16544450e-02 -6.40513469e-03  3.33864125e-03 -2.29407707e-03\\n  2.62745824e-02 -7.03784451e-03 -1.75445024e-02 -4.63638008e-02\\n -1.64213106e-02  8.13606828e-02  8.27389583e-02  2.03869641e-02\\n -1.03847962e-02  5.13319101e-04  5.27023291e-03 -5.16808331e-02\\n -8.13112687e-03 -1.00786258e-02  1.06814336e-02 -1.01446016e-02\\n -3.60375270e-02 -3.87918875e-02 -6.73566014e-02  1.10903406e-03\\n -2.60519162e-02 -3.65629904e-02  1.00273257e-02  6.52238876e-02\\n -2.37493385e-02 -6.04334101e-02  4.17599380e-02  6.79410249e-03\\n -1.30121093e-02 -8.24153144e-03  4.81055230e-02 -6.39892986e-33\\n  2.75682043e-02 -5.79175353e-03  2.27588154e-02 -5.62150078e-03\\n -5.08662648e-02  6.02901028e-03  8.46165977e-03 -7.52913132e-02\\n -3.09020421e-03 -1.73122752e-02 -6.67413548e-02  1.93650797e-02\\n  1.79296173e-02  3.24621499e-02  7.69248605e-03  8.66517366e-04\\n  2.08140295e-02 -3.08938362e-02 -2.82787811e-02  9.81714427e-02\\n  1.39160352e-02  1.62129791e-03  6.64681196e-02 -9.34938807e-03\\n -1.57173462e-02  6.26251567e-03 -7.36718252e-03 -2.96839643e-02\\n -4.04116176e-02  4.03938703e-02 -2.22136918e-02  4.73648496e-02\\n  1.05345324e-02 -7.75776431e-02  8.06541299e-04 -5.73390126e-02\\n -5.04197087e-03 -2.41441987e-02  5.58532663e-02 -9.34118871e-03\\n  5.15366681e-02 -2.01361123e-02  3.79804969e-02  5.05157514e-03\\n -3.85693349e-02  1.69585273e-02 -1.56666171e-02 -7.48700416e-03\\n -1.45675400e-02 -4.75321002e-02  3.44063708e-04  3.09605449e-02\\n  9.22639202e-03  9.71247628e-02  2.29884330e-02 -4.02396210e-02\\n -1.35914572e-02  6.04482256e-02 -1.76810736e-05 -3.33940163e-02\\n  7.64584839e-02  4.72322758e-03  2.35729627e-02 -1.70309786e-02\\n  6.07102513e-02  3.41526307e-02  2.69633569e-02 -1.21238045e-02\\n  2.19586156e-02 -6.48658723e-03 -2.09475644e-02  3.17391334e-03\\n  1.65164676e-02  5.14545152e-03  5.29582798e-02 -9.64877382e-03\\n -3.09280027e-02 -7.17739249e-03 -6.48493916e-02  1.66274980e-02\\n  3.85874882e-02  1.49657270e-02 -2.65139272e-03  2.92185205e-03\\n -2.93212780e-03 -4.25252691e-02  1.65456291e-02 -1.51137328e-02\\n  1.40418308e-02 -2.86419205e-02  6.64681988e-03  1.98122319e-02\\n  2.78188661e-03 -2.52878293e-02 -4.05726768e-02  3.57906371e-02\\n -1.37990685e-02  1.00805443e-02  1.40294782e-03  3.48925814e-02\\n -6.64164498e-02 -4.72109206e-02 -1.95096247e-02  2.09057238e-02\\n  2.34445948e-02 -6.25095703e-03  4.03847061e-02  3.86318304e-02\\n -7.34586865e-02  3.04524042e-02 -3.59354354e-02  2.16152314e-02\\n -3.18961255e-02 -4.78732772e-03  2.05480233e-02  9.39302053e-03\\n  4.12714407e-02 -1.27668558e-02  1.04306024e-02 -6.29704227e-05\\n -5.04687317e-02 -1.24728493e-03 -2.34915260e-02  6.21981919e-03\\n  1.82484966e-02  2.10893862e-02 -3.25180888e-02  2.31396388e-02\\n  5.26776910e-02 -4.00814787e-02 -4.93105641e-03  2.98963301e-02\\n  2.89896349e-07  2.03862842e-02  4.18336876e-03  6.70287684e-02\\n -2.91706119e-02  2.76043080e-03  1.81552619e-02 -1.08473003e-02\\n  4.51353937e-02 -1.15357107e-02  1.13731129e-02  1.50250550e-02\\n -5.22139156e-03  6.09173765e-03  3.66001092e-02 -9.00659189e-02\\n  4.40605283e-02 -6.53610304e-02 -2.42052283e-02 -8.58402103e-02\\n  6.84200004e-02 -4.07473044e-03  9.81401503e-02  1.70569047e-02\\n -4.54387106e-02  3.81209105e-02 -5.05034216e-02  2.33915709e-02\\n -7.39985183e-02 -1.61286313e-02  5.32588409e-03  5.83545640e-02\\n  4.05963324e-02 -4.41105617e-03  1.07002398e-02  1.63739808e-02\\n  4.82165813e-02 -6.17382973e-02  1.15930093e-02  1.58177484e-02\\n  1.77605879e-02  7.34160608e-03 -7.89542496e-02  1.78537164e-02\\n -6.54992834e-02  5.56942113e-02 -9.22330283e-03 -1.57887619e-02\\n  3.37118134e-02  1.12477075e-02 -9.40056220e-02  2.95080841e-02\\n -2.48150490e-02  5.82475066e-02  1.76365469e-02 -3.62215042e-02\\n -5.27346581e-02  2.60202661e-02 -6.53587580e-02  5.59917353e-02\\n -1.02188792e-02 -4.33854833e-02 -5.68956286e-02 -7.22872466e-02\\n  1.95709821e-02  3.35107781e-02  7.25416979e-03  3.01143732e-02\\n  2.76459626e-34  3.59259956e-02 -7.42876083e-02  6.82586953e-02\\n -1.11682713e-02  1.04012396e-02  2.99777407e-02  7.36122057e-02\\n  5.48719941e-03 -5.76601885e-02 -6.45431951e-02 -1.23078544e-02]'},\n",
       " {'page_number': 4,\n",
       "  'sentence_chunk': 'from OpenAI. GPT-3 is a scaled-up version of this model that has more parameters and was trained on a larger dataset. In addition, the original model offered in ChatGPT was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI’s InstructGPT paper (https://arxiv.org/abs/2203.02155). As figure 1.6 shows, these models are competent text completion models and can carry out other tasks such as spelling correction, classification, or language translation. This is actually very remarkable given that GPT models are pretrained on a relatively simple next-word prediction task',\n",
       "  'sentence_chunk_size': 604,\n",
       "  'sentence_chunk_word_count': 90,\n",
       "  'sentence_chunk_tokens': 151.0,\n",
       "  'embedding': '[ 6.33271113e-02 -1.98990740e-02  5.01260906e-03  3.50404233e-02\\n -7.22375512e-02 -3.14286910e-03 -2.92835012e-02  4.73458990e-02\\n  5.34667373e-02 -9.89746768e-03 -6.32058410e-03 -3.17402631e-02\\n  1.09270094e-02  1.58529337e-02  2.83159092e-02 -7.94143602e-02\\n  4.90085334e-02  3.45298797e-02 -1.13202110e-02 -3.83091206e-03\\n  2.56129708e-02  2.91872080e-02 -5.93421923e-04  7.33762458e-02\\n  1.55897466e-02 -3.06931734e-02 -2.24150214e-02 -3.88216451e-02\\n -5.20241633e-02  7.12528871e-03  2.09897831e-02  4.80355844e-02\\n -5.64931100e-03  3.18016596e-02  2.00865247e-06  8.53000674e-03\\n -1.66191757e-02 -2.44678860e-03  4.92783375e-02  2.19771340e-02\\n  4.89809886e-02  4.83060405e-02  6.83048321e-03  1.90772191e-02\\n -2.01095901e-02  3.90374288e-02  6.48820698e-02  8.71692598e-02\\n -5.31501183e-03  2.97343545e-02 -1.10240718e-02 -1.54275047e-02\\n  6.33663312e-02 -2.69651599e-02  6.23622537e-02 -5.75924218e-02\\n -4.03525354e-03  2.27788649e-03  4.08223383e-02  3.72152701e-02\\n -1.21903131e-02  2.21109819e-02  4.68843710e-03  2.75671557e-02\\n -3.11988220e-02  3.68576348e-02 -3.38273831e-02 -3.91159132e-02\\n -2.13101637e-02 -6.43418450e-03  1.51224686e-02 -2.58073565e-02\\n -4.26233672e-02  9.23636835e-04  4.36423346e-03  1.17646996e-02\\n -7.65951211e-03 -7.90746361e-02 -4.88885455e-02  4.47511636e-02\\n  6.40460197e-03 -1.30768185e-02 -2.84016063e-03 -2.81906705e-02\\n  3.73683195e-03  8.63533691e-02  2.35953908e-02 -4.15702388e-02\\n  2.68244743e-02  4.08242792e-02 -5.74513562e-02  1.00542335e-02\\n  1.49869006e-02 -1.54646381e-03  8.49084277e-03  2.92040617e-03\\n -4.67512049e-02  2.77986601e-02  3.04354064e-04 -2.29924433e-02\\n -1.39691224e-02  8.86025056e-02  2.29633190e-02 -3.68123036e-03\\n -1.14248462e-04  3.41803133e-02 -3.86697911e-02  1.20597156e-02\\n -3.65465954e-02 -1.38394712e-02 -4.48910482e-02 -1.43719390e-02\\n -7.92197064e-02  3.84962671e-02 -1.83738284e-02 -4.99565303e-02\\n -5.97280152e-02  2.33623702e-02  1.68266557e-02 -3.88737656e-02\\n  7.44661242e-02  2.07828842e-02 -6.27722219e-02 -4.55997372e-03\\n -4.21154685e-02  1.12440418e-02  2.64623668e-03 -2.11686268e-02\\n  1.05279190e-02 -4.24024723e-02 -6.35240320e-03 -9.81544051e-03\\n -1.31204352e-02 -5.92099503e-02  4.90878755e-03  3.32182981e-02\\n -5.23434877e-02 -6.55425936e-02 -4.03095372e-02  4.99567911e-02\\n -1.74192116e-02 -7.24618360e-02 -2.40631383e-02 -7.84554239e-03\\n -1.75453201e-02  1.67872719e-02 -2.23320536e-02 -3.62506672e-03\\n  1.30091272e-02  4.07879427e-02 -6.80657700e-02  2.55897306e-02\\n -1.43137584e-02 -7.23938085e-03  4.08682600e-02  1.07216919e-02\\n -1.61075573e-02  3.15891057e-02 -2.22179294e-03  1.56409703e-02\\n  1.74958538e-03 -2.56341714e-02  6.89177727e-03  6.19472340e-02\\n  1.91718910e-03 -2.36298470e-03  4.66990024e-02 -3.01953610e-02\\n  2.57221702e-02  1.42966341e-02 -1.04511874e-02  9.09194425e-02\\n -1.51909993e-03  5.89505630e-03  6.84653893e-02  6.44825250e-02\\n  2.96507664e-02  5.68586551e-02  1.62611436e-02  3.55334729e-02\\n  5.76830655e-02  6.05455339e-02 -1.22521566e-02 -8.15679040e-03\\n -5.80728464e-02  3.50586884e-02  2.87015438e-02  4.76992279e-02\\n  2.02349899e-03 -2.31151935e-02  2.76173390e-02  2.22740462e-03\\n  7.07403896e-03 -2.98949964e-02  2.90777944e-02 -1.79774109e-02\\n -7.15496689e-02 -3.02572772e-02  3.08912359e-02 -2.47497298e-02\\n -2.88855359e-02 -5.95553368e-02  1.92989744e-02  6.15902282e-02\\n  5.85197471e-02 -4.25170269e-03 -1.23541448e-02  1.06056258e-02\\n -4.35678661e-03  3.23951319e-02  8.46402720e-03  5.26764728e-02\\n -7.59628695e-03 -4.53000180e-02 -1.89219229e-02  3.44799943e-02\\n  1.06970742e-02  3.08872424e-02  5.68507947e-02  1.47566888e-02\\n -1.50335580e-03  1.37483766e-02  2.58683395e-02 -2.27352767e-03\\n -4.52605225e-02 -2.02834681e-02 -2.77615674e-02 -2.14275271e-02\\n  4.56981994e-02  2.56773215e-02 -4.71735606e-03  6.59285672e-03\\n  4.62665875e-03  3.87544893e-02 -8.30433983e-03 -1.39061930e-02\\n -4.20256704e-02  1.40158366e-02  2.12889370e-02  1.22266458e-02\\n  1.97376893e-03  3.40728089e-02  1.37078092e-02 -4.70017791e-02\\n -2.90187038e-02 -1.64578371e-02  7.15831593e-02 -6.87312111e-02\\n  4.61473875e-03  2.16290355e-02  2.04837751e-02 -2.88075372e-03\\n  3.74526195e-02 -9.42611892e-04  5.27225807e-03  1.47353122e-02\\n  3.18286084e-02  2.02384442e-02 -3.66663486e-02 -5.00682853e-02\\n  6.51086196e-02  2.94305030e-02  1.41090378e-02  1.96056385e-02\\n  4.94769588e-02 -6.91437582e-03 -6.87639415e-02 -8.31611827e-02\\n  1.22224335e-02 -2.99711392e-04  1.91896167e-02  2.40415968e-02\\n -4.41822875e-03 -2.87013110e-02  3.32033187e-02 -4.84866835e-02\\n  2.58741975e-02  1.70237329e-02 -2.69678030e-02  1.62729919e-02\\n -1.26145789e-02 -1.69431977e-02 -4.41901237e-02  4.34766663e-03\\n -8.52443185e-03 -4.63205529e-03  2.76544802e-02 -6.97268769e-02\\n -1.20124789e-02  1.34470174e-02 -3.53799462e-02  5.43130003e-02\\n  1.06155649e-02 -6.63677901e-02  8.26568622e-03 -3.81093137e-02\\n  6.83279559e-02 -6.23643107e-04  1.13087809e-02 -3.31676309e-03\\n -6.62931576e-02 -1.16185099e-03  1.02501865e-02  2.42937934e-02\\n -7.56150857e-02 -7.44348243e-02  6.86746603e-03 -1.66054275e-02\\n -3.43822539e-02  3.27475667e-02  5.35985595e-03 -1.50450449e-02\\n -7.63434246e-02 -3.50324288e-02 -2.12760293e-03  2.42439751e-02\\n -1.07304044e-02  2.81428751e-02 -5.95420152e-02 -4.07507010e-02\\n -3.32627483e-02 -2.49646567e-02  3.36791873e-02 -6.06497638e-02\\n -3.40528414e-02 -5.43712415e-02  4.98512574e-03  2.32052542e-02\\n  4.17822376e-02  3.93918082e-02  1.76747963e-02  5.96164260e-03\\n -9.15170554e-03 -3.61104240e-03  4.50226478e-02  3.92368622e-02\\n  9.43654031e-03 -2.60917237e-03 -7.14751985e-03  3.71197052e-02\\n  1.45741804e-02 -1.11394317e-03  3.97055447e-02 -2.34352192e-03\\n -4.30865632e-03 -9.67672933e-03  5.73355854e-02 -1.03094736e-02\\n -1.59772225e-02  5.70851262e-04 -1.03221496e-03  7.88081363e-02\\n -6.41430467e-02 -1.44117363e-02 -4.17316742e-02 -1.02457693e-02\\n  3.69648961e-03 -3.94095071e-02 -3.85622196e-02  8.49842280e-03\\n  1.47380698e-02  4.28300090e-02  5.77518670e-03 -7.20860884e-02\\n  2.39139199e-02 -1.58409663e-02  3.51179838e-02 -3.21967937e-02\\n -5.88384792e-02  6.73235282e-02  2.42550429e-02 -2.57545039e-02\\n -4.24251310e-04  7.29986876e-02 -7.04010390e-03 -8.58441517e-02\\n -4.15467843e-02  1.52289029e-02  5.36383949e-02  6.63295807e-03\\n -6.31311396e-03 -5.53078316e-02 -2.34767944e-02  3.98694025e-03\\n -1.54944006e-02  5.53062418e-03 -1.40383532e-02 -1.11207059e-02\\n -5.06559387e-02  3.88828740e-02  1.12358239e-02 -2.45877672e-02\\n  4.04482055e-03  1.51374098e-02 -2.16570776e-02  2.43987814e-02\\n -5.32947073e-04  2.85523459e-02 -3.74445366e-03  1.52499024e-02\\n -5.22056455e-03 -4.18999754e-02 -1.95878595e-02  4.06672023e-02\\n -4.58897837e-03  4.10350524e-02  6.47762492e-02  1.45112621e-02\\n -3.38498875e-02 -1.92538369e-02  6.82775676e-02  2.73091742e-03\\n -1.15024503e-02 -1.36751328e-02  7.02965334e-02 -3.80255505e-02\\n  2.50443146e-02 -2.67539285e-02  3.65585163e-02  7.11173750e-03\\n  3.65942381e-02  2.50988733e-03  6.88845217e-02 -7.91995414e-03\\n -8.26649833e-03 -6.93828473e-03 -1.13337040e-02  3.93517502e-03\\n  2.60600876e-02 -1.55931641e-03 -9.98248253e-03 -6.19045757e-02\\n -5.76605871e-02  4.14351933e-02 -1.44774583e-03 -2.27482282e-02\\n -3.80222336e-03 -1.11938617e-03 -3.55607681e-02 -1.87385231e-02\\n -5.29077500e-02  2.39455085e-02 -3.15109342e-02 -5.18354075e-03\\n  5.14463969e-02 -3.88164399e-03  2.69271843e-02  1.29054161e-02\\n -3.47642861e-02  3.51872295e-02 -1.76549088e-02 -8.96668136e-02\\n -6.95328088e-03 -7.65779987e-03 -1.40150739e-02 -2.55174097e-02\\n  9.23772063e-03 -4.23370712e-02 -4.02282923e-02 -1.17784683e-02\\n  5.69120459e-02  3.00086308e-02 -1.86767038e-02 -2.06675883e-02\\n -2.44064443e-03 -1.94447674e-02  2.09619049e-02  3.59200267e-03\\n -8.59138742e-03  7.83467013e-03  7.66600249e-03  9.52335447e-03\\n -5.78230340e-03  4.46459129e-02  9.93210357e-03 -6.96109980e-02\\n -9.17842088e-04  6.58852533e-02  3.40389274e-02  6.46544397e-02\\n  2.89218444e-02  5.27149290e-02 -2.00514346e-02  4.47453335e-02\\n -4.25418280e-02 -7.43076019e-03 -6.37632161e-02  2.82952338e-02\\n -7.07422569e-02  1.53474305e-02  6.63085058e-02 -2.67294887e-02\\n -2.70568375e-02 -6.91198260e-02 -9.44313128e-03  3.41485180e-02\\n -5.77513576e-02 -3.98161598e-02 -3.18857692e-02 -2.46056952e-02\\n  1.20734898e-02 -4.02573422e-02  2.92388303e-03 -2.01853421e-02\\n -3.16816084e-02  2.05287617e-02 -2.36092992e-02  9.02024209e-02\\n  1.83836464e-02 -1.11999251e-02  5.64107001e-02  3.03852521e-02\\n -6.03395738e-02  1.01162372e-02 -6.92875907e-02  1.01259522e-01\\n  7.84148946e-02  3.51956859e-02  3.02622467e-02 -4.21420112e-02\\n -3.19641866e-02 -1.17956931e-02  1.45443464e-02  2.76077352e-03\\n -1.77521743e-02 -5.68457134e-02 -2.31261551e-02 -2.27360819e-02\\n -3.92290391e-02  8.91452059e-02  4.47793566e-02  2.65048854e-02\\n  1.73891503e-02 -6.45539016e-02 -2.33365148e-02 -6.76140264e-02\\n -2.39087362e-02  5.56954630e-02  1.07662044e-02  1.26398290e-02\\n -3.79818939e-02 -1.82162481e-03 -7.19668567e-02  2.07743924e-02\\n -3.32647599e-02 -6.60656393e-02  2.27491707e-02  5.92079200e-02\\n -3.46651375e-02 -7.65278935e-02 -2.72782557e-02 -2.82427873e-02\\n -1.18698180e-02  1.63549911e-02  1.42064341e-03 -5.96700585e-33\\n -3.01176570e-02  1.73104636e-03  2.13443283e-02  6.41822368e-02\\n -3.41419540e-02 -5.32805882e-02  1.90106872e-02 -5.61416615e-03\\n -1.15311537e-02 -3.12322611e-03 -4.04017083e-02  6.07812963e-03\\n  1.19803809e-02  1.62680671e-02 -3.44981737e-02  2.50812545e-02\\n -6.39913138e-03 -2.71904841e-02 -9.97876842e-03  5.45161255e-02\\n  8.46528274e-04  4.70063724e-02  7.34483078e-02 -1.03840139e-02\\n  2.70394310e-02 -1.33817950e-02 -3.35272886e-02 -4.38101813e-02\\n -7.03782588e-02  2.98887193e-02 -3.37302014e-02  1.84698999e-02\\n  6.31540129e-03 -2.32801400e-02  2.15859264e-02 -5.32947341e-03\\n -2.54217628e-02 -1.09863374e-02  5.54593187e-03 -2.17367336e-02\\n -4.33261022e-02  2.30707936e-02 -5.35767945e-03 -2.73257587e-02\\n -6.63040653e-02 -2.44991388e-03 -1.30686620e-02 -3.02367378e-02\\n -3.60408351e-02  2.38762200e-02 -3.99925001e-02  1.48369428e-02\\n  4.01595645e-02  8.30460805e-03 -4.95238882e-03 -1.37482816e-03\\n -1.03030298e-02  1.17383059e-02  2.66905278e-02 -3.73611115e-02\\n  5.93683831e-02  7.36328438e-02 -3.18755023e-02 -3.67020294e-02\\n  1.28236096e-02  3.61906588e-02  2.84658261e-02 -4.44988580e-03\\n -6.21280447e-02 -1.96811631e-02 -3.08890594e-03  2.36081723e-02\\n  4.45943177e-02 -8.91989376e-03 -1.24019394e-02 -6.52905703e-02\\n  1.75470416e-03  3.95137779e-02  7.03968899e-03 -2.59649148e-03\\n  2.49315500e-02 -1.87398251e-02 -3.54686715e-02 -3.64224166e-02\\n -1.17350314e-02 -2.34555118e-02 -1.13818832e-02 -6.07039370e-02\\n  4.70023341e-02 -2.16766093e-02  2.56940834e-02  3.14904121e-03\\n  4.46273200e-03 -3.22943437e-03  3.23692821e-02  1.06286798e-02\\n -5.38886003e-02  3.40922512e-02 -1.15443170e-02  5.47955744e-02\\n -4.64615710e-02 -1.04808072e-02 -3.87891047e-02  2.62588877e-02\\n -2.46748663e-02 -3.00156139e-03 -3.38087380e-02  2.58857664e-02\\n -6.34476095e-02  3.00588254e-02 -5.27279302e-02  2.48233341e-02\\n -1.67665724e-02 -1.57792252e-02  3.23597156e-02 -3.27105038e-02\\n  2.21828129e-02  4.30972502e-02  2.25128960e-02  2.15139263e-03\\n -7.09063411e-02 -7.46309012e-02 -1.60879884e-02 -2.49168891e-02\\n  1.00061409e-02  6.63096383e-02 -3.38158458e-02 -6.13373425e-03\\n  1.14665460e-02  3.39671341e-03  7.54255848e-03  5.28932810e-02\\n  2.58989076e-07  2.55211871e-02  7.05310702e-02  4.09024023e-02\\n  5.27035492e-03 -1.34425442e-04 -1.57320015e-02  1.46684395e-02\\n  3.64067443e-02  3.44033167e-02  4.98055900e-03  6.30035028e-02\\n  6.28087902e-03  2.17544883e-02 -1.50890462e-02 -7.57807866e-02\\n  2.95271119e-03 -6.65150732e-02 -1.87593680e-02 -6.53855726e-02\\n  2.35188808e-02  3.31044309e-02  8.98034275e-02  3.90411913e-02\\n -8.75706133e-03 -4.67654318e-03 -5.64021133e-02  7.55057409e-02\\n -3.40507925e-02 -3.38384211e-02  8.74996558e-03  9.62048396e-02\\n  3.89969274e-02  8.49061587e-04  2.53995135e-03  7.37403473e-03\\n -5.70667954e-03  2.11214907e-02  7.40479007e-02 -1.41322669e-02\\n  3.10294461e-02  1.66366622e-02 -3.98591999e-03 -8.87194276e-03\\n -6.29267767e-02  6.66620135e-02 -7.84548838e-03  1.97618641e-02\\n  5.36050498e-02 -2.70894058e-02 -6.92320988e-02  2.32047029e-02\\n -7.64381094e-03 -2.09577996e-02  5.05083753e-03 -1.28036328e-02\\n  1.66511089e-02  2.99090380e-03 -1.49736488e-02  2.43571866e-02\\n -1.47465747e-02 -5.46710454e-02 -5.98148704e-02 -5.76889850e-02\\n -9.37085971e-03  3.59496512e-02 -3.38743217e-02 -1.56939477e-02\\n  2.38328219e-34  4.76449989e-02 -4.96338122e-02  4.89758588e-02\\n  5.70923949e-05  3.93246561e-02  2.32925415e-02  1.19304426e-01\\n  1.37113947e-02 -2.83945575e-02 -6.66413233e-02 -4.07619141e-02]'},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling. This means that we don’t need to collect labels for the training data explicitly but can use the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict. Since this next-word prediction task allows us to create labels “on the fly,” it is possible to use massive unlabeled text datasets to train LLMs. Compared to the original transformer architecture, the general GPT architecture is relatively simple. Essentially, it’s just the decoder part without the encoder. Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of autoregressive model. Autoregressive models incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves the coherence of the resulting text. Architectures such as GPT-3 are also significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times.',\n",
       "  'sentence_chunk_size': 1197,\n",
       "  'sentence_chunk_word_count': 190,\n",
       "  'sentence_chunk_tokens': 299.25,\n",
       "  'embedding': '[ 3.15158330e-02  2.47149281e-02  1.20861316e-02  2.95494702e-02\\n -4.68848683e-02  3.39988992e-02 -2.67395210e-02  3.78330313e-02\\n -3.89114395e-02 -1.64316874e-02 -2.42366567e-02 -4.97656316e-03\\n -9.80548677e-04 -6.59214100e-03  5.05812690e-02 -3.80594097e-02\\n  1.72240157e-02 -1.91875771e-02 -5.54702505e-02 -1.19084138e-02\\n -9.90671571e-03 -1.66024491e-02  9.55394353e-04  2.02504098e-02\\n -5.14876004e-03 -1.56699307e-02 -3.25333886e-02 -2.22684164e-02\\n -3.20229381e-02 -5.27448803e-02 -2.58119348e-02  2.70795729e-02\\n  4.59028855e-02  5.23831137e-02  2.05981701e-06 -2.10652202e-02\\n -1.60681829e-02 -1.71968602e-02 -4.57989099e-03 -3.67955938e-02\\n -5.03716478e-03 -3.69381234e-02 -1.93614475e-02  2.07264945e-02\\n -2.95455195e-02 -2.91701201e-02  5.75048514e-02  7.41448700e-02\\n -4.88051958e-03  4.95332070e-02 -7.15940306e-03 -9.13389027e-02\\n  5.94884306e-02 -7.37676257e-03  3.86124216e-02 -5.36309928e-02\\n -4.88317432e-03 -3.44673470e-02 -1.12684797e-02  5.68362437e-02\\n -5.40636443e-02  3.16663943e-02 -1.19859157e-02  2.50103883e-02\\n -1.59381349e-02  3.03319823e-02  8.02936498e-03 -5.26909344e-02\\n -6.08697571e-02 -3.18757892e-02  4.44711652e-03 -2.42690220e-02\\n -2.45310534e-02 -1.67128369e-02 -2.75465306e-02  2.41448171e-02\\n -1.76237132e-02  9.27934144e-03 -2.17269454e-02  7.03663677e-02\\n  2.18426418e-02  5.44457650e-03  2.53264047e-03 -4.37904559e-02\\n -2.43120566e-02  1.02397062e-01  1.92341469e-02 -5.11507541e-02\\n  6.05654530e-02  7.01016858e-02  4.02180403e-02 -8.98912922e-03\\n  1.52265076e-02  4.00706753e-02  1.57216042e-02  2.52233129e-02\\n -2.24677678e-02  1.87698267e-02 -5.27944602e-03  3.19916708e-03\\n -1.22757780e-03  5.30393049e-02  2.81275921e-02 -2.34176628e-02\\n -3.04345065e-03  5.63468523e-02 -5.87189943e-02  1.49698667e-02\\n -6.01609275e-02 -3.95400077e-03 -2.19960567e-02 -3.93629856e-02\\n -4.69155684e-02  5.83024211e-02  5.62623097e-03 -2.33730152e-02\\n -3.38179395e-02  3.41528617e-02  6.42325580e-02 -5.49745485e-02\\n  5.48599586e-02 -1.67639852e-02 -5.18205948e-02 -1.12244841e-02\\n -4.66997586e-02  2.48824097e-02  8.23704060e-03 -3.93497348e-02\\n -1.85414087e-02 -5.25282212e-02 -2.72824429e-02  1.90570019e-02\\n -9.29987989e-03 -2.06206702e-02 -5.42531535e-03  5.16930744e-02\\n -4.26586084e-02 -6.07395433e-02 -6.82573020e-02  5.04105203e-02\\n -1.17212585e-05 -3.49037424e-02  5.51353134e-02  7.78149767e-03\\n -1.92079786e-02  4.70862240e-02 -2.49473397e-02  2.83251936e-03\\n  1.42920772e-02  2.33989749e-02 -4.37214635e-02  4.80929017e-02\\n  4.25745966e-03 -1.52780733e-03  3.43104601e-02 -2.55838223e-03\\n  2.03400459e-02  4.14530933e-02  7.54042156e-03  1.81278344e-02\\n -7.15313898e-03 -2.06312407e-02  1.67509057e-02  6.41011149e-02\\n -3.71076949e-02  3.99828283e-03  4.90475679e-03 -3.11836563e-02\\n  4.10010815e-02  5.68220578e-02  2.16176864e-02  8.11407194e-02\\n  1.08866009e-03  3.31905228e-03  5.94514087e-02  7.12684095e-02\\n  6.33894429e-02  6.41746596e-02  2.16170829e-02  4.90378104e-02\\n  5.25574163e-02  8.62619653e-02  1.11508444e-02  2.84408275e-02\\n -2.86619570e-02  1.81876607e-02  2.04419754e-02  2.79763937e-02\\n -2.74204388e-02 -2.28655096e-02 -8.92689452e-03 -6.61478052e-03\\n -3.28211696e-03 -7.39143565e-02  1.54861175e-02  2.46227831e-02\\n -1.01449944e-01  1.05942510e-01 -1.33768199e-02 -1.55573832e-02\\n -1.91081520e-02 -4.73901965e-02 -2.69245841e-02  1.49776125e-02\\n  5.70828170e-02  1.78453568e-02 -3.91422585e-02 -2.32062172e-02\\n -5.67707717e-02  8.42577815e-02 -3.13338600e-02  3.94796990e-02\\n -4.46887687e-02 -3.39129008e-02 -2.27764212e-02  3.87561657e-02\\n -2.54623182e-02 -7.54371332e-03  2.35258713e-02 -3.03441891e-03\\n -8.60939547e-03 -4.20347694e-03  3.98999676e-02  4.02627811e-02\\n -3.42190191e-02 -6.04002289e-02 -2.54283492e-02 -5.18346997e-03\\n  2.08803713e-02  1.02765365e-02  8.84753373e-03 -8.68486334e-03\\n  4.50667776e-02 -5.70045365e-03 -8.99208058e-03 -1.77018680e-02\\n -1.11909918e-01  2.08183471e-02  1.40975919e-02 -1.51542136e-02\\n  3.96790616e-02  1.75623074e-02  5.22007351e-04 -5.26160933e-02\\n -4.09801863e-02 -5.33583388e-02  6.62581176e-02 -4.54831943e-02\\n  3.57833058e-02  1.99818797e-02  1.40894018e-02 -1.97212566e-02\\n  3.37169431e-02 -2.30306629e-02  1.89904519e-03 -2.10969988e-02\\n  3.44556123e-02  7.16541242e-03  3.20726633e-03 -9.75643937e-03\\n  7.48529434e-02  1.35827269e-02 -5.49761299e-03  1.89417992e-02\\n  1.02442550e-02 -2.99639422e-02 -7.16920048e-02 -8.09035357e-03\\n -1.82150714e-02 -3.90595291e-03  4.93314229e-02  2.26023812e-02\\n -1.02730226e-02 -2.31576301e-02  1.75848641e-02 -2.53385268e-02\\n  4.97912988e-02  9.47567727e-03 -3.62803675e-02  6.26641512e-02\\n -1.21096959e-02 -2.65304316e-02 -3.12899053e-02 -5.86772989e-03\\n  1.61590502e-02  3.56584564e-02  2.85864715e-02 -5.12135327e-02\\n -9.29319113e-02  1.08158505e-02  3.36276218e-02  2.73485295e-02\\n -9.34327487e-03 -6.06679842e-02  3.08776759e-02 -1.33549729e-02\\n  8.29501823e-02 -1.84739437e-02  4.88823578e-02  3.61415595e-02\\n -7.92498048e-03 -1.29688345e-02  1.80909727e-02  4.95382398e-03\\n -6.82628900e-02 -9.58532766e-02 -7.80340806e-02  7.66182668e-04\\n -5.39750466e-03  8.67370889e-02  4.94391704e-03  6.12299470e-03\\n -4.17196564e-02 -3.95453069e-03  3.20952721e-02  9.94413905e-03\\n  3.66255715e-02 -3.14730685e-03 -9.07581225e-02 -3.45311649e-02\\n  2.58014705e-02 -4.34678383e-02  2.19399165e-02 -5.73410429e-02\\n -1.97063647e-02 -4.82560396e-02  2.79542785e-02 -2.56598312e-02\\n  2.79597342e-02  4.56047133e-02 -1.41111173e-04  1.88238639e-02\\n -1.73487701e-02 -1.00258421e-02  2.30144598e-02 -2.79204105e-03\\n  4.30890825e-03  4.46444042e-02 -2.70180069e-02  9.70408879e-03\\n  6.91157486e-03 -1.87975615e-02 -2.55380329e-02  3.31996828e-02\\n -1.49561726e-02 -2.32054163e-02  5.61369248e-02  5.28405048e-03\\n -7.04995990e-02  4.25797235e-03  2.82083638e-02  5.83303086e-02\\n -2.51254868e-02 -2.52213739e-02 -3.81143577e-02  4.45431843e-02\\n -6.93013705e-03  1.84551172e-03  7.43931066e-03  2.25710720e-02\\n  3.11162937e-02  1.64009612e-02 -2.60138977e-03 -2.79393271e-02\\n -8.68884753e-03  1.28737297e-02  8.38737711e-02 -5.20658866e-02\\n -2.76957415e-02  7.09560961e-02  2.09053140e-02 -2.90810168e-02\\n -4.80380468e-03  1.04391508e-01 -3.57521488e-03 -6.38800561e-02\\n -2.52275150e-02  6.92518428e-03  3.00351400e-02  1.87972188e-02\\n  1.11714220e-02 -2.60484200e-02 -2.71636229e-02  1.57820769e-02\\n  5.12750586e-04 -9.40618105e-03 -1.69829726e-02 -1.37496907e-02\\n -4.56710011e-02  2.38121469e-02  8.12212843e-03 -3.08075943e-03\\n  1.70894694e-02  4.00489103e-03 -5.36704734e-02  3.34566198e-02\\n  9.86932870e-03  3.90484482e-02 -3.81988138e-02  3.44527565e-04\\n  1.56894531e-02 -7.74772763e-02 -1.03788069e-02 -4.11024950e-02\\n  1.05508156e-02  5.55078313e-03  7.73810893e-02 -1.90682728e-02\\n -4.02796343e-02 -7.21024126e-02  3.70996371e-02 -2.40156930e-02\\n  3.19201499e-02 -4.03421074e-02  5.17668203e-02 -2.82705538e-02\\n  1.52178686e-02 -1.66690126e-02  2.38782540e-02 -2.17078365e-02\\n  6.81165094e-03 -2.24278960e-02  7.68030882e-02 -4.43097576e-02\\n -4.29633185e-02  2.04367042e-02 -1.72498394e-02 -8.70027486e-03\\n  3.02515347e-02 -1.07037732e-02 -4.14237892e-03 -4.65135686e-02\\n -2.67018527e-02  5.11009805e-02  5.88317309e-03 -5.16745560e-02\\n  1.56451985e-02  1.80799868e-02 -1.67080574e-02 -5.97750535e-03\\n -2.24118587e-02 -2.45076511e-02 -3.13714035e-02  3.27609628e-02\\n  4.12272923e-02  2.43756007e-02 -9.88124125e-03 -3.39158848e-02\\n -4.75737303e-02  1.26022976e-02 -2.50120014e-02 -7.15076029e-02\\n -2.08793674e-02  6.39951453e-02 -6.65583462e-03 -5.94299100e-03\\n -8.38351529e-03 -6.47854656e-02 -3.07661667e-02  2.75732130e-02\\n  1.38352355e-02  1.86879933e-02  4.68345825e-03  4.74060886e-02\\n -1.74185261e-02  4.40280177e-02  2.01445594e-02 -2.19322983e-02\\n  2.00645365e-02  3.36947367e-02  1.56444367e-02 -1.68613985e-03\\n  3.44980834e-03  2.80925874e-02  9.06775799e-03 -2.72386745e-02\\n  1.52446954e-02  2.72933859e-02 -2.24203058e-02  2.44000088e-02\\n  5.36578707e-03  6.83417767e-02 -3.13441940e-02  1.00201413e-01\\n  1.00235338e-03  2.32968517e-02 -1.50968283e-02  4.80016731e-02\\n -3.57833691e-02  2.15192549e-02  4.88858670e-02 -2.00759862e-02\\n -2.45177243e-02 -9.24344454e-03 -4.82455306e-02  6.00486482e-03\\n -2.17836443e-03  4.51300964e-02 -2.67794393e-02 -2.48411000e-02\\n  1.01272902e-02 -2.33121756e-02 -1.49001367e-03 -1.93874370e-02\\n  1.08532161e-02  2.23301593e-02 -7.62797426e-03  8.56543034e-02\\n  1.50639974e-02  2.93576214e-02  2.78247520e-02  2.70015486e-02\\n -4.74069789e-02 -1.81949344e-02 -3.96247469e-02  3.14400382e-02\\n  9.07789450e-03  7.07568135e-03  2.31557190e-02 -3.82830948e-02\\n -4.31917375e-03  7.33923819e-03 -1.94049422e-02 -3.67679745e-02\\n  3.25931348e-02  3.77371572e-02 -1.36123961e-02 -1.70077272e-02\\n -3.17416489e-02  3.88032794e-02  4.57588360e-02  4.08708584e-04\\n  9.69683402e-04 -3.67428362e-02 -2.46920977e-02 -7.07833916e-02\\n  6.10159803e-03 -2.00299099e-02  2.81017926e-02 -6.81559741e-03\\n -1.44961663e-02  2.03114133e-02  3.35415006e-02 -6.46433467e-03\\n -9.95588023e-03 -1.08778715e-01  5.09931892e-02  4.41580415e-02\\n -2.82442681e-02 -5.21882623e-02  1.76648255e-02  7.65484199e-03\\n  2.85772886e-02  4.53896169e-03  1.67942718e-02 -6.33227640e-33\\n -3.85666229e-02 -3.53269055e-02  6.74127266e-02  3.63407992e-02\\n -6.19197898e-02 -4.34689038e-02  4.16658167e-03 -2.58099101e-02\\n  2.98064500e-02 -1.18414015e-02 -4.43400964e-02 -1.85500849e-02\\n  2.17189435e-02  5.03149815e-02 -1.44511890e-02 -2.83329021e-02\\n  2.85199452e-02 -3.96519415e-02 -1.99107686e-03  1.14349667e-02\\n -3.93173359e-02  6.66660145e-02  6.34059459e-02  4.35521128e-03\\n  1.05859870e-02 -3.45542356e-02  2.38851607e-02 -2.09137518e-02\\n  2.09549926e-02  2.50970665e-02 -5.92002198e-02  2.05113906e-02\\n  1.15358243e-02 -1.04873829e-01 -1.24302944e-02  4.48786952e-02\\n -4.96472344e-02 -9.89484508e-03  1.45994371e-03  2.76294444e-02\\n -2.60287616e-02 -1.95041280e-02  1.27610145e-02  2.23208289e-03\\n -1.10928781e-01  1.38345603e-02 -1.16891507e-02 -1.80075224e-02\\n -3.26619372e-02 -7.34513067e-03 -2.35077515e-02  1.51860027e-03\\n  4.81086187e-02  3.91356684e-02  3.97263877e-02 -1.51602188e-02\\n -2.46295277e-02  3.99103239e-02 -4.42550443e-02 -1.00906910e-02\\n  2.65212357e-02  6.78260773e-02  1.55486176e-02  2.84040496e-02\\n  4.82361391e-03  2.97105070e-02  2.58264784e-02 -8.16670805e-02\\n -4.00705859e-02  1.41106099e-02 -1.95712689e-02 -5.40354243e-03\\n  3.87286842e-02 -5.28943762e-02  1.50268646e-02 -2.23545805e-02\\n  1.22916847e-02  1.37482788e-02 -7.19055235e-02  4.48495299e-02\\n  4.14350852e-02  1.22914007e-02  1.83203127e-02 -2.97510345e-02\\n -3.37906331e-02 -2.76047662e-02  2.48385705e-02 -1.42744388e-02\\n  7.47998292e-03  4.84518846e-03  1.12789311e-03  1.26191853e-02\\n  1.05122721e-03 -1.65719166e-03  2.13799085e-02  1.89221203e-02\\n -2.89225001e-02  1.18245382e-03 -1.34911509e-02  2.90999282e-02\\n -4.13573161e-02 -2.53662579e-02 -4.98312572e-03  4.29181904e-02\\n -1.87881701e-02 -1.51712419e-02 -3.93878855e-02  4.65274826e-02\\n -9.65710804e-02  2.32854206e-02 -3.15824263e-02  9.38468426e-03\\n -2.13116035e-02 -7.03073293e-02  1.65547710e-02 -2.85484940e-02\\n  1.12522766e-03  4.03635465e-02  5.45613002e-03 -3.35956924e-02\\n -3.10463784e-03 -2.91985963e-02  2.12152954e-02  1.95733625e-02\\n -5.60077839e-03  4.87880968e-02 -2.54779663e-02  2.85163894e-02\\n  3.21105309e-02 -7.71652237e-02  5.88517217e-03  7.46517256e-02\\n  2.75351994e-07  4.22497727e-02  6.17733933e-02  9.07669514e-02\\n -1.66235748e-03  2.32525002e-02 -1.28698314e-03  1.28905494e-02\\n  5.80397295e-03  3.97618264e-02 -3.90220135e-02  2.58620363e-02\\n  5.09089343e-02  2.00694054e-02 -1.81326992e-03 -6.42093495e-02\\n  1.56995729e-02 -6.47862628e-02  1.57555975e-02 -4.76104319e-02\\n  1.57182291e-02  5.53954653e-02  7.38849863e-02 -2.01594308e-02\\n -2.70623378e-02 -2.15277821e-02 -1.89046990e-02  4.02916186e-02\\n -8.08976032e-03  1.41731277e-02  9.43049265e-04  4.39524539e-02\\n  1.13539090e-02 -1.94350835e-02 -9.13534593e-03  3.36293550e-03\\n  1.72861796e-02 -2.31068302e-02  2.94671077e-02  1.60596613e-02\\n -4.58832793e-02  5.11584692e-02  7.30271498e-03 -4.11852775e-03\\n -1.35604450e-02  6.51457906e-02 -5.37691079e-02 -2.37058084e-02\\n  5.71270883e-02  1.32503416e-02 -3.24579217e-02 -8.43538623e-03\\n -1.98860709e-02 -7.56139308e-03  7.44206423e-04  1.92662589e-02\\n  1.07000535e-02  1.03629827e-02 -5.75089604e-02  1.84272807e-02\\n  2.37893104e-03 -4.64559682e-02 -3.07892095e-02 -8.34240019e-02\\n -6.30856678e-03  1.35714961e-02 -7.29398476e-03 -2.94258017e-02\\n  2.51618859e-34  1.92675926e-02 -4.05073129e-02  2.86159664e-02\\n  1.30017577e-02 -4.03514737e-03  6.67229760e-05  6.78988770e-02\\n  3.38309631e-02 -3.55174877e-02 -1.14774942e-01 -2.46873237e-02]'},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'GPT-3 has 96 transformer layers and 175 billion parameters in total. GPT-3 was introduced in 2020, which, by the standards of deep learning and large language model development, is considered a long time ago. However, more recent architectures, such as Meta’s Llama models, are still based on the same underlying concepts, introducing only minor modifications. Hence, understanding GPT remains as relevant as ever, so I focus on implementing the prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs. Although the original transformer model, consisting of encoder and decoder blocks, was explicitly designed for language translation, GPT models—despite their larger yet simpler decoder-only architecture aimed at next-word prediction—are also capable of performing translation tasks. This capability was initially unexpected to researchers, as it emerged from a model primarily trained on a next-word prediction task, which is a task that did not specifically target translation. The ability to perform tasks that the model wasn’t explicitly trained to perform is called an emergent behavior. This capability isn’t explicitly taught during training but emerges as a natural consequence of the model’s exposure to vast quantities of multilingual data in diverse contexts. The fact that GPT models can “learn” the translation patterns between languages and perform translation tasks even though they weren’t specifically trained for it demonstrates the benefits and capabilities of these large-scale, generative language models. We can perform diverse tasks without using diverse models for each.',\n",
       "  'sentence_chunk_size': 1647,\n",
       "  'sentence_chunk_word_count': 238,\n",
       "  'sentence_chunk_tokens': 411.75,\n",
       "  'embedding': '[ 4.29281555e-02  6.35350645e-02 -2.07456686e-02  4.59065437e-02\\n -6.87272102e-03  1.67921036e-02  1.22053558e-02 -7.60856434e-04\\n -5.48325405e-02  6.42985012e-03 -5.87993637e-02 -5.89356758e-02\\n  2.73348950e-02  8.53965525e-03  4.78869304e-02 -4.96170558e-02\\n  2.88290568e-02 -2.35734098e-02 -5.32240048e-02 -4.50018502e-04\\n -1.80142764e-02  1.53207183e-02  3.74961384e-02  4.82273847e-02\\n  2.51916870e-02 -3.89511930e-03 -8.19084141e-03  8.44890997e-03\\n  7.90705904e-03  7.04032928e-03 -3.81733966e-03  3.44873779e-02\\n  2.73501314e-02  9.72423851e-02  1.95648158e-06 -2.79971659e-02\\n -1.62461922e-02 -1.35961231e-02  3.33391060e-03 -4.80609387e-03\\n  2.15172451e-02 -6.88208267e-02 -3.72393541e-02  4.69387881e-03\\n -5.20489626e-02 -5.16694151e-02  7.73882493e-02  3.69231030e-02\\n  1.17483595e-02  8.60722139e-02 -1.40243471e-02 -6.61085621e-02\\n  3.71831171e-02 -6.04416914e-02  3.32120135e-02 -4.20477651e-02\\n -9.61307436e-03 -5.01076952e-02 -5.22266813e-02  2.66111959e-02\\n -5.54404110e-02  4.30773422e-02  3.36692072e-02  3.33416350e-02\\n -3.49367820e-02  3.90062220e-02 -6.80647045e-03 -7.70699158e-02\\n -2.73174513e-02  2.55608074e-02 -5.37156910e-02 -2.84324884e-02\\n -2.70377696e-02  4.68252599e-03 -1.38898036e-02 -1.66174434e-02\\n -4.24049832e-02  3.35028358e-02  1.90283768e-02  4.96419333e-02\\n  4.18821312e-02  2.46900320e-02  5.05548380e-02 -3.01637482e-02\\n -4.13427725e-02  5.93465902e-02  4.32531014e-02 -1.68376956e-02\\n  6.73812674e-03  5.07799424e-02  4.36513778e-03 -1.27850231e-02\\n  1.75830107e-02 -2.30446830e-03  2.35621240e-02  1.73117854e-02\\n -1.70240812e-02 -4.31121560e-03  8.75891000e-03 -1.48968948e-02\\n  2.82005500e-02  8.01698789e-02 -1.31396679e-02  6.51095435e-03\\n -3.65419388e-02  6.37215376e-02 -8.93095136e-02  2.68609915e-02\\n -7.20827654e-02 -2.47260444e-02 -8.02808441e-03 -1.87660344e-02\\n -3.45090665e-02 -1.09199593e-02 -1.71136633e-02 -1.60578489e-02\\n -2.83354223e-02  2.24751122e-02  3.97439227e-02 -2.46157311e-02\\n  3.26709710e-02 -1.78039438e-04 -8.93649384e-02 -2.86891945e-02\\n -1.21552590e-02  6.55952170e-02 -5.80326887e-03 -1.15877797e-03\\n -3.58222052e-03 -6.29615337e-02 -3.37481573e-02  1.26699358e-02\\n -1.36093777e-02 -3.34491804e-02  4.05026739e-03  5.53360619e-02\\n -4.50291596e-02 -3.47956493e-02 -3.67717631e-02  2.05442663e-02\\n -1.39292963e-02 -7.06365928e-02  5.48652709e-02 -3.61611582e-02\\n  4.74892706e-02  1.67244207e-02  2.16271859e-02 -2.79444233e-02\\n  6.89263176e-03  9.28800926e-03 -1.14034135e-02  6.75031990e-02\\n  2.64799073e-02 -1.28809186e-02  3.23035344e-02  2.03174464e-02\\n  1.53494421e-02  5.62526397e-02  4.13461588e-02  3.87376882e-02\\n  5.93306404e-03 -1.62094794e-02 -1.30215455e-02  9.16194841e-02\\n -6.03748718e-04  2.43774857e-02  4.74835047e-03 -3.26752439e-02\\n  2.32491121e-02  3.32787074e-02  4.83608507e-02  4.01895046e-02\\n -1.43445507e-02 -1.06683318e-02  6.35207072e-02  4.42577712e-02\\n  4.18160250e-03  9.65736657e-02  7.05537871e-02 -3.05478312e-02\\n -1.87488571e-02  4.78607938e-02 -2.90530007e-02  4.51343507e-02\\n -4.10443963e-03 -1.09932767e-02 -1.43107993e-03  7.69925490e-02\\n -2.56159175e-02 -1.12310648e-02  1.42875602e-02 -1.31500540e-02\\n  4.70508449e-03 -4.43905182e-02  1.93096101e-02  3.06570996e-02\\n -5.65434285e-02  7.43422210e-02 -1.86276380e-02 -4.18326706e-02\\n  4.17119218e-03 -3.75510007e-02 -4.91748750e-03  3.54269966e-02\\n  2.82678436e-02  2.18872707e-02 -4.58845384e-02 -9.78295039e-03\\n -1.86461657e-02  8.17663819e-02  4.92292158e-02  3.24323066e-02\\n  1.47176795e-02 -5.71825057e-02 -1.20202405e-02  2.02253349e-02\\n  2.48241401e-03  3.79874818e-02  1.35118058e-02  4.80193123e-02\\n  2.43942738e-02 -1.90174039e-02  5.41299907e-03  2.92027257e-02\\n -5.87154813e-02 -3.20559852e-02  6.50915271e-03 -2.89107586e-04\\n  2.14907844e-02 -5.05588250e-03  9.09908861e-03 -1.54080596e-02\\n  2.70298067e-02 -4.90251556e-02 -1.78305078e-02  3.65491435e-02\\n -8.51843953e-02 -1.17803942e-02  2.70647611e-02 -3.90483625e-02\\n  2.05571540e-02  2.03595757e-02 -4.13331948e-02 -7.24773854e-02\\n  2.04972457e-02 -2.92879827e-02  5.46532981e-02 -4.87784520e-02\\n  1.92222055e-02  2.64215912e-03  5.78920580e-02  6.85560110e-04\\n  2.71859914e-02 -1.43847745e-02 -1.33887688e-02 -1.98692014e-03\\n  1.12612676e-02  2.96013560e-02 -5.13108540e-03 -7.23224133e-03\\n  6.88992068e-02  2.64218226e-02  8.42591934e-03  2.77982354e-02\\n  4.36533317e-02 -1.87369995e-02 -5.00574149e-02 -4.39252108e-02\\n  3.16112749e-02 -2.12548822e-02  4.30684872e-02  3.23782004e-02\\n  4.95736115e-02 -9.03933216e-03  6.25887979e-03 -5.64200170e-02\\n  3.37199718e-02  1.26080140e-02 -1.23134833e-02  1.80221684e-02\\n -1.08630117e-02 -6.66318042e-03 -3.93094420e-02 -1.93610564e-02\\n  7.01912725e-03  5.24541140e-02 -5.67942578e-03 -2.51004510e-02\\n -5.81870973e-02 -2.56683165e-03 -2.04739068e-02  2.04515625e-02\\n -8.70264415e-03 -9.25210044e-02  5.67288464e-03 -2.86732074e-02\\n  8.17042366e-02  1.59272943e-02 -9.58092511e-03  1.74614172e-02\\n  2.21862886e-02 -2.17316067e-03 -2.13320442e-02  1.18112238e-03\\n -6.27617836e-02 -5.65701090e-02 -7.29821473e-02 -1.71460398e-02\\n -2.74045095e-02  6.44835234e-02  1.25850365e-02 -7.19697075e-03\\n -1.00645348e-01 -1.33097488e-02  6.61878847e-03 -1.79033596e-02\\n  4.42202650e-02  2.26979405e-02 -2.98582092e-02  2.26587784e-02\\n -2.03822553e-02 -4.94060107e-02  4.22325768e-02 -3.94265577e-02\\n -4.78583239e-02 -3.40272002e-02  9.23103280e-03 -2.97304541e-02\\n  5.49691282e-02  1.79877598e-02  2.16637254e-02  1.14114722e-02\\n -2.09856145e-02 -6.14796728e-02  3.44534740e-02 -8.90440494e-03\\n -2.37135161e-02  3.75110731e-02 -3.98757495e-02 -2.67780945e-02\\n -1.24123544e-02 -3.46628129e-02 -1.61709525e-02  4.41685952e-02\\n -1.66274663e-02  7.25966180e-03  1.20025091e-02 -4.49083969e-02\\n -5.96349314e-02 -2.09998004e-02  2.17963383e-02  8.52236003e-02\\n -3.39077674e-02 -7.89918564e-03 -3.45448330e-02  1.70297977e-02\\n  2.44831340e-03  1.28036393e-02  2.72314791e-02 -2.86137648e-02\\n  3.05392947e-02  3.25308852e-02  5.21703251e-03 -4.96567925e-03\\n -2.13728435e-02  1.73717886e-02  6.66573569e-02 -6.16139453e-03\\n -4.97514531e-02 -3.96560281e-05  4.54089642e-02 -4.56376970e-02\\n -1.77708529e-02  1.02094010e-01  6.77495729e-03 -6.48418739e-02\\n -5.86703792e-03 -2.41315621e-03  2.94315424e-02  3.27398553e-02\\n  1.41451862e-02 -9.24169868e-02 -2.21635774e-02  9.54701100e-03\\n  1.35764037e-03  1.06428321e-02  1.45854736e-02 -1.67165790e-03\\n -8.62488598e-02  4.71380092e-02  3.48372236e-02 -4.31308374e-02\\n -3.00697889e-02 -2.09371746e-03 -4.33892608e-02 -1.22010000e-02\\n  1.81189738e-02 -9.74546652e-03  1.11338850e-02 -1.18913306e-02\\n  3.09967971e-03  1.64716840e-02 -4.60749306e-02  1.13477455e-02\\n -1.94131359e-02  4.13922733e-03  9.32979882e-02  1.97228696e-02\\n -3.33107077e-02 -6.41184598e-02  4.56348946e-03 -2.21702959e-02\\n -2.92996899e-03 -1.56743061e-02  2.47396268e-02  2.54588034e-02\\n -2.02874728e-02  1.57589670e-02  2.73680631e-02 -6.31237030e-02\\n  1.07021686e-02 -1.77845135e-02  9.36778113e-02 -2.38595102e-02\\n -5.94870076e-02  2.98179481e-02 -1.01824934e-02 -1.49476165e-02\\n  4.42422815e-02  4.42156708e-03 -2.93996800e-02 -3.14957835e-02\\n -4.89684083e-02  2.57494412e-02 -5.11378981e-02 -5.84321655e-02\\n  6.01437362e-03  2.81171296e-02 -5.47985286e-02 -5.94803095e-02\\n -3.09896674e-02  8.80730979e-04 -1.18142618e-02  3.27250510e-02\\n -5.76214399e-03  8.60602420e-04 -1.63867436e-02 -2.90298443e-02\\n -8.39434098e-03 -8.22912157e-03 -1.53888650e-02 -4.42961194e-02\\n -7.06743589e-03  6.28304258e-02 -4.55565602e-02 -2.55845464e-03\\n  2.77162381e-02 -9.44383144e-02 -5.13481349e-02 -1.53688351e-02\\n  7.18853548e-02  2.46539209e-02  1.73964724e-02 -3.47111747e-03\\n -1.48772066e-02  3.17622945e-02  5.40377107e-03  1.34612601e-02\\n  7.88465794e-03  3.57084125e-02 -2.27406416e-02 -4.60839970e-03\\n  3.68480161e-02  1.78782847e-02  8.70811846e-03 -1.08815115e-02\\n  1.30973579e-02  1.27151040e-02 -4.05225866e-02  3.98174189e-02\\n -2.70054452e-02  5.52357212e-02 -3.85987833e-02  2.35708114e-02\\n -3.00079212e-02  2.95524877e-02  1.13344081e-02  3.05977818e-02\\n -5.39197139e-02 -4.80033597e-03  9.10438150e-02  2.62082797e-02\\n -8.58629530e-04 -3.48714297e-03 -4.49916627e-03  2.97148945e-03\\n -3.54396701e-02  1.78603716e-02 -5.41398861e-03  2.26044515e-03\\n  6.59171771e-03  2.07466297e-02  3.94378370e-03  2.19705347e-02\\n  7.49973406e-04  3.05848792e-02 -4.09750007e-02  3.97335403e-02\\n -5.06039709e-03  1.25588588e-02  1.98390149e-02  3.27666849e-02\\n -6.72294796e-02 -4.77242395e-02 -3.01528517e-02  9.88016836e-03\\n  7.33577833e-02  6.10932847e-03  8.26344714e-02 -3.49953324e-02\\n -2.03860868e-02  1.12008350e-02  1.41477706e-02 -3.64360437e-02\\n -1.06522832e-02  5.42739453e-03 -2.40536742e-02 -1.32174377e-04\\n -2.29291003e-02  5.88790737e-02  3.87659669e-03  5.66386320e-02\\n -1.26490835e-04 -1.21371830e-02 -1.18450057e-02 -4.67106290e-02\\n  6.77119428e-03 -3.55031453e-02 -1.13406526e-02 -5.62106026e-03\\n -5.33154048e-03 -3.62944007e-02  7.80297630e-03  3.07477615e-03\\n  1.46264501e-03 -4.85825278e-02  2.47149765e-02  8.99720564e-02\\n -5.70728481e-02 -3.80467363e-02 -1.80113278e-02 -1.77455023e-02\\n  4.75837057e-03 -5.79271698e-03 -8.47185403e-03 -6.38863621e-33\\n -2.23588757e-02  1.63726825e-02  3.19985338e-02  2.80510169e-03\\n -4.84860502e-02 -3.51384319e-02 -1.91243365e-02 -2.41629593e-02\\n -1.51201477e-02  9.30643640e-03 -9.37177148e-03  1.50116840e-02\\n  2.16069054e-02  2.18136460e-02  2.75895521e-02 -6.17434690e-03\\n  5.42187877e-02 -2.23687775e-02 -7.85344001e-03  4.32505682e-02\\n  1.35962628e-02  3.62202860e-02  1.27406165e-01 -1.87421702e-02\\n  5.92158642e-03 -7.97692314e-03 -3.94519903e-02  6.78973924e-03\\n -4.45981100e-02  3.46354581e-02 -3.73001955e-02  1.58710890e-02\\n  1.28071299e-02 -7.13328868e-02 -2.51006521e-02  2.50157807e-03\\n -6.94503561e-02 -2.55432464e-02  2.71135084e-02  4.58930321e-02\\n -1.58819072e-02 -4.89782877e-02  4.61054109e-02 -3.11397500e-02\\n -3.65770459e-02 -9.59869381e-03 -8.71063303e-03 -3.32825519e-02\\n -2.08275393e-02 -2.14811377e-02 -7.25965500e-02 -1.41350338e-02\\n  1.15981409e-02  1.34092579e-02  3.79013419e-02  3.38198245e-02\\n -1.96480099e-02  2.60049459e-02 -6.91012889e-02  1.54574774e-03\\n -5.87294716e-03  6.37075901e-02  5.20735383e-02  3.24092135e-02\\n -9.20073316e-03  9.50737484e-03  2.27222312e-02 -4.95447293e-02\\n -1.26380799e-02 -6.36036834e-03  3.31028365e-02  1.26462532e-02\\n  3.85006778e-02  1.66405551e-02  2.62069572e-02 -2.20137965e-02\\n -3.86393107e-02  3.65269482e-02 -1.09473160e-02  2.02695001e-02\\n  2.77196579e-02  1.22822402e-02  1.96687393e-02 -3.31296911e-03\\n -3.38984653e-02 -6.69368729e-02 -3.16665671e-03 -2.78926063e-02\\n  7.85840559e-04  1.63197555e-02  1.91560201e-03  9.04187933e-03\\n  5.75268380e-02 -3.31620201e-02  1.60978530e-02  1.41243767e-02\\n -5.69914952e-02  1.02174887e-02 -4.48415196e-03  6.16576858e-02\\n -1.46528929e-02 -5.51820584e-02 -1.48763438e-03  1.03818914e-02\\n -2.09951046e-04 -2.09202664e-03  2.32638642e-02  6.62182346e-02\\n -6.78743795e-02  8.94480478e-03 -6.24730773e-02  1.28837945e-02\\n -6.81573292e-03  1.36587443e-02  1.16839623e-02 -1.18746413e-02\\n -1.78302303e-02  7.14949425e-03  3.88108157e-02  1.89226754e-02\\n -3.98418084e-02  1.59134716e-02  8.45621899e-03  4.45958115e-02\\n -5.22310808e-02  2.47273892e-02 -5.18755279e-02  1.99188851e-02\\n  4.69030850e-02 -7.14730993e-02 -3.21113467e-02  3.82813811e-02\\n  2.68168350e-07  5.93405962e-02  3.92954350e-02  2.64191665e-02\\n  1.73668675e-02  3.13125253e-02  1.78583805e-02 -2.43673846e-02\\n  5.36333881e-02  1.90793257e-02  9.96651594e-03  5.30510023e-02\\n  3.45248021e-02  5.57746962e-02  1.27390912e-02 -7.02153295e-02\\n -7.31616383e-05 -3.42255943e-02  1.92031898e-02 -5.80277182e-02\\n -2.10885257e-02  1.85578018e-02  1.44656658e-01 -4.20768512e-03\\n -2.42478419e-02  3.78981978e-03  5.78712556e-04  8.73472393e-02\\n -2.96561178e-02  1.84307452e-02  7.92517327e-03  3.82916220e-02\\n  3.89025845e-02 -4.04831693e-02 -2.66096853e-02 -2.23242510e-02\\n  2.31559779e-02  1.17617408e-02  6.61267266e-02  2.25457661e-02\\n -2.61270422e-02  3.34604420e-02 -4.84327087e-03 -5.80298342e-03\\n -8.07889402e-02  5.07586077e-02 -5.40984645e-02 -1.29831564e-02\\n  4.79531996e-02  3.82897491e-03  1.63581688e-02  2.06450652e-02\\n -4.50505838e-02 -1.52518386e-02  1.51406657e-02  3.46578024e-02\\n  1.12523194e-02 -1.13092428e-02 -4.11512293e-02 -2.08387766e-02\\n -8.06291774e-03 -5.26997782e-02 -3.59890200e-02 -6.64745271e-02\\n -4.02356759e-02  5.49989333e-03 -1.46171600e-02 -1.70517787e-02\\n  2.36931769e-34  2.21954994e-02 -1.19425664e-02  3.37227844e-02\\n -1.53751532e-03  1.57661363e-02 -3.60492878e-02  5.03509752e-02\\n  1.93610713e-02 -1.88322552e-02 -6.13968261e-02 -4.85905893e-02]'},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'Working with text data During the pretraining stage, LLMs process text one word at a time. Training LLMs with millions to billions of parameters using a next-word prediction task yields models with impressive capabilities. These models can then be further finetuned to follow general instructions or perform specific target tasks. But before we can implement and train LLMs, we need to prepare the training dataset. This involves splitting text into individual word and subword tokens, which can then be encoded into vector representations for the LLM. You’ll also learn about advanced tokenization schemes like byte pair encoding, which is utilized in popular LLMs like GPT. Lastly, we’ll implement a sampling and data-loading strategy to produce the input-output pairs necessary for training LLMs. Understanding word embeddings Deep neural network models, including LLMs, cannot process raw text directly. Since text is categorical, it isn’t compatible with the mathematical operations used to implement and train neural networks. Therefore, we need a way to represent words as continuous-valued vectors.',\n",
       "  'sentence_chunk_size': 1106,\n",
       "  'sentence_chunk_word_count': 165,\n",
       "  'sentence_chunk_tokens': 276.5,\n",
       "  'embedding': '[ 1.63511951e-02 -1.55524863e-02 -2.18766741e-03  5.43568507e-02\\n -3.39320973e-02  4.14788574e-02 -2.30374318e-02  4.09217589e-02\\n -7.39090424e-03 -6.62907138e-02 -2.84433458e-02 -4.19414714e-02\\n -3.52572999e-03  1.11856451e-02  5.93854040e-02 -1.57060847e-02\\n  5.54968603e-02 -9.42043727e-04 -2.72246953e-02  1.27898268e-02\\n  7.05790427e-03  4.25155601e-03  1.11910570e-02  2.25631092e-02\\n -4.02374305e-02 -2.94780731e-02 -1.68481302e-02  1.40015520e-02\\n -2.49536913e-02 -2.24004295e-02  1.67937006e-03 -1.06485132e-02\\n  7.31268972e-02 -1.04591798e-03  1.82392807e-06 -2.81066839e-02\\n -2.22412683e-02 -1.20618884e-02 -3.91022041e-02 -1.23576578e-02\\n  2.34788936e-02 -2.14832295e-02 -9.36347060e-03  2.44332794e-02\\n -1.65553205e-02  2.90563493e-03  2.49862112e-02  4.52571250e-02\\n  4.42447737e-02  9.01053026e-02  3.56880203e-03 -7.10433871e-02\\n  5.32447435e-02 -4.07574093e-03  2.78013237e-02 -2.16805506e-02\\n  4.24929000e-02 -3.09472047e-02 -2.39768717e-02  5.88366687e-02\\n -1.93040241e-02  2.00150646e-02 -1.48808665e-03  3.56072076e-02\\n  1.38633410e-02  3.04065272e-02  1.19468858e-02 -4.27100398e-02\\n -3.70299146e-02  3.01359817e-02 -4.15056907e-02 -1.08570373e-03\\n  1.43141458e-02 -1.72111094e-02 -2.55746674e-02 -3.26445475e-02\\n -3.12974937e-02  2.28399038e-02  3.06913257e-02  8.58775303e-02\\n  3.73773254e-03 -1.43958051e-02  3.48351598e-02  9.29431058e-03\\n -4.00552861e-02  6.26467094e-02 -2.96265557e-02 -1.88408419e-02\\n  2.47329269e-02  3.68218520e-04  1.33029150e-03 -3.20970640e-02\\n  5.06182909e-02  6.30442351e-02  5.25223017e-02  3.31100896e-02\\n -3.60382907e-02 -2.30338629e-02  2.68285144e-02 -9.13287774e-02\\n  2.98794694e-02  3.61299142e-02 -2.15121359e-02  1.09617822e-02\\n -4.80163991e-02  2.55625546e-02 -1.73727572e-02  5.50580919e-02\\n -3.98507454e-02 -9.04871989e-03 -1.52391782e-02 -3.17535214e-02\\n -4.64776568e-02  7.82752410e-02 -8.25542863e-03 -2.24874988e-02\\n -2.16959659e-02  5.62487058e-02  3.07241566e-02  4.88496339e-03\\n  9.92218405e-03 -2.62695327e-02 -9.77684278e-03  1.73185281e-02\\n -7.98828155e-02 -4.51020338e-02 -1.73291694e-02  1.12204989e-02\\n -1.24659091e-02 -1.57609694e-02 -8.70072097e-03  2.74534207e-02\\n  1.83903461e-03 -3.45320962e-02  3.33172306e-02  7.17398301e-02\\n  1.69202092e-03 -2.61626914e-02 -8.91556963e-02 -9.29525821e-04\\n  1.06324051e-02 -1.91133078e-02  4.70875017e-02 -2.70282086e-02\\n  4.74906899e-02  1.59535874e-02 -4.02766988e-02  1.48551492e-03\\n  1.91147290e-02  4.84377854e-02 -7.45672286e-02  8.65174159e-02\\n -2.38198396e-02 -1.60230901e-02  2.75365636e-02  3.76215414e-03\\n  3.84432450e-02  5.28484806e-02  3.10792811e-02  3.69343869e-02\\n  2.02712584e-02  4.80018090e-03 -1.70704778e-02  2.99368333e-02\\n  2.40046643e-02 -1.82626825e-02 -5.34068942e-02 -1.71464519e-03\\n  5.52635454e-03  6.71254620e-02  3.30634639e-02  4.48150449e-02\\n -2.80419569e-02 -1.41392583e-02  3.10829189e-02  9.80980247e-02\\n  9.03172716e-02  4.83040176e-02  4.17280868e-02  1.59216544e-03\\n  3.61856781e-02  2.13570949e-02 -1.81028154e-02  2.04079244e-02\\n -2.59264698e-03 -9.55524202e-03  7.11821392e-03  2.27053389e-02\\n -2.96812318e-02 -5.24740368e-02 -4.04336341e-02 -2.69244760e-02\\n  1.33019276e-02 -6.02981001e-02  7.19443755e-03 -2.01492831e-02\\n -6.11678883e-02  8.25405866e-02 -1.22956624e-02  2.94426968e-03\\n  1.12521611e-02 -4.02695350e-02 -1.96644347e-02  7.11457878e-02\\n  3.39982174e-02 -2.18741726e-02 -3.45924646e-02 -2.57940982e-02\\n -4.27053031e-03  7.43649260e-04 -1.92529541e-02  1.74137428e-02\\n -2.49141473e-02 -7.29297698e-02 -6.84490800e-03  2.84770019e-02\\n -1.55046741e-02 -1.44436117e-03 -4.26719151e-02  2.71048211e-02\\n  2.60973396e-03 -2.59636883e-02 -1.91181041e-02  5.39291613e-02\\n -4.04345244e-03 -6.57223240e-02 -1.24870788e-03 -3.19420919e-03\\n -3.04272678e-03  4.61070985e-02  8.61288235e-03  5.96228894e-03\\n  5.17478548e-02 -3.26776691e-02 -2.58343685e-02 -2.38880757e-02\\n -5.83522022e-02  8.87758564e-03  5.87851815e-02 -1.28079010e-02\\n  5.39044924e-02  1.78955644e-02 -7.78976968e-03 -1.71179119e-02\\n -4.24442627e-02 -2.74827741e-02  2.33970582e-02 -6.28078431e-02\\n -1.05819572e-02  3.56724113e-02  2.84894444e-02 -1.68109443e-02\\n  2.89871376e-02 -1.41047249e-02 -3.76481027e-03  2.38240929e-03\\n  3.88493091e-02  1.01879835e-02 -3.34126987e-02 -6.61820639e-03\\n  5.45318313e-02 -3.80975020e-04  2.27731001e-02 -3.92473079e-02\\n  1.54210646e-02 -3.40822972e-02 -1.48237143e-02 -5.34795634e-02\\n  9.92769189e-03 -3.04358336e-03  2.69948095e-02 -1.38413766e-02\\n -4.86506820e-02 -1.33306684e-03  6.46071555e-03 -4.91701551e-02\\n  4.41750847e-02 -3.62554751e-02 -2.61567999e-02  9.03382227e-02\\n  1.32909110e-02  1.98726226e-02 -2.49289703e-02 -2.10237335e-02\\n -3.21908407e-02  5.97531982e-02 -1.08426008e-02 -3.41056623e-02\\n -6.36083037e-02  3.55251096e-02 -2.27669138e-03  2.77605485e-02\\n  1.04379666e-03 -6.20219074e-02  2.40997504e-02 -2.97796763e-02\\n  3.90041731e-02  4.10112552e-02  2.99124122e-02  7.90447593e-02\\n  8.94218963e-03  2.93416064e-03 -5.25021926e-02  1.52004929e-02\\n -2.19360460e-02  2.43137330e-02 -5.95092066e-02 -1.01046981e-02\\n -7.39287771e-03  6.98113516e-02 -7.98514951e-03 -1.66220893e-03\\n -3.74154933e-02  1.03065530e-02  4.07191627e-02 -1.05542755e-02\\n -1.87012441e-02  2.34215185e-02 -6.35133237e-02 -2.01467965e-02\\n  1.40343308e-02 -2.93551013e-02  2.52808966e-02 -4.25749384e-02\\n -1.29937278e-02 -4.09845561e-02 -2.16892976e-02 -3.47938836e-02\\n  1.65856574e-02 -5.69180306e-03 -4.41314699e-03 -3.93935405e-02\\n -9.37719177e-03 -6.53081900e-03  2.66851131e-02 -3.66097912e-02\\n -2.62797363e-02  2.58669499e-02  1.44184865e-02 -2.78003607e-02\\n -3.94428335e-02  2.87725460e-02 -4.07047048e-02 -1.78296361e-02\\n -1.19586168e-02  1.17865447e-02  1.87503658e-02  7.92418793e-03\\n -1.95655078e-02 -2.49457005e-02  2.44893767e-02  2.88828916e-04\\n -1.92874949e-02  1.72961839e-02 -6.82821199e-02  2.79633570e-02\\n  6.62131188e-03  2.35301722e-02  9.21788905e-03 -3.67368245e-03\\n  1.53246627e-03  3.62927318e-02 -6.13380969e-03 -2.61552371e-02\\n -3.82959954e-02  1.92893967e-02  5.27543575e-02 -3.94062474e-02\\n -4.57156741e-04  1.87012833e-02  2.31435113e-02 -3.87032293e-02\\n  3.47670205e-02  9.67947692e-02  3.64298113e-02 -5.84858842e-02\\n -8.06719996e-04 -5.31109795e-03 -2.97918823e-02  2.52667069e-02\\n  2.44729333e-02 -8.27521607e-02 -2.32829638e-02  3.16457171e-03\\n -2.19497979e-02  5.00744209e-03 -1.59763172e-02 -2.44961269e-02\\n -2.79164296e-02  5.90431737e-03  2.75366660e-02  8.93073436e-03\\n -2.87442263e-02  8.25343095e-03 -5.01308143e-02  4.85975668e-02\\n  4.81674299e-02  3.54935881e-03  5.15862927e-03 -1.03854407e-02\\n  4.96846214e-02 -3.42394374e-02 -7.92373437e-03 -2.36722808e-02\\n -4.98344153e-02  6.70079887e-02  6.97619617e-02  3.77970897e-02\\n -4.98507209e-02 -3.50951403e-02  4.43558618e-02 -3.08034588e-02\\n  1.27653973e-02 -5.65296113e-02  3.59316766e-02 -3.68798822e-02\\n -1.89196039e-03  3.42279635e-02  2.69650854e-02 -2.17814818e-02\\n -4.31038998e-02 -2.81513687e-02  9.05075669e-02 -5.13950102e-02\\n -5.16170189e-02 -4.77258228e-02 -1.57838091e-02  9.79283825e-03\\n  5.87925464e-02  4.14726287e-02 -2.74262894e-02 -4.22169566e-02\\n -3.26335505e-02  1.80526767e-02 -1.44982515e-02 -4.36326191e-02\\n -1.19147599e-02 -4.54263017e-03  3.44334431e-02 -3.90323438e-02\\n -2.94011664e-02  3.31924148e-02 -9.82536469e-03 -5.37191750e-03\\n  5.46375476e-03 -2.45037619e-02 -3.16929109e-02 -7.58188814e-02\\n  1.90201961e-02  1.00346515e-02  3.08747012e-02 -4.36897166e-02\\n -3.31290439e-02  7.76863024e-02 -3.67681272e-02 -2.94680595e-02\\n -3.49879973e-02 -7.22047016e-02 -4.81670313e-02 -7.60813896e-03\\n -4.47298138e-04  7.22698960e-03  2.38807611e-02  1.12085501e-02\\n -8.72284397e-02  7.11344630e-02  4.13485914e-02 -3.56612764e-02\\n  9.40969586e-03  6.61708117e-02  6.08158968e-02  3.11023183e-02\\n  1.44761335e-02 -1.74952485e-02  1.23308515e-02 -1.65773369e-02\\n  2.47371942e-03 -4.29624179e-03 -3.14378180e-02 -1.78813729e-02\\n -8.00088607e-03  3.69484834e-02  3.13100554e-02  2.42660996e-02\\n -1.17612481e-02  4.27098274e-02  4.69003426e-04  2.89223455e-02\\n -4.13771830e-02  1.38667542e-02  1.45926804e-03 -7.78769236e-03\\n -2.76252851e-02 -2.25918200e-02 -1.07057672e-02  6.79022819e-03\\n  1.63262580e-02  5.35084009e-02  6.48505986e-03 -7.50921061e-03\\n -2.83086533e-03  1.01859085e-02 -1.47484951e-02  1.37639809e-02\\n -3.94891091e-02  7.11726537e-03 -2.89619882e-02 -8.67986586e-04\\n  4.00749594e-03  4.07708995e-02 -3.86711955e-02  4.96584326e-02\\n -1.53124537e-02 -1.24776866e-02 -1.83645245e-02  4.69816104e-02\\n  3.37483026e-02 -1.24752093e-02 -3.17336209e-02 -2.81507447e-02\\n -4.62149596e-03  3.90801625e-03 -1.73920358e-03 -6.85727820e-02\\n  5.76589890e-02  7.82017037e-02  8.51792842e-03 -4.20856476e-02\\n -5.84190115e-02  4.43115383e-02  4.40331884e-02 -4.53491491e-04\\n  5.25644049e-03 -8.41207150e-03 -1.15495697e-02 -3.85778444e-03\\n  5.73693439e-02  3.81400734e-02 -1.24708079e-02 -1.63983274e-03\\n  6.32492034e-03 -4.42437530e-02  8.87332633e-02  5.79048647e-03\\n -1.81080308e-02 -2.97203772e-02  2.47887466e-02  5.37676774e-02\\n  1.03427609e-03  1.43988980e-02  8.32423801e-04  2.62153391e-02\\n  2.83268802e-02 -6.42466471e-02  1.98788941e-02 -6.27520321e-33\\n -9.17335693e-03 -4.69120629e-02  5.68619706e-02  2.14444064e-02\\n -3.43012586e-02 -1.02665201e-02  2.74489969e-02 -3.67953256e-02\\n  9.27081238e-03 -1.31581435e-02 -3.00665349e-02  7.66442204e-03\\n  2.24999934e-02  1.62679181e-02 -2.73875222e-02 -7.10410476e-02\\n  5.27104875e-03 -6.25812933e-02  3.37985903e-02 -2.24417970e-02\\n -1.72749367e-02  7.92112872e-02  6.09179810e-02 -4.73254807e-02\\n  4.90622520e-02 -9.68923979e-03  2.60581309e-03  4.71497932e-03\\n  6.07135333e-03  6.95114508e-02 -5.89408427e-02  3.41407163e-03\\n  3.37714516e-02 -2.93451268e-02 -3.95647995e-02  8.44333172e-02\\n -6.45871311e-02 -5.39693832e-02  2.41190847e-02  4.00606319e-02\\n -7.05643147e-02 -6.64715543e-02  7.85979554e-02  2.29667779e-03\\n -5.02163693e-02  2.05758866e-02  5.69161086e-04 -2.52765659e-02\\n -2.72049885e-02 -3.35935615e-02 -2.29335446e-02 -2.87774397e-04\\n  3.98008339e-02  4.24098149e-02  8.91665369e-02 -5.93944937e-02\\n -5.88034885e-03  4.83256392e-02 -6.28007203e-02 -1.64197888e-02\\n -4.65694908e-03  5.99270351e-02 -1.42608397e-03  7.72526721e-03\\n -2.47655041e-03  5.43369986e-02  1.81289352e-02 -8.98236036e-02\\n -3.23124751e-02  2.33072359e-02  5.86422198e-02  2.99606025e-02\\n  1.76455155e-02  9.50919837e-03  3.73246111e-02 -4.33546714e-02\\n -4.68777344e-02  1.48031013e-02  5.44135459e-03 -1.77125297e-02\\n  3.51540446e-02  1.59948471e-03 -1.23956231e-02 -1.87632646e-02\\n -1.93417650e-02 -4.09464315e-02 -3.50157954e-02 -3.94718423e-02\\n  1.25279427e-02  9.66464262e-03 -1.64195779e-03  2.23471522e-02\\n  1.52190700e-02 -5.48856184e-02 -1.60426553e-02 -7.79437646e-03\\n -2.35815812e-03  3.52090364e-03  3.74868922e-02  2.16477849e-02\\n -4.73349392e-02 -6.49885163e-02 -3.92968161e-03 -1.87588818e-02\\n  2.37392485e-02  6.47353753e-03  4.24298411e-03  4.71827537e-02\\n -4.34028059e-02 -1.59160346e-02 -8.38718284e-03 -2.34443676e-02\\n -1.39255626e-02  3.10436878e-02 -6.64099446e-03 -5.77236293e-03\\n  1.18929362e-02  5.04244901e-02  1.34939328e-02 -4.66790497e-02\\n -1.59260165e-02 -1.47604542e-02  1.67550612e-02  6.30197451e-02\\n -5.02905883e-02 -6.51692599e-03 -2.65407562e-02  1.78943761e-02\\n  4.75971103e-02 -8.19019154e-02 -4.70000115e-04  4.92477156e-02\\n  2.55872834e-07  5.88202067e-02  7.06150681e-02  3.86050865e-02\\n  2.12072283e-02  3.28082666e-02  1.65008623e-02 -3.04819793e-02\\n  5.04091531e-02  1.39780715e-03 -5.71361892e-02  4.14269790e-02\\n  5.10622449e-02  6.49696030e-03  1.82710811e-02 -8.40289593e-02\\n  1.06040752e-02 -3.70415151e-02  9.67669860e-03 -4.73360084e-02\\n  1.75052751e-02  5.50358780e-02  1.22013457e-01 -7.73218111e-04\\n -1.76796187e-02  1.77241967e-03 -4.02665660e-02  5.05626388e-02\\n -2.80411486e-02  3.85123938e-02 -1.29613960e-02  2.62020119e-02\\n  1.61277540e-02  1.27629079e-02 -3.27195376e-02  1.99841568e-03\\n  4.60215919e-02  1.26293674e-02  8.52541402e-02 -1.77853163e-02\\n -7.54295737e-02  8.38525034e-03 -2.06233170e-02 -8.97630118e-03\\n -1.25160590e-02  7.56428391e-02 -2.49536615e-02 -8.01633298e-03\\n -6.12895750e-02  7.13458359e-02 -3.29900905e-02 -2.87813926e-03\\n -1.40965423e-02 -2.25160366e-05  6.36765361e-03  2.70622279e-02\\n -1.03035932e-02  1.99079118e-03 -1.00838117e-01 -2.90462654e-03\\n -9.20779351e-03 -5.95757626e-02 -2.37605851e-02 -5.47322929e-02\\n  1.08349463e-02  6.98585659e-02 -3.28164883e-02 -2.35444829e-02\\n  2.52442279e-34  1.67609677e-02 -1.63336191e-02 -7.03738350e-03\\n  9.62183177e-02 -2.12502517e-02 -2.38219346e-03  7.67338183e-03\\n  2.87511554e-02 -5.11890603e-03 -5.05704433e-02 -3.53986323e-02]'},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'The concept of converting data into a vector format is often referred to as embedding. Using a specific neural network layer or another pretrained neural network model, we can embed different data types—for example, video, audio, and text. However, it’s important to note that different data formats require distinct embedding models. For example, an embedding model designed for text would not be',\n",
       "  'sentence_chunk_size': 397,\n",
       "  'sentence_chunk_word_count': 62,\n",
       "  'sentence_chunk_tokens': 99.25,\n",
       "  'embedding': '[-5.96022233e-02  5.09771286e-04 -2.44711414e-02  4.08769213e-02\\n  2.62873955e-02  2.41267011e-02  3.57510671e-02  3.67975682e-02\\n -5.10192104e-02 -3.24317366e-02  8.67594965e-03  1.52290013e-04\\n  2.82290056e-02  2.27917992e-02  6.42060712e-02 -7.89849982e-02\\n  2.30092220e-02  2.62450688e-02 -1.55841568e-02  2.47783642e-02\\n  1.71688274e-02 -1.19640091e-02  7.18430057e-02  2.81778965e-02\\n -3.27413902e-02  1.25826998e-02 -1.99812353e-02 -6.92945207e-03\\n -1.40866719e-03  1.82873178e-02 -3.08981910e-03 -3.78216757e-03\\n  1.14368759e-01 -6.84212614e-03  1.84714315e-06  4.51875404e-02\\n  5.58735745e-04  6.33265357e-03 -5.96399931e-03  2.02601729e-03\\n  1.08607812e-02 -5.15982583e-02  2.43089441e-03  1.45207364e-02\\n -1.57030076e-02 -7.91870952e-02  4.88243923e-02  4.75875996e-02\\n  4.52034883e-02  5.25883585e-02 -2.84466613e-02 -7.19170794e-02\\n  4.16665934e-02 -2.66342666e-02 -2.20401282e-03  7.41468444e-02\\n  6.16253074e-03  1.51239075e-02 -1.81858186e-02  7.64665008e-02\\n -6.63191006e-02  3.47516946e-02 -4.90631908e-04  3.85559611e-02\\n  5.81016168e-02  3.48856077e-02  2.16005221e-02  5.61583787e-03\\n  1.54033853e-02  4.33655344e-02 -5.17331995e-02 -9.10118409e-03\\n  4.62316209e-03 -1.96004473e-02  7.73260975e-03 -1.87729523e-02\\n -4.28419448e-02 -6.36409316e-03  3.84742804e-02  4.58354913e-02\\n -4.91666831e-02  2.66797170e-02  2.95504518e-02 -3.61101516e-02\\n -3.69233862e-02 -4.15435359e-02 -1.72647648e-02 -1.96534917e-02\\n -5.32188863e-02 -1.98672321e-02  2.01217663e-02 -9.77953710e-03\\n  2.56594531e-02 -1.96906943e-02  6.50720075e-02 -2.22691279e-02\\n -4.47865389e-02 -8.21262449e-02  8.82953405e-03 -7.78435692e-02\\n  9.76340622e-02  9.87467635e-03 -3.08508184e-02  1.98372304e-02\\n -4.15251553e-02  2.83132885e-02 -7.00609162e-02  4.09814455e-02\\n -2.68174931e-02  1.61608681e-02 -3.69497277e-02 -1.81658566e-02\\n -4.56773788e-02  2.28484236e-02 -1.67612880e-02 -4.47077602e-02\\n -2.45800819e-02  4.69584651e-02 -1.69907380e-02  1.93350222e-02\\n -4.25201878e-02  1.07285269e-02  8.56245402e-04  3.67671549e-02\\n -2.39960942e-03  1.43887121e-02 -2.65592877e-02  3.64137404e-02\\n  3.84483375e-02  1.49987545e-02  1.15828803e-02  2.25875974e-02\\n  2.73188185e-02 -1.90622956e-02  2.05991627e-03 -1.26062762e-02\\n  3.53198033e-03  2.22469401e-03 -3.88377197e-02 -7.56701967e-03\\n  3.51291295e-04  2.74576433e-02  7.37932771e-02 -4.63310368e-02\\n  7.14062750e-02 -1.53389443e-02 -1.13933887e-02 -2.33088452e-02\\n -2.60668248e-02  2.48826221e-02 -8.57774466e-02  9.17363539e-02\\n -2.71287989e-02 -6.21732697e-03  6.17244840e-02 -4.41336213e-03\\n  4.92716953e-02  2.15824810e-03  1.23570170e-02 -2.85945199e-02\\n  2.15271264e-02 -7.10805506e-02  1.46585479e-02  1.46449450e-02\\n  4.43945192e-02 -4.25993875e-02 -9.18296352e-02  2.70682015e-03\\n  5.03714243e-03  5.24068587e-02  3.50894891e-02  3.55441496e-02\\n -5.34838550e-02 -6.76645637e-02  4.08039801e-02  7.63719156e-02\\n  6.86252713e-02  4.62928712e-02  4.57789451e-02  3.91950877e-03\\n  2.99096871e-02  1.03407735e-02 -7.77306827e-03  4.05781902e-02\\n -8.42793263e-04  1.20283887e-02  2.31197360e-03  2.36826167e-02\\n -6.25591949e-02 -5.13465665e-02  7.80676864e-03 -5.62613048e-02\\n  6.16206005e-02 -4.31315079e-02 -4.20250185e-03  1.79606490e-02\\n -1.27610788e-02 -9.46140848e-03 -5.69116995e-02 -2.67361701e-02\\n -3.19694309e-03 -2.52213515e-02  9.40894615e-03  5.09455940e-03\\n -9.31583997e-03  2.31103622e-04  3.10695404e-03 -6.20328188e-02\\n -6.55334210e-04  1.04620913e-02  2.57324297e-02 -5.86779509e-03\\n -9.59432311e-03 -6.15130104e-02 -1.45287055e-03  9.81179625e-03\\n  1.26102651e-02  2.98368442e-03 -7.30311424e-02  4.45106626e-03\\n  2.93078599e-03 -4.08741497e-02 -1.32128093e-02  4.01966693e-03\\n -1.71860727e-03 -2.17357054e-02  9.58126690e-03  3.26406420e-03\\n -2.35880651e-02  2.17129644e-02  5.17323846e-04  1.58288945e-02\\n  3.75963077e-02 -4.61650454e-02 -5.25093600e-02  1.64003707e-02\\n -3.51768397e-02  6.03012857e-04 -1.84123740e-02 -3.79162915e-02\\n  5.50268739e-02 -9.53734852e-03 -4.07735212e-03 -1.21576898e-02\\n -1.59214786e-03  2.36312137e-03  3.01473849e-02 -2.23283879e-02\\n -2.28688642e-02  1.52884834e-02  4.57017422e-02 -1.15137629e-03\\n -2.48920359e-03 -4.84253652e-03  3.46059515e-03  2.66056433e-02\\n  2.40484774e-02  1.19312620e-02 -2.19449885e-02 -3.94772068e-02\\n  6.19846806e-02 -2.69834977e-02  9.28929541e-03  1.93698215e-03\\n -6.14173859e-02 -7.64328102e-03  8.59555230e-03 -5.67836016e-02\\n -6.17241208e-03  2.96729207e-02  2.11142134e-02 -2.82110833e-02\\n -1.35269295e-02  1.36801088e-02 -9.36988555e-03 -6.52105212e-02\\n  2.15470847e-02 -1.53734116e-02  2.93837786e-02  4.87836562e-02\\n -1.30962464e-03  2.28390433e-02 -3.39154750e-02 -4.58165742e-02\\n -3.41582508e-03  7.09371343e-02 -4.86443713e-02  1.04372110e-03\\n -2.28463169e-02 -3.12834717e-02 -1.08049409e-02  1.30896149e-02\\n  3.15488651e-02 -3.51859890e-02 -3.33280489e-03 -1.46834822e-02\\n  3.24372128e-02  6.04132675e-02  2.91597797e-03  4.76310737e-02\\n  1.42396009e-02  2.05652583e-02 -4.12112549e-02  5.87799354e-03\\n -1.99179407e-02  7.50303045e-02 -2.16764901e-02  4.36276104e-03\\n -1.47752613e-02  1.33145619e-02  5.96213061e-03  1.45165036e-02\\n -4.28639315e-02  3.41125801e-02  1.07180269e-03  2.12377533e-02\\n -4.57727239e-02 -1.44589366e-02 -5.82148414e-03 -1.36766396e-02\\n  2.14267001e-02 -2.19192319e-02  2.27384153e-03 -1.59382354e-02\\n -9.69527941e-03  2.13467926e-02 -3.55151407e-02 -2.18653376e-03\\n -2.40578447e-02  2.81752050e-02  7.37108942e-03 -3.25333215e-02\\n  1.44073442e-02 -9.96494573e-03  9.99608822e-03  4.42756992e-03\\n -4.01066877e-02  2.29007993e-02  4.53842394e-02 -6.88631833e-02\\n -5.12524322e-02  2.84835871e-04 -7.46976398e-03 -5.08625340e-03\\n  4.54289168e-02  3.06808874e-02 -1.32035995e-02 -7.37797329e-03\\n -3.77534963e-02 -2.00057612e-03 -1.59901101e-02  3.07553504e-02\\n -2.01664902e-02 -1.58080179e-02 -3.45471948e-02  1.03695370e-01\\n -1.82749536e-02  2.61903349e-02 -5.22730052e-02  2.23766789e-02\\n  1.23814242e-02  1.78861152e-02 -3.69879045e-02  2.22190619e-02\\n -8.56622458e-02  2.41371412e-02  5.09588327e-03  4.05769832e-02\\n -7.25134388e-02  2.05401424e-02 -7.10732117e-03 -5.24116568e-02\\n  2.42906734e-02  3.79350968e-02  2.20294949e-02 -2.10893136e-02\\n -1.48159321e-02  4.47131554e-03 -2.93688457e-02  6.49637822e-03\\n  4.65267971e-02 -6.04128987e-02  1.94466617e-02 -1.39850155e-02\\n -2.14752052e-02  3.30716483e-02  9.72552784e-03  4.70891334e-02\\n -1.02315797e-02  5.25039583e-02  2.05367990e-02 -1.13595268e-02\\n -7.94334859e-02 -2.50964053e-02 -1.02050202e-02  1.36461519e-02\\n  1.83417983e-02 -4.84383218e-02 -3.09736864e-03 -3.36028636e-03\\n  4.44142334e-02 -2.73099206e-02 -3.80560346e-02 -6.34997562e-02\\n -3.60123292e-02  7.77499005e-02  5.66886179e-02  6.39366433e-02\\n -5.69984205e-02 -5.95159307e-02 -7.42636099e-02 -1.91846211e-03\\n -1.19937444e-02 -9.36955307e-03  7.34576583e-02  2.96439156e-02\\n  1.75924283e-02  3.98236550e-02  3.30631062e-02  5.79937622e-02\\n -3.84913571e-02 -6.16273731e-02  8.99510458e-02 -3.43552493e-02\\n -1.36198495e-02 -8.10868815e-02 -4.80980463e-02  6.73315814e-03\\n  4.29205559e-02  3.32182571e-02 -3.08177434e-02 -2.11935993e-02\\n -3.38569544e-02  5.06035164e-02 -1.34375077e-02 -5.11915982e-02\\n  1.24160247e-02  4.27491823e-03 -1.86525099e-02 -5.12581840e-02\\n -5.51787298e-03  7.19136419e-03  5.22189355e-03  8.72889441e-03\\n -4.17677015e-02  2.42907051e-02 -2.84366347e-02 -8.74821618e-02\\n  3.44655593e-03  5.98887466e-02  1.98426880e-02 -5.15542133e-03\\n -1.03104440e-02  3.56667265e-02 -2.23215502e-02 -1.28469160e-02\\n -3.85664441e-02 -3.32990177e-02 -1.54788541e-02 -3.35231726e-03\\n  2.44067833e-02  5.56192081e-03  1.19236959e-02  2.17508525e-02\\n -2.76078600e-02  1.01794749e-01  5.33636846e-02 -2.96336841e-02\\n -1.40145747e-03  6.71780407e-02 -4.18224782e-02 -2.08442546e-02\\n -1.92359705e-02 -5.41413669e-03  1.73776764e-02 -2.05672197e-02\\n -7.06964359e-02  7.65328715e-03  2.58150697e-02  8.07325632e-05\\n -5.47582507e-02  3.35893892e-02  2.41477806e-02 -1.37256440e-02\\n -1.33257164e-02 -2.66064610e-02 -1.63688771e-02  2.48492211e-02\\n -3.33542041e-02 -2.17114892e-02  1.61412209e-02  4.60960567e-02\\n -1.51281888e-02  1.59966424e-02  3.66176963e-02 -1.13609980e-03\\n -7.64908642e-02  1.70266349e-02  3.29810530e-02 -1.06651448e-02\\n -3.77881750e-02  4.11459059e-02 -6.14911644e-03  2.56533213e-02\\n -4.39587496e-02  1.32942880e-02 -4.85637523e-02  3.62395833e-04\\n  9.72953532e-03  5.81519715e-02 -3.65501568e-02  6.38425425e-02\\n -1.45446947e-02  1.39841922e-02 -2.87813302e-02  2.06055548e-02\\n  1.08541511e-02 -4.20603827e-02 -1.18730031e-02 -2.22098175e-03\\n -2.46034330e-03 -1.79226436e-02  1.63919944e-02 -3.93298501e-03\\n  8.93933512e-03  6.89867586e-02 -1.43179037e-02 -3.05458996e-02\\n -5.49295023e-02  4.73677218e-02 -1.67530105e-02  9.92345996e-03\\n -3.88386101e-02 -1.76348016e-02  1.66526332e-03  4.53247465e-02\\n  1.01129025e-01  3.21195275e-02 -2.22679339e-02 -3.82126458e-02\\n  4.09882562e-03 -5.89148104e-02  2.33331621e-02  2.85108183e-02\\n -1.25909345e-02  1.80052761e-02 -1.73841156e-02  1.18490877e-02\\n  1.55776888e-02 -6.46787416e-03  1.24255568e-02  9.55831259e-03\\n -3.99148650e-02 -5.75959049e-02  1.26535241e-02 -5.76786462e-33\\n  6.70295255e-03  2.73998477e-03  4.35174163e-03 -1.39456745e-02\\n -1.14409700e-02  2.42812466e-02  4.30815108e-02 -1.27211837e-02\\n  1.17945541e-02 -1.01078758e-02 -3.14880610e-02 -2.64587486e-03\\n -6.81160903e-03  1.88029390e-02  1.27977096e-02 -1.11816172e-02\\n  3.91069390e-02 -7.71756768e-02  1.47688501e-02 -6.09761523e-03\\n  4.84421570e-03  8.88202060e-03  1.31173879e-02 -8.89560804e-02\\n -5.30381547e-03  1.55785801e-02 -2.63783224e-02 -1.55211920e-02\\n  2.87975613e-02  7.72655383e-02 -3.61558422e-02  2.34595221e-03\\n  5.39193116e-03 -5.03602140e-02 -3.36219445e-02  1.28848881e-01\\n -8.78337622e-02 -1.64838545e-02 -1.12322850e-04  2.91398317e-02\\n -8.75460654e-02 -2.24922728e-02  4.71182801e-02 -4.13548946e-02\\n  2.34607644e-02 -5.68517577e-03  2.02051792e-02 -3.95329073e-02\\n  2.96235923e-03  9.42341518e-03 -1.90897305e-02  1.32986195e-02\\n  2.22545140e-03  2.58302521e-02  1.37881324e-01  2.57190410e-02\\n  1.31986020e-02 -6.55870978e-03 -1.06060408e-01  3.54111344e-02\\n -3.22643071e-02  1.83469299e-02 -8.52944050e-03 -1.86254755e-02\\n -5.84466802e-03  2.11269334e-02  1.39507772e-02 -7.89396688e-02\\n -2.43613254e-02  1.63457356e-02  2.56541334e-02  7.96329379e-02\\n  4.76953164e-02  4.63779783e-03  4.61661257e-02 -5.03825545e-02\\n  4.88441763e-03  4.73211259e-02 -1.33695472e-02 -7.84036219e-02\\n -9.69621632e-03  7.81897362e-03  5.10585681e-03 -3.10125984e-02\\n -8.99029057e-03 -7.36665577e-02 -1.20474603e-02 -4.50810492e-02\\n  3.49570513e-02  1.14651853e-02  2.69252118e-02  2.84310449e-02\\n  5.03895730e-02 -5.67171387e-02  2.10250858e-02  3.64747643e-02\\n -9.18882899e-03  3.52225564e-02  1.82827320e-02 -2.16916203e-02\\n  6.43138192e-04 -3.80683616e-02  7.03763515e-02 -3.22699174e-02\\n  3.13655026e-02  1.21667711e-02  5.49512729e-02  5.78559749e-02\\n -3.10609601e-02 -2.29167193e-02  3.34311761e-02 -4.68346439e-02\\n -4.74739500e-04  1.83476713e-02  6.46460289e-03  2.05453001e-02\\n  3.18041723e-03  7.51713198e-03 -7.19769252e-03  1.94259286e-02\\n -3.54948677e-02 -5.76926842e-02 -1.06763262e-02  4.04332504e-02\\n -1.33304382e-02 -2.39431988e-02 -3.94578837e-02  2.88644619e-02\\n  6.22342341e-02 -3.79173383e-02 -8.76439549e-03  3.47347371e-02\\n  2.38266267e-07  1.13064991e-02  3.94156538e-02  1.37039330e-02\\n -2.37215627e-02  3.65339071e-02  2.65285652e-02 -4.17490629e-03\\n  8.20477679e-02 -3.14279692e-03  9.01334360e-03  3.28012779e-02\\n -9.16452892e-03  1.08406600e-02  5.38291335e-02 -3.34649459e-02\\n -1.42533137e-02 -5.14594540e-02  2.38393173e-02 -9.19121038e-03\\n  2.09368789e-03  4.97910194e-02  1.06928818e-01 -9.30016395e-03\\n -2.63601402e-03  3.18703987e-02  5.84710669e-03  3.36233601e-02\\n -2.11704988e-03  4.30936776e-02 -3.08446139e-02  5.94351441e-03\\n  4.86136414e-02  1.42729962e-02  2.18907073e-02 -2.48895697e-02\\n  2.06867978e-02  4.60398868e-02  5.45645356e-02 -7.45381927e-03\\n  3.00905574e-02 -4.58282940e-02  2.41681822e-02 -4.74811494e-02\\n -3.43343057e-02  3.06483507e-02 -2.82978471e-02 -2.29248274e-02\\n -3.79184596e-02 -5.53995138e-04  3.36008929e-02  4.27469872e-02\\n -7.40322378e-03  3.71795334e-03 -5.64326625e-03  4.19017822e-02\\n -1.22334892e-02 -7.11555709e-04 -4.36238945e-02 -3.28512164e-03\\n  2.42597684e-02 -3.48229595e-02  8.93099513e-03 -7.38427266e-02\\n  5.73495636e-03  1.07752748e-01  9.08093527e-03 -2.26846430e-02\\n  2.27930765e-34 -3.71818095e-02  8.84473324e-03  7.39708077e-03\\n  2.43240930e-02 -3.18177342e-02 -3.17119360e-02  5.11775315e-02\\n  6.93846075e-03 -2.06219014e-02 -5.35486899e-02 -4.63284813e-02]'},\n",
       " {'page_number': 6,\n",
       "  'sentence_chunk': 'suitable for embedding audio or video data. At its core, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space— the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process. While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for retrieval-augmented generation. Retrieval-augmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text. Since our goal is to train GPT- like LLMs, which learn to generate text one word at a time, we will focus on word embeddings. Several algorithms and frameworks have been developed to generate word embeddings. One of the earlier and most popular examples is the Word2Vec approach. Word2Vec trained neural network architecture to generate word embeddings by predicting the context of a word given the target word or vice versa. The main idea behind Word2Vec is that words that appear in similar contexts tend to have similar meanings.',\n",
       "  'sentence_chunk_size': 1250,\n",
       "  'sentence_chunk_word_count': 193,\n",
       "  'sentence_chunk_tokens': 312.5,\n",
       "  'embedding': '[ 6.94329431e-03 -2.04977933e-02  6.12922711e-03  3.54862511e-02\\n -6.05280288e-02 -2.52609700e-03 -1.71178933e-02  5.06380014e-02\\n -1.23524386e-02 -3.37427780e-02  2.93259528e-02 -2.66027153e-02\\n  9.05382074e-03  1.20676379e-03  6.52135536e-02 -3.39587890e-02\\n  6.25995696e-02  2.93925628e-02 -3.80257741e-02  1.36064626e-02\\n -1.19046569e-02  9.69342235e-03  1.56397503e-02  2.26156972e-02\\n -5.37239946e-02  7.42422882e-03 -1.33053977e-02 -5.06979274e-03\\n  1.60168130e-02 -2.28415206e-02 -2.13767178e-02 -2.10229643e-02\\n  6.00458309e-02  2.05196552e-02  1.89677291e-06 -9.26441047e-03\\n -3.78864333e-02 -4.86151641e-03  2.31929477e-02 -2.53462344e-02\\n  3.91866677e-02 -2.13441793e-02 -1.83736105e-02  3.62321362e-02\\n -4.99636978e-02 -2.68708821e-02  7.86346495e-02  6.25244752e-02\\n  4.49201502e-02  7.69338533e-02 -3.71291414e-02 -8.21200982e-02\\n  2.62434483e-02 -2.79943235e-02  6.54769689e-02  1.15373479e-02\\n  3.08208428e-02 -2.03221161e-02  1.73625269e-03  6.84510767e-02\\n -2.11646780e-02  3.69915776e-02 -4.14172746e-03 -9.47251543e-03\\n  7.98553303e-02  6.27954602e-02 -4.90675531e-02 -3.71910892e-02\\n  5.96480153e-04  4.14588042e-02  3.18034901e-03 -9.47685912e-03\\n  4.42411378e-03  3.61882057e-03 -1.33345427e-03  4.32614498e-02\\n -4.81450744e-02 -5.75472089e-03  2.37918999e-02  6.60955310e-02\\n -3.79542727e-03 -4.15568333e-03  2.23354865e-02 -3.58744226e-02\\n  1.04842521e-03  9.16381739e-03  3.12829837e-02 -2.93645859e-02\\n -2.21517403e-02 -2.97638611e-03 -2.57631317e-02 -2.20195185e-02\\n  1.12623656e-02  3.14035378e-02  6.84240386e-02  3.56211513e-02\\n -5.54337651e-02 -1.87222529e-02  2.93695573e-02 -6.14232458e-02\\n  6.61762133e-02  2.11157221e-02 -2.11086515e-02  4.11488526e-02\\n -1.02804013e-01  4.77491599e-03 -3.59458551e-02 -6.93282485e-03\\n -3.85274403e-02 -9.26851388e-03 -4.01395932e-02 -4.25207876e-02\\n -6.28225878e-02  5.80101125e-02  7.43965572e-03 -7.00279847e-02\\n -6.05515540e-02  4.88556437e-02 -1.55975269e-02  2.11793762e-02\\n -2.09926292e-02  1.49774486e-02 -2.83054281e-02  2.86776870e-02\\n -2.73072477e-02 -1.96104161e-02 -2.42672265e-02  1.57849714e-02\\n -7.97574408e-03 -1.30583169e-02  5.76388375e-05  2.60417666e-02\\n  4.43204641e-02  1.12856487e-02 -1.26765342e-02  6.54214472e-02\\n -1.50007836e-03 -3.94748710e-02 -6.62306026e-02 -3.65738682e-02\\n  9.68834199e-03 -1.38550494e-02  2.38217600e-02 -6.08762763e-02\\n  4.38844562e-02 -9.68645327e-03 -4.26435657e-02 -9.65097640e-03\\n  2.53902823e-02  3.64975035e-02 -7.72892758e-02  9.19636264e-02\\n -2.09613554e-02 -4.64255735e-03  6.60428032e-02  2.32243408e-02\\n  7.40707666e-02  2.44314335e-02  3.68360579e-02 -9.00899433e-03\\n  3.28739882e-02 -3.12853954e-04 -4.06366307e-03  2.75901761e-02\\n  2.34191455e-02 -1.15445387e-02 -4.01302688e-02 -2.66962405e-02\\n  2.29234658e-02  6.18225783e-02 -2.63753813e-03  7.90720135e-02\\n -4.65763174e-02 -3.92522896e-03  5.21387644e-02  1.09486602e-01\\n  6.29462898e-02  4.98100854e-02  5.75552471e-02 -6.22831983e-03\\n -1.99530236e-02  3.54739912e-02 -1.77747998e-02  6.05868623e-02\\n -2.79850885e-02  1.68357361e-02  3.77813727e-02 -3.16506550e-02\\n -3.02383583e-02 -3.12538408e-02 -1.54635189e-02 -1.67161673e-02\\n  2.54361536e-02 -7.67957643e-02 -4.31675650e-03  1.93192419e-02\\n -8.34147111e-02  5.42093329e-02 -3.88762876e-02 -6.37245923e-02\\n  1.29308486e-02 -1.53285991e-02  1.36089157e-02  4.41432968e-02\\n  5.61045557e-02 -2.04159599e-02 -3.68739536e-04 -2.59794146e-02\\n -2.60894690e-02  6.19277768e-02 -4.37130127e-03  2.81794183e-03\\n -3.28957997e-02 -6.51409104e-02 -2.40177494e-02 -1.09539377e-02\\n -1.34565281e-02  2.43466031e-02 -1.98652931e-02  4.19527888e-02\\n  2.94487774e-02 -2.46764198e-02  2.37530284e-02  3.50473225e-02\\n -3.01836282e-02 -4.85071242e-02  3.01059708e-03  1.30432611e-02\\n -2.07224507e-02  3.65845188e-02  8.29928147e-04  2.85611991e-02\\n  6.13019951e-02 -3.17731202e-02 -4.71975617e-02  1.05946679e-02\\n -6.51367679e-02 -2.74698380e-02  1.82388946e-02 -5.46474196e-02\\n  5.30468710e-02 -1.31204119e-02 -8.55925027e-03 -4.92742695e-02\\n -1.80107038e-02 -3.85570414e-02  7.04353154e-02 -8.08778554e-02\\n  4.67879884e-03 -1.60918792e-03  1.86294429e-02 -5.65809384e-03\\n  3.00106425e-02 -1.37689281e-02  3.68910306e-03  1.37061235e-02\\n  5.18929511e-02  3.03863585e-02 -2.69395411e-02 -3.32154073e-02\\n  4.44807857e-02  1.70137249e-02  1.04022175e-02  9.64014698e-03\\n  7.67215295e-03 -4.28781323e-02 -4.72306684e-02 -9.54325497e-02\\n  7.50353327e-04  2.33420078e-02 -2.81221746e-03  5.20653790e-03\\n -1.66443810e-02  2.41619628e-02 -6.34180568e-03 -7.52411857e-02\\n  4.46818806e-02  8.29618052e-03  1.57727785e-02  7.24453703e-02\\n -7.08179781e-03 -2.33559404e-02 -3.42408679e-02 -1.44013111e-02\\n  5.68268495e-03  4.57961001e-02  1.67173110e-02 -1.23064723e-02\\n -7.08939955e-02  5.21719195e-02 -2.73503680e-02  2.23050788e-02\\n  1.36661744e-02 -3.51383276e-02  1.15019204e-02 -1.91903226e-02\\n  7.83371404e-02  4.25902493e-02  5.21558635e-02  2.98702195e-02\\n  1.39373681e-02  9.34602972e-03 -1.47393420e-02 -2.42245514e-02\\n -4.83847186e-02  1.21530248e-02  6.89280313e-03  2.44854297e-03\\n -1.56884696e-02  2.12883521e-02  4.87630218e-02 -5.65825030e-03\\n -5.92057332e-02  2.60504670e-02  1.45769063e-02 -7.01801910e-04\\n -3.61946821e-02 -4.45550047e-02 -4.17582095e-02 -9.48697235e-03\\n -8.47781263e-03 -5.25665209e-02  4.22344543e-02 -3.21982875e-02\\n  6.80717453e-03 -5.51561825e-03 -2.54549030e-02 -2.02448163e-02\\n  9.25501529e-03  9.02437698e-03  1.26717994e-02 -2.93020289e-02\\n -1.51856579e-02 -4.23472039e-02  5.30596683e-03  2.19516605e-02\\n -5.72636537e-02  2.40970142e-02 -1.94417802e-03 -3.40914796e-03\\n -1.46735841e-02 -1.81984566e-02 -4.08854820e-02  4.42818302e-04\\n  5.15100621e-02  2.90688686e-02  2.96004140e-03 -8.26279167e-03\\n -3.06749754e-02 -3.23384139e-03  1.22588139e-03  1.48657486e-02\\n -2.56232955e-02 -1.70293413e-02 -3.66826579e-02  7.54618570e-02\\n  3.50021683e-02  7.97078173e-05 -4.23613004e-02  8.83485563e-03\\n  5.30207995e-03  2.62184460e-02 -6.49980735e-03  1.25640472e-02\\n -4.64427434e-02  2.94769164e-02  1.63346045e-02 -3.22375819e-02\\n -5.55492379e-02  8.75246804e-03  1.65882539e-02 -1.55319162e-02\\n  1.10531943e-02  8.51799622e-02  1.11923823e-02 -4.13379334e-02\\n -2.16920637e-02 -3.52449827e-02 -6.98157959e-03  1.99761614e-02\\n  8.38554278e-03 -9.33555216e-02  1.85125396e-02 -2.00493205e-02\\n  1.30748609e-02  1.26927234e-02 -9.88091249e-03 -2.26713698e-02\\n -9.19059068e-02  6.14167713e-02  3.53370532e-02 -9.84434783e-03\\n -3.97086553e-02 -2.72986405e-02 -1.56240286e-02  5.17451614e-02\\n -2.24749241e-02  4.12584981e-03 -5.84956631e-03 -1.25876470e-02\\n  5.07185142e-03 -1.34583795e-02 -2.73564495e-02 -3.97239178e-02\\n -5.07604033e-02  4.30149771e-02  8.72246101e-02  7.76322782e-02\\n -1.12280706e-02 -5.88576086e-02 -3.82973487e-03 -2.28570998e-02\\n -2.08937074e-03 -2.18730830e-02  4.80370373e-02 -2.18923222e-02\\n  1.39678242e-02  2.66620219e-02  4.48891446e-02  2.16244832e-02\\n -1.64324474e-02 -3.42495441e-02  8.54879022e-02 -2.35475469e-02\\n  6.87733851e-03 -7.60420710e-02  1.00436779e-02  1.94740109e-02\\n  5.00400998e-02  2.27936674e-02 -1.57931056e-02 -2.52073035e-02\\n  1.70356696e-04  6.27915263e-02 -2.49368697e-02 -1.73366163e-02\\n -2.36260779e-02 -2.09805425e-02  2.75158845e-02 -8.31225216e-02\\n -2.65611913e-02  3.04115529e-04  2.43952945e-02  1.38610639e-02\\n  3.92894596e-02 -8.92428961e-03 -3.20860855e-02 -9.45880339e-02\\n -3.01242322e-02  3.88907269e-02  1.84648279e-02 -2.35336926e-02\\n -2.63089146e-02  5.94984964e-02 -2.45745704e-02 -1.02681760e-02\\n -2.21446343e-02 -5.01685105e-02 -5.45529686e-02 -3.59158404e-02\\n  2.63076834e-02  1.73961651e-02  3.84223238e-02  1.25118038e-02\\n -1.43557712e-02  8.88908952e-02  3.38997878e-02 -1.84298549e-02\\n  2.01570485e-02  1.57588292e-02  2.79862471e-02 -1.02544921e-02\\n  2.42367908e-02 -2.73485109e-02  2.04819217e-02 -3.98242213e-02\\n  8.67987331e-03  3.66083346e-02 -4.26335819e-02  1.03534302e-02\\n -4.33910713e-02  2.45185290e-02 -3.28088664e-02  3.02371662e-02\\n -4.27966900e-02 -9.76146478e-03 -4.12293933e-02  1.82218570e-02\\n -3.47563550e-02  4.63139750e-02  3.23385559e-02 -2.24531293e-02\\n -2.27955841e-02 -1.94589067e-02 -4.42537107e-03  2.56601516e-02\\n -1.15858810e-02  2.38203909e-02  3.76019021e-03 -2.86310352e-02\\n -1.97813604e-02  1.41906375e-02 -1.20868720e-02  7.58576207e-03\\n -4.93607149e-02  2.57298891e-02 -5.59213273e-02  4.90881503e-02\\n  8.19231849e-03  3.05567458e-02 -1.57357454e-02  5.35294190e-02\\n -3.16732153e-02  2.44531166e-02 -4.59625237e-02  4.91613410e-02\\n  6.75833300e-02 -3.20427120e-02  2.63311230e-02 -3.04164272e-02\\n -2.51198430e-02  5.10930736e-03  2.13359706e-02 -2.74750236e-02\\n -1.92729477e-02  2.40062065e-02  1.98703278e-02 -2.66616493e-02\\n -1.41242743e-02  2.60517690e-02 -2.08583358e-03 -2.22874386e-03\\n -5.59475906e-02 -5.60386386e-03 -3.99341546e-02  2.29575057e-02\\n  7.14664683e-02  3.87723818e-02  1.69053988e-03  2.36659441e-02\\n -3.90932783e-02 -4.80528846e-02  4.71287072e-02  3.23407017e-02\\n -2.65909899e-02 -8.75022670e-04  3.79911028e-02  4.68954854e-02\\n -5.21422504e-03  1.57285370e-02 -2.53654625e-02  3.31767686e-02\\n -2.86200177e-02 -5.22533730e-02 -1.62805803e-02 -6.26236901e-33\\n -4.33438085e-02 -6.85305297e-02  1.24460021e-02  1.61950551e-02\\n -5.56498095e-02  3.17057502e-03  3.96865094e-03 -2.33456325e-02\\n  1.72725506e-02 -3.53449164e-03 -1.75961256e-02  6.16471923e-04\\n  7.03692483e-03  1.01431422e-02 -1.29303178e-02  1.44601529e-02\\n  2.53642295e-02 -4.17077839e-02 -9.61641967e-03  2.70207617e-02\\n  5.07971309e-02  4.08038162e-02  4.34511118e-02 -4.59761843e-02\\n  3.76200350e-03 -7.85818323e-03  2.14124024e-02 -1.87127236e-02\\n  3.28250863e-02  6.07315823e-02 -3.90996784e-02  7.94191845e-03\\n  1.34015009e-02 -2.76365913e-02 -1.45112285e-02  8.82879198e-02\\n -8.48165676e-02 -5.09673767e-02 -1.16269691e-02 -1.86332855e-02\\n -1.03972167e-01 -3.53528857e-02  5.55211678e-02 -3.03391460e-02\\n -2.40238979e-02  1.22450301e-02  4.10262570e-02 -2.66571660e-02\\n -3.40366289e-02 -1.45270759e-02 -5.45327514e-02  7.28983805e-03\\n  1.83087941e-02  1.53348763e-02  5.75882941e-02 -4.37704809e-02\\n  3.71039137e-02 -1.23996916e-03 -7.11266324e-02  2.44014617e-02\\n -1.72680281e-02  6.96294680e-02  1.52928010e-02 -8.29605036e-04\\n  1.29652151e-03  7.69719258e-02  1.67689864e-02 -7.86779299e-02\\n -2.11486034e-02  1.65321038e-03  1.50914192e-02  5.49519025e-02\\n  4.06599268e-02  3.62063535e-02  6.15759827e-02 -5.05925268e-02\\n -9.53943469e-03  4.25859466e-02 -2.29700655e-02 -2.26466060e-02\\n  5.37245162e-02  3.60369263e-03 -1.69274979e-03 -9.04021692e-03\\n -2.54309569e-02 -3.37125808e-02 -3.28003764e-02 -6.37254938e-02\\n  4.00305502e-02  2.91038696e-02  2.70768139e-03 -1.26286503e-02\\n  2.31475532e-02 -2.46616639e-02  2.09801346e-02  4.59727757e-02\\n -3.32017802e-02 -5.89722302e-03 -4.71494254e-03  8.85300338e-03\\n -8.30917736e-04 -5.00419997e-02  3.78114432e-02 -1.44982489e-03\\n  3.95709230e-03  9.73263010e-03 -1.82523429e-02  3.79770063e-02\\n -7.48026147e-02  3.56095424e-03 -8.41138605e-03 -1.88198350e-02\\n -3.43423896e-02 -1.76647510e-02 -2.35390151e-03 -1.52711030e-02\\n  9.74835176e-03  2.46091262e-02  4.72947257e-03  1.23288305e-02\\n -2.40763314e-02 -5.06522208e-02 -7.92343076e-03  5.51250465e-02\\n -3.34273390e-02  5.73555380e-02 -3.65098976e-02  7.25264102e-02\\n  4.61673811e-02 -5.42443767e-02 -3.54934833e-03  5.75562119e-02\\n  2.70490403e-07  3.24287824e-02  3.81958447e-02  5.74876927e-02\\n -3.42632737e-03  4.34522368e-02  1.49162626e-03  6.00272184e-03\\n  4.50648554e-02 -9.73579194e-03 -2.37692501e-02  2.22082157e-02\\n  3.69301788e-03  2.55061015e-02  2.19708253e-02 -7.13283867e-02\\n  5.06571718e-02 -6.00605756e-02  1.77725442e-02 -3.96458656e-02\\n  1.97637938e-02  1.02954082e-01  7.83469006e-02  2.15783250e-02\\n -1.44885816e-02 -2.12630201e-02 -4.74940129e-02  6.10424876e-02\\n -4.82918974e-03  2.70257462e-02 -3.32832374e-02  1.90146267e-02\\n  4.43006456e-02  1.10660354e-02 -1.20479651e-02 -2.94476897e-02\\n  4.87429313e-02  3.85488160e-02  4.36521769e-02 -1.46958949e-02\\n -8.01871158e-03  7.45433662e-03 -9.52502340e-03 -2.75174975e-02\\n -3.36723067e-02  5.50915711e-02 -2.26959325e-02 -1.93760432e-02\\n -1.67831462e-02  1.61470100e-02  9.23306867e-03  3.06924619e-02\\n -2.29738522e-02 -5.27216308e-02 -8.46233964e-03  3.04553229e-02\\n -4.25873045e-03  4.07018047e-03 -7.53210708e-02  1.63513515e-02\\n  8.18353798e-03 -4.46045250e-02 -1.85178034e-02 -5.37526570e-02\\n -1.12238815e-02  4.57762890e-02  1.07487505e-02 -4.37343530e-02\\n  2.53108018e-34 -1.60900073e-03 -2.89805904e-02  1.18095509e-03\\n  3.10248639e-02 -9.27939266e-03 -4.89947945e-03  3.37163061e-02\\n  4.14558500e-02 -1.43227689e-02 -5.90011626e-02 -5.70045821e-02]'},\n",
       " {'page_number': 6,\n",
       "  'sentence_chunk': 'Consequently, when projected into twodimensional word embeddings for visualization purposes, similar terms are clustered together. Word embeddings can have varying dimensions, from one to thousands. A higher dimensionality might capture more nuanced relationships but at the cost of computational efficiency. While we can use pretrained models such as Word2Vec to generate embeddings for machine learning models, LLMs commonly produce their own embeddings that are part of the input layer and are updated during training. The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand. Unfortunately, high-dimensional embeddings present a challenge for visualization because our sensory perception and common graphical representations are inherently limited to three dimensions or fewer, which is why figure 2.3 shows two-dimensional embeddings in a two-dimensional scatterplot. However, when working with LLMs, we typically use embeddings with a much higher dimensionality. For both GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality of the model’s hidden states) varies based on the specific model variant and size. It is a tradeoff between performance and efficiency. The smallest GPT-2 models (117M and 125M parameters) use an embedding size of 768 dimensions to provide concrete examples.',\n",
       "  'sentence_chunk_size': 1428,\n",
       "  'sentence_chunk_word_count': 209,\n",
       "  'sentence_chunk_tokens': 357.0,\n",
       "  'embedding': '[-3.12300045e-02 -1.44422753e-02 -1.74616259e-02  5.29265031e-02\\n -1.65389944e-02  1.74851865e-02 -2.38935906e-03  4.99707200e-02\\n -3.07899783e-03  1.06585417e-02  1.81003285e-04 -3.67936753e-02\\n  7.68206024e-04  2.34860256e-02  4.85855043e-02 -7.94034526e-02\\n  5.46903089e-02  8.62479210e-03 -5.13034798e-02 -1.35308197e-02\\n -3.26042972e-03 -1.21887857e-02 -8.63352441e-04  2.87076589e-02\\n -3.49310748e-02 -2.66000070e-02 -1.23767303e-02 -2.93399747e-02\\n  3.38753350e-02 -5.17742150e-02 -7.22926185e-02  1.86344814e-02\\n  9.53526124e-02  2.38382481e-02  2.23105803e-06 -1.51894409e-02\\n  9.87899955e-03  2.83994023e-02  1.12084402e-02  6.13483898e-02\\n  2.87979748e-02 -5.42907678e-02  1.58540923e-02 -2.91892421e-02\\n -3.74981835e-02 -3.00597399e-02  5.48149534e-02  6.37511164e-02\\n -1.08771631e-02  9.13834572e-03 -1.72548965e-02 -3.77829820e-02\\n  4.65660580e-02 -3.70279700e-02  1.54807968e-02  4.09326609e-03\\n -7.50875741e-04 -2.41659749e-02 -2.51986347e-02  4.25490811e-02\\n -7.61715695e-02  1.43750533e-02 -2.90576555e-03  3.05905547e-02\\n  6.63649365e-02  7.15566948e-02  2.12132465e-02 -5.67258662e-03\\n  4.76994901e-04  4.31007668e-02 -3.51923555e-02  4.11172882e-02\\n -1.81015264e-02 -4.13701385e-02 -1.12320734e-02  4.44312356e-02\\n -4.80610430e-02 -3.99617553e-02  2.85288319e-02  4.98394407e-02\\n -3.08298655e-02 -1.10867824e-02  4.50420305e-02 -4.93284054e-02\\n -2.95950603e-02  3.30209695e-02  2.98546329e-02 -1.92264318e-02\\n -1.13081550e-02 -2.09743530e-02 -2.90572029e-02  2.60392912e-02\\n  2.34251982e-03 -1.57663058e-02  7.00466409e-02  7.36813433e-03\\n  1.56542268e-02 -4.08478500e-03  2.45642774e-02 -5.93753383e-02\\n  1.04557864e-01  1.97426379e-02 -2.33511981e-02  3.66657563e-02\\n -5.30289970e-02  3.50249298e-02 -4.71597798e-02  4.97384779e-02\\n -2.08891090e-02  3.84927765e-02  4.49563144e-03 -2.09938791e-02\\n -2.87985317e-02  4.09937650e-03  1.83584273e-03 -5.52355275e-02\\n -5.38399033e-02  2.26377919e-02 -1.16028208e-02 -1.44530600e-02\\n  6.25624694e-03  2.46499721e-02  2.81623993e-02  1.39612928e-02\\n -5.00016399e-02  1.55874882e-02 -5.18161841e-02  1.45808551e-02\\n -4.05844599e-02 -1.51377905e-03 -2.28286032e-02 -4.31107078e-03\\n  1.84393339e-02 -1.73030701e-02 -1.64018050e-02  7.24185184e-02\\n  3.32459472e-02 -4.77059409e-02 -3.72481882e-03  1.75927766e-02\\n -3.35839614e-02  1.03892991e-02  6.37826845e-02 -2.16375291e-02\\n  5.31907752e-02 -9.90702584e-03  4.82059829e-03 -2.86047477e-02\\n  1.66902249e-03 -8.76158569e-03 -5.15234321e-02  9.72380489e-02\\n -3.58007737e-02 -2.89181303e-02  2.46232841e-02  1.29657378e-02\\n -7.30080297e-03  2.80337762e-02  5.07187210e-02  2.47356184e-02\\n  1.79655440e-02 -6.51753098e-02 -9.57692973e-03 -1.68247614e-02\\n  2.25199736e-03  5.79898013e-03 -6.21559061e-02 -3.90871651e-02\\n  1.56150991e-02  6.04865178e-02  5.91731966e-02  2.83337403e-02\\n -5.83101586e-02 -4.74054702e-02  4.39160019e-02  1.43472522e-01\\n  6.86298385e-02  3.51332501e-02  2.59326994e-02  2.32243184e-02\\n -1.08633535e-02  1.52749689e-02 -4.86583402e-03  4.92216125e-02\\n  3.95076443e-03  2.26291316e-03  3.78151797e-02  1.09294774e-02\\n -5.10977907e-03 -3.31458040e-02 -5.30714728e-02 -1.64891928e-02\\n  3.31582054e-02 -5.01665100e-02  1.55621227e-02 -5.26341982e-02\\n -4.12904248e-02  5.05584590e-02  9.10324138e-03 -4.88698743e-02\\n -3.09725050e-02 -9.78290569e-03 -4.20014560e-03  5.86807542e-02\\n  1.78748500e-02 -1.45651009e-02 -8.96910846e-04 -1.01045519e-02\\n -1.60190184e-02  4.42292020e-02  3.37690227e-02  2.61782054e-02\\n  2.04249495e-03 -5.13437614e-02 -4.09894586e-02 -3.38162668e-02\\n  1.66302882e-02  3.13500278e-02 -1.54809114e-02  1.02570467e-02\\n  5.49623277e-04 -5.19894399e-02  2.61324067e-02  2.26399563e-02\\n -1.52386092e-02 -1.11627867e-02 -2.10984871e-02  9.44234896e-03\\n -3.02840327e-03 -2.43814383e-03 -3.66237620e-03  1.08029265e-02\\n  4.40354906e-02 -3.12311258e-02 -1.89511422e-02 -4.07545380e-02\\n -3.24264690e-02  1.69630721e-02  1.97601859e-02 -2.89439764e-02\\n  5.16741574e-02 -2.42524222e-02 -6.20587310e-03 -1.26194013e-02\\n -7.18785310e-03 -4.21520807e-02  6.41123056e-02 -4.46659997e-02\\n -2.81454809e-02 -2.58656032e-02  6.34133518e-02 -1.88414678e-02\\n  2.59799361e-02 -6.41521588e-02  1.13066873e-02 -1.52946003e-02\\n  1.10735316e-02  1.98651999e-02  1.01365277e-03  3.95813695e-04\\n  2.20468398e-02  3.23382132e-02  7.08871242e-03 -2.20997985e-02\\n -2.67932992e-02 -2.54400540e-02 -1.57342087e-02 -3.84464934e-02\\n -2.45105028e-02  3.47355902e-02  8.36119242e-03 -2.98955161e-02\\n -4.29868847e-02 -1.98544357e-02  1.48563292e-02 -5.07263606e-03\\n  5.32754026e-02 -3.25126387e-02  1.63840689e-02  7.29713812e-02\\n -2.34267190e-02 -2.24377271e-02 -4.00156081e-02 -1.03793303e-02\\n -1.11589897e-02  3.16689238e-02 -1.98442675e-02 -3.23492065e-02\\n -2.02925056e-02  3.27467956e-02 -2.63524540e-02  2.06992682e-03\\n  6.99070562e-03 -3.73269804e-02  3.86002213e-02 -2.48243511e-02\\n  4.53923158e-02  2.32258085e-02  1.68895759e-02  9.78817344e-02\\n  6.78389370e-02  1.27475001e-02 -2.23244708e-02  4.81698755e-03\\n -4.65709232e-02  3.79616097e-02 -1.51074650e-02 -8.26819800e-03\\n -2.98609156e-02  5.18175662e-02  3.76011394e-02 -6.15447480e-03\\n -5.52711561e-02  2.17963047e-02 -1.88738387e-02  3.49981897e-03\\n -3.23430635e-02  1.34202040e-04 -3.94272543e-02  8.13800652e-05\\n  1.11253234e-02 -6.16663769e-02  1.62529126e-02 -1.51947308e-02\\n  2.09527016e-02 -3.48682585e-03 -1.69363022e-02 -1.27517022e-02\\n  5.23588359e-02  1.20498426e-02  2.21183430e-02 -6.19712509e-02\\n -1.34215886e-02 -2.57532741e-03  2.88517512e-02  1.65722370e-02\\n -3.29121910e-02  2.12984439e-02 -4.06090496e-03 -3.78264710e-02\\n -3.98814864e-02 -1.10673998e-02 -3.02937068e-03  2.05165171e-03\\n -1.94324739e-02  1.39717944e-02  2.93985624e-02 -4.39658687e-02\\n -1.99143495e-02  1.41943088e-02  1.55464374e-02  2.35848385e-03\\n -2.55984487e-03 -2.11480577e-02 -1.98365971e-02  1.55519536e-02\\n  4.76998696e-03  3.75025645e-02  1.26480842e-02  3.43292183e-03\\n  2.71552596e-02  9.59428214e-03 -4.11044359e-02  3.56908254e-02\\n -3.27333547e-02  3.19043063e-02  2.23631039e-02 -7.35742040e-03\\n -5.51946126e-02  3.09455534e-03  4.95486334e-02 -1.74064301e-02\\n  1.70466285e-02  9.73396152e-02  1.03881480e-02 -7.22396374e-02\\n  2.31597964e-02 -4.18648459e-02  2.69558001e-03  3.38590592e-02\\n -2.30072513e-02 -5.03848232e-02  1.28682051e-02 -2.45142635e-02\\n -3.06361057e-02  2.34486572e-02 -5.19377925e-03  3.45992530e-03\\n -7.04632625e-02  5.16090691e-02  2.92733200e-02 -1.01585053e-02\\n -3.22610326e-02 -5.69500327e-02 -1.52365481e-02  3.93827632e-02\\n  1.95811130e-02 -3.50607559e-02 -3.56413461e-02 -9.52543970e-03\\n  1.57695338e-02 -3.06755938e-02 -1.39848441e-02  4.58991562e-04\\n -1.66272186e-02 -5.33364201e-03  3.01577002e-02  5.17137311e-02\\n -5.63092604e-02 -1.03881486e-01  5.29577956e-03  4.12949063e-02\\n  4.02262360e-02 -2.13630591e-02  5.19013554e-02  3.38182151e-02\\n  6.22891635e-03 -2.97145974e-02 -3.12309922e-03  4.42126989e-02\\n -1.29618933e-02 -5.97853810e-02  1.13444343e-01 -4.12879884e-02\\n -4.11566012e-02 -4.77016345e-02  3.07632108e-05 -6.79440126e-02\\n  4.48599905e-02  7.05437586e-02 -2.74163727e-02 -1.68236345e-02\\n -9.93709359e-03  7.46439546e-02  2.14283783e-02 -3.46017145e-02\\n  1.52397633e-03  2.15732753e-02  2.78567243e-02 -2.21144464e-02\\n  4.39089257e-03  1.37112532e-02  1.35782911e-02  8.35399237e-03\\n  4.71346192e-02 -3.82813327e-02 -2.42900904e-02 -5.05885147e-02\\n  1.72283109e-02  1.83633752e-02  1.39708193e-02  1.51005583e-02\\n  1.23188121e-03  2.79735215e-02  1.17797907e-02 -3.51930745e-02\\n -3.27749029e-02 -5.69038726e-02 -3.09660621e-02 -3.37802991e-02\\n -6.84770430e-03  7.34800706e-03  3.56911402e-03  2.58267540e-02\\n -5.12443818e-02  8.94117057e-02  2.60898508e-02  2.18765866e-02\\n  8.83683469e-03  2.76647843e-02  2.92428806e-02 -1.20780086e-02\\n  1.12138875e-02  8.93655419e-03 -1.27418507e-02 -2.92363912e-02\\n -3.50094424e-03  5.57254301e-03 -1.98342595e-02 -2.43769605e-02\\n -2.78157219e-02  9.76602361e-02  1.16799008e-02  2.98967818e-03\\n -4.21867743e-02 -5.04769059e-03 -4.18784432e-02  3.75440978e-02\\n -7.97739904e-03 -1.89208873e-02  5.59482351e-02 -4.45926450e-02\\n -3.80682349e-02 -7.20673939e-04  1.63036473e-02 -2.78301863e-03\\n -4.95346114e-02  5.14553599e-02  2.62572169e-02 -5.57442531e-02\\n -4.52000531e-04  3.97648551e-02 -3.85720693e-02  3.40497820e-03\\n -8.04649889e-02 -1.96684338e-02  9.16351192e-03  1.56336557e-02\\n -3.23157646e-02  7.38138054e-03 -5.32580242e-02  5.42303734e-02\\n -4.24514636e-02 -5.21534756e-02 -7.32742809e-03  5.56647293e-02\\n  6.12659007e-02 -6.98357355e-03  4.04156521e-02 -2.53647529e-02\\n -2.46363916e-02  9.51753184e-03 -1.49435666e-03 -3.64089757e-02\\n  1.07539827e-02  9.90162566e-02  1.95116680e-02 -2.86886096e-02\\n -4.68840115e-02  5.54275513e-02  3.52680869e-02  3.56237441e-02\\n -4.33262624e-02 -1.65213421e-02 -7.59311020e-03  1.94504337e-05\\n  5.57835549e-02  7.38786310e-02 -1.08808605e-02  1.55172180e-02\\n -6.19531497e-02 -5.07039018e-02  5.91231138e-03 -2.20166403e-03\\n -2.95923520e-02 -2.25484744e-02  3.79439965e-02  3.17412317e-02\\n  1.79060141e-03 -2.35369839e-02  1.03592006e-02  1.35112535e-02\\n  1.03870733e-02 -6.89243153e-02  3.31854932e-02 -6.60561924e-33\\n -1.49576785e-02 -4.10075821e-02  1.45404479e-02 -3.52067240e-02\\n -6.44254535e-02  1.76864173e-02  6.47226069e-03  5.78269036e-03\\n -2.72032302e-02  4.13430408e-02 -3.11711710e-02 -2.04613991e-03\\n -1.21025462e-03 -1.46313906e-02  1.09014362e-02  3.34811513e-03\\n  6.99025765e-03 -1.97203830e-02  8.24238453e-03  8.23864248e-03\\n  6.25189114e-03  4.30904701e-02  6.70142993e-02 -3.07779796e-02\\n  4.68594162e-03  4.75749783e-02  3.61764245e-02 -1.35245286e-02\\n  1.73696503e-03  4.85387743e-02 -7.75459409e-02  4.65768725e-02\\n  1.60132721e-02 -1.76500045e-02 -4.92796525e-02  2.69933743e-03\\n -1.62236404e-03 -5.52517958e-02  7.47289183e-03 -1.47858597e-02\\n -6.18773177e-02 -3.11362911e-02  3.97528000e-02 -3.85169056e-03\\n -3.92842442e-02 -2.69488264e-02  3.05227451e-02 -3.39114629e-02\\n -8.35833177e-02  1.71823036e-02 -3.10822520e-02  2.87026707e-02\\n  1.25301881e-02  3.53508778e-02  7.30272084e-02 -4.50974815e-02\\n  3.98719981e-02  2.53126714e-02 -4.61918563e-02  3.52966674e-02\\n  6.21701125e-03  1.32682761e-02  3.01950090e-02  4.90863854e-03\\n  1.72092579e-02  9.52122733e-02  1.60537511e-02 -4.60358076e-02\\n -2.75731198e-02 -2.20916513e-02  2.75656190e-02  7.47730061e-02\\n  5.79698272e-02 -2.25557089e-02  4.90938723e-02 -4.24395315e-02\\n -3.58008929e-02  5.46873314e-03 -3.23951170e-02 -1.21097965e-02\\n  2.75501329e-02  2.07117144e-02 -4.55942713e-02 -3.14605050e-02\\n -2.36950833e-02 -3.65612246e-02 -2.69336570e-02 -5.17386720e-02\\n  3.13206464e-02 -3.04860175e-02  3.55792381e-02 -1.46525530e-02\\n -8.77009146e-03 -2.02037338e-02 -8.45193677e-03  1.07406443e-02\\n  1.34301241e-02  1.73721779e-02  1.30655337e-02 -1.53174940e-02\\n  3.44281271e-02 -4.00959179e-02 -3.07035148e-02 -1.96492262e-02\\n  1.92503482e-02 -9.36390087e-03 -3.36640235e-03 -1.48165282e-02\\n -5.08664399e-02 -1.46112312e-02 -1.98781621e-02 -1.06121905e-01\\n  7.16705620e-03 -1.42336246e-02  4.69905846e-02 -3.77343930e-02\\n -3.19757103e-03  5.26420958e-02  1.12831062e-02  2.55579296e-02\\n  3.00411979e-04 -3.62074189e-02 -4.90554236e-02  5.52572571e-02\\n  4.08548769e-03  6.93673640e-03 -4.91946414e-02  1.66586600e-02\\n  7.47899637e-02 -8.26962739e-02 -4.92380336e-02  4.75981459e-03\\n  2.88004685e-07  5.28355502e-02  9.00800526e-02  2.52552927e-02\\n  9.05107940e-04  2.82175560e-02  8.90376978e-03  6.26621535e-03\\n  4.59358990e-02 -7.91947767e-02 -2.26529837e-02  1.94600485e-02\\n  6.32037893e-02  3.38507704e-02  2.64461040e-02 -9.01166350e-03\\n  3.29868831e-02 -5.18075414e-02  3.65552492e-02 -4.32834886e-02\\n  2.72205845e-02  6.29808828e-02  7.57370591e-02  1.90469939e-02\\n -5.37026813e-03 -1.03833782e-03 -4.06370610e-02  4.95156795e-02\\n -7.54082575e-02  7.14976564e-02 -1.72798559e-02  1.00068152e-02\\n  2.48468257e-02  3.18669043e-02 -1.29808765e-02 -8.87137931e-03\\n  3.50993648e-02  1.89826023e-02  3.57229374e-02 -1.16226934e-02\\n  1.12352753e-02  1.53258117e-02 -3.49762104e-03 -4.79830382e-03\\n -6.53182715e-02  5.88658415e-02 -2.30303016e-02 -2.19195858e-02\\n -2.17044763e-02  4.93405610e-02 -1.50654595e-02  9.30857211e-02\\n -3.13220099e-02 -2.21226588e-02 -1.42911952e-02 -1.00374995e-02\\n -4.05168980e-02  9.82238445e-03 -4.95039448e-02  1.46138621e-02\\n  3.79870161e-02 -5.49273677e-02 -3.29964161e-02 -5.55382669e-02\\n  9.92561784e-03  8.73912573e-02  4.19878624e-02 -5.35635576e-02\\n  2.81917065e-34  3.59241478e-02 -2.40726788e-02  4.49962094e-02\\n  3.40464897e-02  1.83881577e-02 -2.27999799e-02  4.41407599e-02\\n -8.35400261e-03  8.05180985e-03 -8.87676775e-02 -3.07591315e-02]'},\n",
       " {'page_number': 6,\n",
       "  'sentence_chunk': 'The largest GPT-3 model (175B parameters) uses an embedding size of 12,288 dimensions. Tokenizing text Tokenization is a fundamental step in Natural Language Processing (NLP). It involves dividing a Textual input into smaller units known as tokens. These tokens can be in the form of words, characters, sub-words, or sentences. It helps in improving interpretability of text by different models. Let’s understand How Tokenization Works. Depending on the LLM, some researchers also consider additional special tokens such as the following: • [BOS] (beginning of sequence) —This token marks the start of a text. It signifies to the LLM where a piece of content begins. • [ EOS] (end of sequence) —This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one ends and the next begins. • [',\n",
       "  'sentence_chunk_size': 974,\n",
       "  'sentence_chunk_word_count': 155,\n",
       "  'sentence_chunk_tokens': 243.5,\n",
       "  'embedding': '[ 2.95650568e-02 -5.14596924e-02 -1.86201558e-03  4.45580333e-02\\n -4.86873575e-02  3.10077518e-02  2.75826314e-04  4.09601852e-02\\n -1.49098700e-02 -2.93679927e-02  4.96761538e-02 -3.80105302e-02\\n  1.24358861e-02  4.83316928e-02  4.12162952e-02 -7.73418546e-02\\n  5.90457991e-02 -9.97230224e-03  6.42705150e-03  3.56050255e-03\\n  1.00191897e-02 -1.17426123e-02  2.90128104e-02  4.47660945e-02\\n -3.64730209e-02 -2.13097897e-03 -3.35033536e-02 -8.04782845e-03\\n -3.40828188e-02 -3.01029421e-02 -7.47483969e-02  1.97086502e-02\\n  2.63249073e-02  3.19348350e-02  2.04914909e-06 -4.28724885e-02\\n -5.91647625e-02  2.67690867e-02  4.49140416e-03  4.28975224e-02\\n  5.00758700e-02 -3.84405591e-02 -1.26546342e-02 -2.19044159e-03\\n  2.51932978e-03 -9.66320559e-03  4.09798622e-02  1.05386324e-01\\n  1.05352011e-02  6.40250519e-02 -1.10210264e-02 -5.55115640e-02\\n  5.03364503e-02  1.60295125e-02  3.90066989e-02 -1.56869385e-02\\n  4.44458425e-02 -3.65473554e-02  1.97800212e-02  6.71478212e-02\\n -1.72153562e-02 -1.30793685e-02  1.50441565e-02  1.65452436e-02\\n  9.72259417e-03  4.78895530e-02 -6.46213666e-02 -1.73703600e-02\\n -1.66112948e-02 -5.35132643e-03  3.74657623e-02 -3.37081496e-03\\n  8.02775379e-03 -1.52928699e-02  4.82678935e-02 -3.07272859e-02\\n -5.91934770e-02 -1.06961122e-02 -4.59445780e-03  2.46229637e-02\\n  7.05605140e-03 -7.69174378e-03 -4.57628921e-05 -5.16829342e-02\\n -6.73564002e-02  9.98421907e-02  1.37357041e-02 -4.09851633e-02\\n -3.57821025e-02 -3.93210091e-02 -5.70481084e-02 -2.11860426e-02\\n  5.31950779e-03 -2.54669804e-02  3.41599733e-02 -1.11917360e-03\\n -1.99158024e-02 -2.28075478e-02  4.35840786e-02 -1.92660224e-02\\n -1.25958389e-02  3.46245766e-02  3.48747300e-04  3.90682332e-02\\n  7.89410621e-03  4.75326739e-02 -4.86049689e-02  2.03424692e-02\\n -2.30147727e-02 -8.15779250e-03 -2.08880333e-03 -6.20572492e-02\\n -5.80161437e-02  6.41164407e-02 -4.45273183e-02 -6.54538348e-02\\n -3.81939113e-02  2.30608825e-02  2.82560289e-02 -6.01104414e-03\\n -2.10425593e-02  1.10502215e-02  1.46252811e-02  1.12803401e-02\\n -3.15895188e-03 -2.34404001e-02 -4.36647348e-02 -2.21567322e-02\\n  8.26744828e-03 -1.63467787e-02 -1.36102503e-02  1.71057135e-02\\n  2.65089162e-02 -4.78042699e-02 -1.00722909e-02  7.13956580e-02\\n  4.61191982e-02 -3.82944942e-02  4.65623196e-03 -2.96254060e-03\\n -4.31947000e-02 -3.17334309e-02  5.07388439e-04 -3.64994705e-02\\n  1.33172637e-02  3.77541296e-02 -2.83393282e-02 -1.34032145e-02\\n -1.24712372e-02  1.16571635e-02 -6.46617413e-02  6.34970665e-02\\n -3.67113426e-02 -3.28042954e-02  2.16504801e-02 -1.52434595e-02\\n  3.09646199e-03  4.01877724e-02  8.99827946e-03  2.65895966e-02\\n  6.86907917e-02 -2.35850438e-02 -2.36945227e-02  2.99448334e-03\\n  5.24385832e-03  2.80539948e-03 -3.51145193e-02 -1.08309174e-02\\n  5.06888963e-02  6.62239492e-02  4.14632377e-04  9.32594687e-02\\n -3.42389867e-02 -2.23236419e-02  1.40697481e-02  1.54249996e-01\\n  8.90896842e-03  6.74630180e-02  4.05380540e-02  4.35082056e-02\\n  4.62148413e-02  4.14942317e-02 -2.88701374e-02 -6.63767429e-03\\n  1.27161425e-02  1.79908946e-02  8.34601969e-02  3.19218822e-02\\n  3.18539096e-03 -1.51573727e-02 -6.07374236e-02  2.11360995e-02\\n -3.56662576e-03 -1.05899870e-01  1.28628882e-02 -4.98498231e-02\\n -6.29274920e-02  6.01011626e-02 -2.50341073e-02 -5.98215871e-02\\n  3.43740988e-03 -5.01985149e-03  1.25317732e-02  5.92425875e-02\\n  1.19936801e-02 -3.34694907e-02 -1.15897376e-02 -1.42282611e-02\\n  1.79770142e-02  3.09684593e-02 -7.72934873e-03  4.52656522e-02\\n -1.82470456e-02 -3.25812772e-02  7.87114259e-03 -6.71266951e-03\\n  1.11250374e-02  1.53081827e-02 -1.98006164e-02  9.62459855e-03\\n -1.42277370e-03 -5.16415872e-02  3.64071205e-02  2.11183727e-02\\n  6.23367075e-03 -1.41775580e-02 -4.55502421e-02 -9.18709394e-03\\n  4.49844413e-02  3.18580382e-02  1.00221168e-02  1.90190151e-02\\n  4.34999466e-02  2.15654331e-03 -4.14647870e-02  3.82495369e-03\\n -6.12430163e-02  2.50425655e-02  3.60780284e-02 -1.00443289e-02\\n  2.56757736e-02 -7.63237523e-03  7.29097845e-03 -2.44685784e-02\\n  1.74635407e-02 -4.09481451e-02  9.00029838e-02 -4.39039953e-02\\n  6.53713522e-03 -1.38082169e-02  1.30711300e-02 -9.60442051e-03\\n  2.28489228e-02 -1.15352366e-02  3.02902367e-02  1.84750855e-02\\n -1.09489197e-02  6.71419203e-02 -2.97161154e-02 -1.17355715e-02\\n  1.16431862e-02 -1.02567235e-02  9.06222314e-03  2.01923270e-02\\n -2.16062460e-02 -2.93312334e-02 -7.15145245e-02 -9.39510688e-02\\n -1.83046190e-03  1.35967471e-02  2.42601931e-02  4.86135576e-03\\n -2.77033858e-02 -8.52558296e-03  4.28121723e-02 -1.83565784e-02\\n -3.20964269e-02 -1.04436604e-02  1.56690180e-02  7.63273761e-02\\n  6.27915608e-03 -4.44537960e-02 -2.23906115e-02 -1.12947331e-04\\n -4.04895023e-02  8.41702744e-02  8.68424773e-03 -4.47226539e-02\\n -4.25386839e-02 -1.09393913e-02  4.04319679e-03  1.62391923e-02\\n  1.90303884e-02 -8.30950588e-02  1.83586963e-02 -3.40722278e-02\\n  8.24478939e-02  3.91256772e-02  2.40078587e-02  8.88121873e-02\\n -8.52812082e-03 -1.19242268e-02  6.06468646e-03 -1.88543126e-02\\n -4.08287756e-02  6.82977885e-02 -2.75943223e-02  1.74913637e-03\\n  1.54958544e-02  4.20763902e-02  4.66169277e-03 -6.13837969e-03\\n -7.68745244e-02  5.54361753e-02 -1.31389042e-02 -1.11612515e-03\\n -1.26375472e-02 -1.71896685e-02 -8.44046623e-02  9.90888570e-03\\n  6.32339111e-03 -1.07639944e-02  1.42333871e-02 -6.32416904e-02\\n -3.62040917e-03 -2.44196393e-02  2.46350933e-02 -5.39940409e-02\\n  3.85113135e-02  1.82596780e-02  3.50970216e-02 -1.61038507e-02\\n -6.62745014e-02  1.62835214e-02  1.95503924e-02 -1.80710815e-02\\n -3.52430646e-03 -5.12599014e-03  7.21378298e-03 -1.54826045e-02\\n -5.87232457e-03 -7.03726476e-03  3.05659119e-02 -9.36786830e-03\\n  2.02154201e-02  2.39502396e-02  5.85683845e-02 -3.92292365e-02\\n -5.28318286e-02  2.65374798e-02  4.00597937e-02  3.95821668e-02\\n -4.84165847e-02  2.44028177e-02 -3.75946201e-02  3.48271206e-02\\n -1.22664934e-02 -1.46280872e-02  1.37806209e-02 -1.38194440e-03\\n  3.29975523e-02  5.24639115e-02 -7.50688510e-03 -2.46112118e-03\\n -1.83812436e-02  1.03116936e-05  2.87555605e-02 -4.95793112e-02\\n -7.12184459e-02  2.91630868e-02  5.96179776e-02 -3.58372852e-02\\n  1.09688565e-02  1.15994491e-01  1.60842557e-02 -3.50730233e-02\\n  1.78433340e-02 -2.11634766e-02  1.14372363e-02  2.39069480e-02\\n -2.17561666e-02 -4.70737927e-02  7.88232312e-03  1.02190943e-02\\n -4.64144275e-02  1.56709272e-02 -1.30814882e-02 -1.14327129e-02\\n -5.17928936e-02  3.77591550e-02  1.43377008e-02 -5.62537834e-02\\n -5.72365858e-02 -2.04598196e-02 -3.22937183e-02  3.25432681e-02\\n  4.30538245e-02  4.47345190e-02 -2.52517760e-02  4.42178361e-03\\n  2.65315585e-02 -4.02140506e-02 -4.71524103e-03 -3.69649902e-02\\n -3.36980447e-02  3.51462923e-02 -4.78501851e-03  3.51880230e-02\\n -5.60483076e-02 -2.63139382e-02  4.05666828e-02  9.28132795e-03\\n -1.17941445e-03 -1.96974594e-02  5.20677082e-02  2.52747326e-03\\n  1.60675105e-02 -3.87851545e-03  2.03864463e-02  8.82710051e-03\\n -2.05680225e-02 -3.57692838e-02  8.68109092e-02  2.22349446e-03\\n -5.44998329e-03 -4.40084115e-02 -1.87713876e-02 -7.52560943e-02\\n  5.16920947e-02  3.85346673e-02 -7.22685922e-03 -4.67078649e-02\\n -3.07243913e-02  3.45788188e-02 -9.44713131e-03  8.46819952e-03\\n -2.99979281e-02  1.69355813e-02  1.33504272e-02 -3.12066879e-02\\n  7.84965046e-03  1.63813699e-02  3.94633003e-02  1.19112385e-02\\n  5.52482791e-02 -5.76434024e-02 -6.32463209e-03 -1.94028318e-02\\n  1.75065603e-02  2.93206945e-02 -1.09605463e-02 -7.55385309e-02\\n  4.30349894e-02  5.53272548e-04 -1.36803160e-03 -4.69829552e-02\\n -4.16891910e-02 -1.16111338e-02 -6.81369901e-02  1.22090876e-02\\n -1.66920405e-02  2.59816349e-02  5.91870258e-03  4.35605869e-02\\n -4.41865660e-02  1.90521218e-02  1.79760251e-02 -3.38835567e-02\\n  2.51872428e-02  1.93130411e-02  2.28205062e-02  1.31064877e-02\\n  8.24731495e-03  6.14340883e-03 -2.93433294e-02 -3.01955864e-02\\n -9.68821906e-03  3.22990939e-02  2.04479974e-02  6.44772267e-03\\n  1.66017525e-02  3.79398577e-02  5.20341704e-03  1.71182994e-02\\n -6.75826939e-03  1.05548399e-02 -8.87100548e-02  2.20004171e-02\\n -1.77738089e-02  2.52652671e-02  4.14357893e-03 -2.26960070e-02\\n -5.22689745e-02 -3.34729329e-02 -2.28054449e-02  1.64435916e-02\\n  6.47948647e-04  4.15837532e-03 -1.07015669e-02 -3.30846608e-02\\n  4.12345603e-02 -2.39699129e-02 -2.81712934e-02 -2.76171714e-02\\n -7.46982619e-02 -8.98894947e-03 -7.37989247e-02 -3.65183596e-03\\n  2.83859149e-02  1.46484468e-02 -1.90857686e-02  7.34332949e-02\\n -5.38713522e-02  1.50415571e-02 -2.07772087e-02  1.75732039e-02\\n  1.67649593e-02 -4.67098347e-04  2.16845330e-02 -2.61443797e-02\\n -3.59917432e-02  5.58703020e-03  2.46995203e-02 -1.64606962e-02\\n  2.88325432e-03 -1.25957569e-02  2.31983513e-02 -5.59758246e-02\\n -4.25878726e-02  4.60104197e-02  8.39462131e-02  4.04992187e-03\\n  1.51761966e-02 -2.23878790e-02  1.14847144e-05  1.42538417e-02\\n  3.17716412e-02  5.57058156e-02 -2.83889403e-03  6.28653774e-03\\n -6.91458359e-02 -2.06465740e-02 -4.37136181e-03  2.26576999e-02\\n -2.84087360e-02 -1.63920838e-02  4.71114414e-03  6.61598668e-02\\n  1.73356943e-02 -5.90622872e-02  3.89728248e-02 -4.77530388e-03\\n  1.07492181e-02 -3.71218100e-02  6.29897416e-02 -6.43908108e-33\\n -1.03529776e-02 -4.24170196e-02  5.48765715e-03 -1.53288022e-02\\n -4.64178659e-02 -6.00833911e-03  1.64336115e-02 -7.93043803e-03\\n  5.35843410e-02 -4.63569304e-03 -3.45595144e-02 -1.01674972e-02\\n  8.25880934e-03  2.55611390e-02  2.86753080e-03  4.24616858e-02\\n  1.77413877e-02 -1.15517648e-02 -6.33695861e-03  2.00805720e-02\\n  2.71541029e-02  2.71021016e-02  4.13916111e-02  1.11543033e-02\\n  9.75275040e-03  1.42319994e-02 -3.02206189e-03 -1.01262713e-02\\n  1.08030345e-02  6.16478585e-02 -2.59349700e-02  9.34857130e-03\\n  9.00864042e-03 -6.37820959e-02 -3.67769529e-03  3.52145024e-02\\n -3.62809896e-02 -6.30434901e-02  5.74370846e-02 -8.97886511e-03\\n -2.26119701e-02 -5.75936474e-02  1.98988598e-02 -2.38699671e-02\\n -2.81786267e-02  4.81780805e-03  2.10210867e-02  5.57498494e-03\\n -4.45842855e-02  1.60920657e-02 -4.85535711e-02  2.08915807e-02\\n  1.11824851e-02  8.51964876e-02  5.56834973e-02 -5.49005009e-02\\n  1.21904165e-02  2.83168424e-02 -1.36593496e-03 -9.33910068e-03\\n  2.05952525e-02  5.14526255e-02 -2.80939694e-02 -3.19499038e-02\\n  1.86966192e-02  6.91985339e-02  2.99053397e-02 -1.19620422e-02\\n -3.48234773e-02 -6.56390144e-03 -6.24003960e-03  2.97098216e-02\\n  4.39364463e-02 -5.97802689e-03  2.60155983e-02 -2.68163625e-02\\n -3.68415266e-02 -1.79227907e-02 -3.98896746e-02  1.17381988e-02\\n  3.14730741e-02  2.68029911e-03 -2.12849751e-02 -2.48301141e-02\\n -3.46274227e-02 -4.39767502e-02 -1.30518759e-02 -5.94727434e-02\\n  3.49956825e-02 -3.17001455e-02 -3.37778181e-02 -2.64247414e-02\\n  5.59235364e-03 -1.52539182e-02 -1.13846157e-02  5.14818504e-02\\n -1.89095885e-02  1.16889887e-02 -1.20627228e-02  3.30998451e-02\\n -3.62232141e-02 -7.69808292e-02 -9.27676912e-03 -1.67539753e-02\\n  2.51660626e-02 -1.06141698e-02 -2.06345879e-03  1.95812751e-02\\n -8.35896805e-02  1.17910374e-02  8.20172671e-03 -2.42592171e-02\\n -1.69058312e-02 -3.86675471e-03  2.71776151e-02 -5.45107806e-03\\n  2.55203154e-02 -2.65667361e-04  2.34082174e-02 -1.83893903e-03\\n -3.46226543e-02 -1.55475857e-02 -2.31532101e-02  3.96644473e-02\\n -3.74825560e-02  1.47902612e-02 -4.53710817e-02  3.16274874e-02\\n  6.59785196e-02 -5.07119717e-03 -2.42983643e-02  4.95439693e-02\\n  2.77284045e-07  2.32615303e-02  9.45280269e-02  9.06170607e-02\\n -1.80358738e-02  2.95383707e-02  1.59279779e-02  1.23597961e-02\\n  3.11119575e-02 -1.38411103e-02  3.10306321e-03  1.59545932e-02\\n  1.65081769e-03  3.20729450e-03  1.13134542e-02 -1.12848073e-01\\n  7.05596209e-02 -5.21593392e-02  4.21479810e-03 -6.61630630e-02\\n  5.26186191e-02  3.98995690e-02  1.07108384e-01  1.62852146e-02\\n  2.07141843e-02 -4.21812162e-02 -7.78250396e-02  2.90362816e-02\\n -2.66081356e-02  3.01367417e-02  6.37828652e-03  1.45306271e-02\\n -1.53808380e-02  2.20676027e-02  1.13348488e-03  1.55630149e-02\\n  3.71671654e-02  1.16737485e-02  6.34661689e-02 -2.48105098e-02\\n  2.75774393e-02  3.30563262e-02 -4.76676486e-02 -1.98462158e-02\\n -9.19345096e-02  5.39382435e-02 -2.02527437e-02 -3.31044197e-02\\n -3.78920212e-02  2.32102517e-02 -6.03242405e-02  3.18414159e-03\\n -1.48934713e-02  4.78699477e-03  3.27289142e-02  4.25828015e-03\\n -3.94762233e-02  5.03758490e-02 -6.42652735e-02  6.25514099e-03\\n  1.06841570e-03 -4.29078378e-02 -4.26019765e-02 -5.51565327e-02\\n  3.63758914e-02  5.94875179e-02 -2.68581230e-02 -4.46194001e-02\\n  3.33222779e-34  4.35438305e-02 -5.75150698e-02  4.93043773e-02\\n  4.48832363e-02  1.26587134e-02  9.70020657e-04  3.61156873e-02\\n  2.22353041e-02 -2.02208720e-02 -9.39060301e-02 -2.47406252e-02]'},\n",
       " {'page_number': 6,\n",
       "  'sentence_chunk': 'PAD] (padding) —When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or “padded” using the [PAD] token, up to the length of the longest text in the batch. The tokenizer used for GPT models does not need any of these tokens; it only uses an <|endoftext|> token for simplicity. <|endoftext|> is analogous to the [EOS] token. <|endoftext|> is also used for padding. When training on batched inputs, we typically use a mask, meaning we don’t attend to',\n",
       "  'sentence_chunk_size': 570,\n",
       "  'sentence_chunk_word_count': 98,\n",
       "  'sentence_chunk_tokens': 142.5,\n",
       "  'embedding': '[-9.37749865e-04 -9.74952132e-02  1.43881803e-02  2.46285256e-02\\n -1.00569185e-02  4.27761860e-02  4.03229259e-02  2.61852667e-02\\n -3.34735289e-02  1.27062630e-02  1.36708189e-02 -6.60200715e-02\\n  2.13919077e-02  5.23832673e-03  7.71460384e-02 -4.39737178e-02\\n  3.37174423e-02  7.04949582e-03 -6.59763301e-03  2.61514634e-03\\n -4.50188946e-03  8.68840050e-03 -5.05089108e-03  3.35018709e-02\\n -4.86226901e-02 -3.87401246e-02 -3.15627716e-02  1.86153762e-02\\n -1.69150429e-04 -5.21545298e-02 -1.26874521e-02 -2.24023405e-02\\n  3.85236628e-02  1.88263308e-04  2.05964125e-06 -1.94626357e-02\\n -1.12431571e-02  2.17873659e-02 -1.56215737e-02  4.02727574e-02\\n -2.93254643e-03 -2.31628045e-02  3.02798371e-03  2.14005262e-02\\n  2.56920885e-02 -1.59395300e-02  3.96687873e-02  1.04012586e-01\\n -2.17767097e-02  5.75630851e-02  5.35213761e-03 -1.89693365e-02\\n  4.33736630e-02 -7.03703146e-03  8.17501470e-02 -4.67568561e-02\\n  1.08596794e-02 -4.01955955e-02 -8.66096118e-04  6.15690602e-03\\n -3.19359377e-02 -2.30815616e-02  1.32789910e-02  1.47192990e-02\\n  7.94474967e-03  3.32816839e-02  7.18778232e-03 -9.96627565e-03\\n -2.62451340e-02 -4.52996530e-02  3.85109223e-02  6.12732535e-03\\n  3.44186602e-03 -3.29759531e-02  2.69039106e-02 -5.56035079e-02\\n -3.94574143e-02  2.41385102e-02 -3.22268275e-03  7.69644156e-02\\n -2.46742591e-02 -2.06158347e-02  2.74848510e-02 -5.49877137e-02\\n -6.68021291e-02  9.98844802e-02 -7.51761626e-03 -4.96418737e-02\\n -1.76316090e-02 -1.50377676e-02 -2.59195846e-02 -5.76227568e-02\\n -1.57235172e-02  3.22086141e-02  1.74520593e-02  1.43328877e-02\\n -3.34007032e-02 -3.92528214e-02  3.71176451e-02  2.77346540e-02\\n -4.23565023e-02  4.90076020e-02  2.36407295e-02 -3.83133069e-03\\n -2.33681779e-02  6.56827390e-02 -2.35534627e-02  6.00415654e-02\\n -6.59844046e-03 -2.74465363e-02  1.38421152e-02 -4.18470986e-02\\n -6.61842972e-02  9.05640796e-02 -3.67320189e-03 -1.23461168e-02\\n -4.98010330e-02  3.67322303e-02  9.67995264e-03  7.14017442e-05\\n  6.53966963e-02 -5.68923727e-02 -2.09715348e-02 -1.32549452e-02\\n -3.26522440e-02 -1.49280867e-02 -3.07178609e-02 -1.15637539e-03\\n  1.15097116e-03 -1.75038073e-02 -4.88616936e-02 -5.44129172e-04\\n -3.73878181e-02 -3.32660675e-02 -2.50542574e-02  7.13479370e-02\\n  1.41673042e-02  3.55819450e-03 -6.27492294e-02  3.00478153e-02\\n  8.79260711e-03 -4.72761877e-03  1.21011892e-02  1.27574513e-02\\n -1.47846714e-02  3.90767939e-02  1.68187134e-02  2.18277574e-02\\n -1.97408888e-02  4.62352820e-02 -2.96768472e-02  3.63635644e-02\\n -2.10639946e-02 -1.35702109e-02  5.31206541e-02 -3.46301086e-02\\n -2.98405793e-02  4.57365960e-02  5.16038574e-02  4.53815348e-02\\n  2.67575532e-02 -2.71639414e-02 -3.77345271e-02  1.81429703e-02\\n  2.32878104e-02 -1.37152951e-02 -3.61119360e-02 -6.08999282e-03\\n  6.79306835e-02  8.55967030e-03  6.99424446e-02  6.53985813e-02\\n -2.86019538e-02  2.03931984e-02 -1.94885687e-03  1.54734448e-01\\n  5.31119388e-03  1.51070133e-02  2.10907049e-02 -1.83062404e-02\\n  1.43526457e-02  5.36577217e-02 -2.23570783e-02 -2.98326332e-02\\n  1.83840524e-02  3.61235403e-02  7.50485137e-02  4.06926833e-02\\n -9.11450107e-03 -1.95586029e-02 -3.85703295e-02  1.76504776e-02\\n -3.03020645e-02 -7.74240419e-02  4.48479690e-03 -2.82043796e-02\\n -8.63853246e-02  8.01444054e-03 -3.68014676e-03 -9.04613454e-03\\n -9.54535382e-04 -1.57449115e-02  1.96685661e-02  3.01677156e-02\\n  1.13339406e-02 -6.03883527e-04 -7.20730796e-02 -2.84867864e-02\\n -5.95364384e-02  1.81598123e-02 -1.71656106e-02  3.45709771e-02\\n -1.14566777e-02 -5.44527806e-02 -2.52173617e-02  5.19705517e-03\\n -1.31809302e-02  7.34305533e-04 -1.36993155e-02  3.02503929e-02\\n  4.23606578e-03 -7.11279735e-02  2.71857646e-03  5.23431748e-02\\n -1.43265231e-02 -5.65597340e-02 -4.28686999e-02 -7.41299614e-03\\n  3.03814262e-02  1.94846764e-02  1.37992539e-02 -1.42031023e-02\\n  1.90719981e-02  1.10292984e-02  1.30767198e-02 -5.21214493e-02\\n -7.06923604e-02  5.81247658e-02  5.95608018e-02  2.62589678e-02\\n -2.43299524e-03 -2.04409342e-02 -2.83526257e-02 -3.11831087e-02\\n -3.99869569e-02  4.16522584e-04  5.81681207e-02 -4.07614894e-02\\n  5.92442695e-04 -5.84276766e-03  1.09122638e-02 -1.38083948e-02\\n -4.31701215e-03  4.36814874e-03 -4.23760340e-02 -1.28940204e-02\\n  2.03634929e-02  4.81905416e-02 -6.40635043e-02  9.50779766e-03\\n  7.22399279e-02  1.86423380e-02 -2.84055509e-02  6.79930579e-03\\n  1.28042307e-02 -3.05429790e-02 -3.99155542e-02 -9.49266478e-02\\n  6.34019300e-02  2.47635264e-02  3.68682779e-02  1.11992052e-02\\n -4.20307629e-02 -5.66167273e-02  1.56076681e-02 -4.65835519e-02\\n -6.81674182e-02 -8.97633098e-03 -4.20195051e-02  5.31635843e-02\\n  1.41408481e-02  2.18934268e-02  2.11086608e-02  1.90800000e-02\\n -1.06396200e-02 -3.85483401e-03 -3.18513513e-02 -7.55440444e-02\\n -9.35705155e-02  7.09419325e-03  1.85833629e-02  4.63434821e-03\\n  2.03279369e-02 -4.12930548e-02  8.62059463e-03 -2.83996873e-02\\n  3.61540504e-02  2.85505485e-02  4.19214889e-02  6.69780895e-02\\n -3.16605307e-02  5.41046262e-02 -2.01718099e-02  3.52731831e-02\\n -1.05030900e-02  1.05526811e-02 -4.00209278e-02  3.19707245e-02\\n  2.75173895e-02  1.97143555e-02 -2.33716518e-02 -2.80247559e-03\\n -7.71596953e-02  5.44216158e-03  1.62392613e-02  1.86639093e-02\\n  4.91916295e-03 -1.71712413e-02 -4.27654535e-02 -1.38856778e-02\\n  1.29682366e-02 -4.90809754e-02  1.94455087e-02 -6.52958527e-02\\n -2.48624980e-02 -7.74811441e-03 -9.57762171e-03 -1.30330408e-02\\n -3.27285752e-02  2.00142302e-02  2.64421618e-03 -1.08223567e-02\\n -5.94083369e-02  1.45160099e-02  3.08067240e-02 -5.30869626e-02\\n  2.54750792e-02  4.99558588e-03  3.66745144e-02 -1.12466048e-02\\n -3.15549299e-02 -1.90685410e-02  1.92699907e-03  5.15534319e-02\\n  9.43991262e-03 -8.40814319e-04  4.04223613e-02 -3.61749120e-02\\n  7.60242203e-03  1.68206040e-02  2.28363797e-02  8.10506660e-03\\n -4.04169895e-02  2.29456909e-02 -4.67996672e-02 -2.64890939e-02\\n -3.30097298e-03 -5.15878238e-02 -5.80311799e-03 -2.50131432e-02\\n -1.17165968e-03  1.18661979e-02  4.36199382e-02 -5.17066568e-02\\n -2.05626134e-02 -2.52009779e-02  3.26583311e-02 -3.82297933e-02\\n -4.13412154e-02  9.66983661e-03  8.82491171e-02 -4.03000861e-02\\n  5.34625538e-02  1.10897169e-01 -3.49943079e-02 -5.05585521e-02\\n  2.84577883e-03 -4.44957055e-04  7.55150169e-02  8.17457680e-03\\n -4.31937026e-03 -3.04814670e-02  1.16395869e-03  2.03068405e-02\\n  1.64484847e-02  8.04678129e-05  1.68107694e-03 -3.83151956e-02\\n -5.99007644e-02  3.59024890e-02  3.52931470e-02 -1.04185939e-01\\n -4.78011370e-02 -2.02288553e-02 -6.23014290e-03  4.24085334e-02\\n  9.38184559e-02  3.09835970e-02 -2.24139281e-02  3.25422622e-02\\n  4.94590327e-02  3.49170454e-02  1.33177941e-03 -4.69457284e-02\\n -1.25807733e-03  7.35262632e-02  4.33859276e-03 -6.95636198e-02\\n -4.30944934e-02 -3.19355503e-02  8.91936123e-02  8.18931311e-03\\n -8.16750908e-05 -2.79461890e-02 -2.27144398e-02 -2.11986173e-02\\n  6.86479406e-03  2.34547257e-02  1.25770774e-02  2.89349779e-02\\n -4.51489957e-03  2.47644316e-02  1.08162262e-01 -4.68445336e-03\\n -2.11886540e-02 -2.90966444e-02 -2.46995948e-02 -3.75912823e-02\\n  2.39748135e-02  3.94265987e-02 -9.55180824e-03 -4.39243689e-02\\n -4.90352772e-02 -1.12192584e-02  1.04914168e-02  6.31518736e-02\\n -3.70998755e-02 -7.02386647e-02  3.37007281e-04  3.10073514e-02\\n -4.62074615e-02  3.42388637e-02  4.29729670e-02  2.09863554e-03\\n  6.18177317e-02 -2.44482737e-02  1.12982662e-02  1.03002321e-02\\n -4.00847197e-03  9.22197010e-03 -2.06875633e-02 -1.20364420e-01\\n -1.24458689e-02 -2.59521231e-02 -3.18643413e-02 -1.42385364e-02\\n -3.39935347e-03 -6.07897490e-02 -6.26168326e-02 -4.71077813e-03\\n -9.02324542e-03  2.73171328e-02  4.10648249e-02  3.59616168e-02\\n -2.13446766e-02  6.78298324e-02  4.72222455e-03  1.40901785e-02\\n  2.22305376e-02  1.30505124e-02  1.98338926e-02  8.89884215e-03\\n -1.16718728e-02  1.61258280e-02 -7.30280345e-03 -2.66683251e-02\\n -3.29286465e-03  2.13679690e-02  6.54229894e-02 -2.13590208e-02\\n  3.11292950e-02  3.13007012e-02 -1.29676135e-02  3.53569984e-02\\n -2.21640524e-02  5.66854067e-02 -3.28407995e-02  3.12894881e-02\\n -8.99994653e-03  1.76362935e-02 -3.93010229e-02 -2.91806906e-02\\n -3.09228040e-02 -6.37846068e-02 -1.06364153e-02  4.57312874e-02\\n  2.96860412e-02 -2.08756439e-02 -7.26126730e-02  4.43622377e-03\\n  1.20654227e-02 -6.43238286e-03 -3.89397261e-03 -2.54305992e-02\\n -9.60869621e-03 -2.13773716e-02 -3.16733383e-02  6.17445968e-02\\n  3.70895676e-02  3.03590614e-02  9.60902497e-03  4.71348800e-02\\n -3.63447405e-02 -2.37686764e-02 -3.56475487e-02  2.55091283e-02\\n -2.90194806e-02 -1.10499766e-02  4.69226111e-03 -2.24807020e-02\\n -1.37837315e-02 -1.75012532e-03  3.00481939e-03 -1.28463963e-02\\n  4.26001512e-02  5.66556072e-03  6.13614172e-03 -3.96055728e-02\\n -5.97177781e-02  5.16179875e-02  3.74311209e-02  3.23318988e-02\\n -1.63500700e-02 -3.10670454e-02  1.97526924e-02 -2.68483609e-02\\n  5.41417822e-02  2.02089902e-02  2.93874331e-02 -3.05018090e-02\\n  1.53792230e-02 -4.08677151e-03  1.88550744e-02  1.22168968e-02\\n  6.37264166e-04 -6.10800236e-02  1.01464484e-02  2.65302677e-02\\n -3.77142839e-02 -2.97563113e-02  3.25497352e-02 -7.36725563e-03\\n  7.67971575e-02 -1.57979259e-03  6.91267010e-03 -6.10457726e-33\\n -2.70005707e-02 -3.68386060e-02  1.68142933e-02  1.89194828e-02\\n -7.90186226e-02 -2.09075883e-02  3.66067588e-02 -4.18564491e-02\\n  6.04711697e-02  1.72449853e-02 -2.43000910e-02  2.05000630e-03\\n  1.33513063e-02  1.95597969e-02 -7.76236039e-03 -1.01397065e-02\\n  2.69307848e-02 -2.96599679e-02  1.59700960e-02  3.75324748e-02\\n -7.50543410e-03  2.63389200e-02  4.02749591e-02  2.02966873e-02\\n  2.50028409e-02 -7.38147739e-03 -8.33751541e-03 -1.10280598e-02\\n  5.83017292e-03  2.18354128e-02  5.13160252e-04 -2.78568137e-02\\n  1.52999703e-02 -4.74260785e-02 -5.38669154e-03  4.31936271e-02\\n -8.26262869e-03 -3.47196348e-02  3.85649577e-02 -1.06190490e-02\\n -1.81109849e-02 -7.98247382e-02 -2.23035552e-03  1.12262357e-03\\n -7.93217793e-02  9.66230815e-04 -8.11401568e-03 -1.37369484e-02\\n  1.22995805e-02 -9.40560643e-03  3.14359665e-02  6.32224604e-03\\n  1.60689019e-02  8.94837156e-02  3.58435176e-02 -8.48170891e-02\\n  1.34291435e-02  9.17617977e-02 -2.15065423e-02 -1.73004530e-02\\n  7.71284252e-02  3.54764573e-02  2.18449645e-02  1.71777084e-02\\n  2.54829451e-02  1.68616120e-02  9.39987600e-03  9.96261556e-03\\n -2.45566778e-02  9.56569239e-03 -3.81608978e-02 -4.19659577e-02\\n  1.39877084e-03 -4.53313813e-02  8.97915959e-02 -2.24659443e-02\\n -2.58627366e-02  1.24591216e-02 -3.65384258e-02  2.43896879e-02\\n -7.12101720e-03  2.60425825e-02  4.22271378e-02 -7.17105158e-03\\n -1.44569715e-02 -7.85547718e-02  9.66295134e-03 -3.22049111e-02\\n  9.36021004e-03 -1.04383351e-02  6.13468327e-02 -1.15538994e-03\\n -1.16206314e-02  1.26378797e-02 -2.07063323e-03 -2.33561508e-02\\n  8.60355143e-03 -1.49272997e-02  9.73473396e-03 -1.16172526e-03\\n  2.70234677e-03 -3.65486555e-02  2.60158386e-02  1.44227566e-02\\n  3.78781110e-02  7.44857243e-04  5.48616983e-02  1.49010532e-02\\n -7.82110021e-02  2.93569770e-02  6.88577653e-04  4.87038642e-02\\n -1.24015594e-02 -1.76597461e-02  4.99229238e-04 -2.97593866e-02\\n  1.49352392e-02  4.03029658e-02 -1.32786538e-02 -4.29195538e-02\\n  2.83293612e-03 -1.23175886e-02  6.54762378e-03  1.53015703e-02\\n -1.88866239e-02  4.50019129e-02 -7.65847787e-03  4.70042937e-02\\n  6.12492189e-02 -1.21474350e-02  2.13718191e-02  4.51755449e-02\\n  2.68893899e-07  1.31103536e-02  4.79618423e-02  9.95263755e-02\\n -3.69324093e-03  2.94175427e-02  2.06670561e-03  3.31410975e-03\\n  2.46787611e-02 -7.98701216e-03  1.29248714e-02  1.26824304e-02\\n -7.24043278e-03 -1.85486190e-02 -3.57222371e-02 -4.81570885e-02\\n  2.85308510e-02 -3.56006511e-02 -1.86440498e-02 -7.54757002e-02\\n  5.52153587e-02  1.93830580e-04  3.17458734e-02  3.53780901e-03\\n -5.91058843e-03  1.35292793e-02 -5.42390102e-04 -1.26700196e-02\\n -3.65702696e-02  5.34782931e-02 -3.59280035e-03  4.37399372e-02\\n -2.17773691e-02  2.09639724e-02  1.28377704e-02  1.15284501e-02\\n  4.09509018e-02 -9.24853235e-03  4.71096300e-02 -3.51130366e-02\\n -2.37395219e-03  2.42490657e-02  3.54874926e-03 -2.15277374e-02\\n -9.11716595e-02  3.93477790e-02  4.91595604e-02  5.04112467e-02\\n -5.13852276e-02  1.95201095e-02 -3.95863391e-02 -8.47622287e-03\\n -4.09031212e-02  2.32192166e-02  3.90792042e-02  1.04295611e-02\\n -1.14508038e-02  2.11510453e-02 -4.14492190e-02  1.78677477e-02\\n -5.20833880e-02 -5.80376014e-02 -4.21069562e-02 -2.11112667e-02\\n  3.86353373e-03  2.38369312e-02 -5.93636818e-02 -4.98510078e-02\\n  2.97525633e-34  2.18529776e-02 -9.21990722e-02  2.41124723e-02\\n  4.25138101e-02  1.35264704e-02 -2.61163861e-02  4.07895334e-02\\n  2.62088161e-02 -2.41900869e-02 -3.79683711e-02 -3.89611311e-02]'},\n",
       " {'page_number': 7,\n",
       "  'sentence_chunk': 'padded tokens. Thus, the specific token chosen for padding becomes inconsequential. Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token for out-of-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks words down into subword units. Attention in LLMs In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by “soft” weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size. Unlike “hard” weights, which are computed during the backwards training pass, “soft” weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme. Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks.',\n",
       "  'sentence_chunk_size': 1361,\n",
       "  'sentence_chunk_word_count': 205,\n",
       "  'sentence_chunk_tokens': 340.25,\n",
       "  'embedding': '[-3.28065897e-03 -6.43335581e-02 -1.26882056e-02  3.82998288e-02\\n -5.12118228e-02  3.63652483e-02  1.77986063e-02  3.13672656e-03\\n -4.90792878e-02 -1.17476452e-02  1.48134604e-02 -5.71499132e-02\\n  2.37732977e-02 -8.07945384e-04  8.75097141e-02 -1.67424437e-02\\n  5.33821508e-02 -5.17165463e-04  2.51253322e-03 -2.59059086e-03\\n -1.08549548e-02 -2.64773276e-02  1.37986699e-02  4.70632501e-02\\n  7.13764085e-03 -3.06207268e-03 -1.14134764e-02 -1.15365451e-02\\n  9.06950794e-03 -4.41559926e-02 -2.87812613e-02  1.42550450e-02\\n  5.30978478e-03  1.79517232e-02  2.37521249e-06 -1.51586710e-02\\n -2.46728910e-03 -2.64694379e-03  1.78750809e-02  3.13521810e-02\\n  2.48984285e-02 -2.80467495e-02 -3.18497838e-03 -1.80341638e-04\\n  4.62271878e-03 -3.45597113e-03  8.84402245e-02  7.41573051e-02\\n  7.10840803e-03  7.15917051e-02  1.15855481e-03 -4.84084524e-02\\n  1.59350540e-02 -3.71189937e-02  5.16876690e-02 -5.56065589e-02\\n  3.28853801e-02 -5.98999262e-02 -3.77911851e-02  2.55078040e-02\\n -5.61382286e-02 -3.74289486e-03  1.14974137e-02  1.83927845e-02\\n  3.46383862e-02  1.10827200e-02 -3.43072489e-02 -9.62340925e-03\\n -1.65582877e-02  1.96587089e-02 -5.65953739e-02 -1.42864147e-02\\n -3.55526363e-03 -2.85333842e-02  2.17203069e-02 -2.42933836e-02\\n -6.21562228e-02  1.34235863e-02  1.71487741e-02  6.35986179e-02\\n  5.26423194e-02 -2.32482739e-02  3.62849012e-02 -3.20125781e-02\\n -5.51891848e-02  6.21472225e-02 -2.08710530e-03 -2.23134868e-02\\n -2.12533846e-02  5.14051272e-03 -3.59251350e-02 -4.67275940e-02\\n  4.11571451e-02  4.93828729e-02  4.75075319e-02  6.98406622e-03\\n -1.02415925e-03 -3.71666588e-02  3.23779918e-02 -3.76556441e-02\\n  2.11712718e-02  5.11094891e-02 -3.57224941e-02 -8.68868735e-03\\n -5.41099310e-02  7.99447671e-02 -4.11451496e-02  3.16328779e-02\\n -3.23029682e-02  5.67939552e-03  2.20842138e-02 -3.06624565e-02\\n -4.99494076e-02  9.48526487e-02 -1.01932641e-02 -1.74024645e-02\\n -3.63870934e-02  3.77697013e-02 -4.17489093e-03 -3.61615345e-02\\n  1.85354035e-02 -1.61274076e-02 -4.19383198e-02  3.41210999e-02\\n -1.45773226e-02  4.47235554e-02 -3.76504324e-02 -1.64398756e-02\\n  7.59394374e-03 -2.68628653e-02 -5.00381775e-02 -1.30034448e-03\\n -1.12366360e-02 -1.80726442e-02 -3.92811038e-02  9.57325324e-02\\n  1.49473362e-02 -1.65809169e-02 -2.96651945e-02  6.61957264e-03\\n  1.67897379e-03 -1.95544511e-02  1.06500089e-02 -1.25514632e-02\\n -1.30997188e-02  3.93799394e-02 -1.84857100e-02  5.24145272e-03\\n  3.01411888e-03  3.24530900e-02 -5.91275506e-02  4.31743078e-02\\n  3.57198231e-02 -2.99971104e-02 -1.13605214e-02 -9.75670572e-03\\n  1.42177744e-02  3.04067172e-02  1.13020837e-02  6.83854520e-02\\n  3.55748297e-03 -4.78500947e-02 -2.19531730e-02 -5.16900420e-03\\n  1.51802991e-02 -2.42831483e-02 -4.87823002e-02 -4.55306545e-02\\n  1.63151771e-02  9.51147825e-03  4.18456234e-02  4.58191708e-02\\n -1.92907248e-02  1.92880351e-03  6.24420382e-02  7.89981335e-02\\n  4.23480906e-02  1.08017035e-01  3.83673375e-03 -2.33211871e-02\\n -1.50312099e-03  6.74380735e-02 -4.34392877e-02  6.21726960e-02\\n -1.60869863e-03  5.05611859e-03  5.49347289e-02  5.09091467e-03\\n -1.51937897e-03 -3.29601765e-02 -1.12603456e-02  6.82021771e-03\\n  2.48701926e-02 -5.61205931e-02 -2.74107023e-03 -1.90059320e-04\\n -8.03176090e-02  7.58135095e-02 -1.25955511e-03 -6.19039964e-03\\n -1.83287705e-03 -2.17567496e-02 -2.43172888e-02  2.89319064e-02\\n -1.09367780e-02 -2.47750655e-02 -2.80087851e-02 -9.19085741e-03\\n -1.71728358e-02  3.39700617e-02  1.94451641e-02  5.55216186e-02\\n -4.28003073e-02 -4.99766804e-02 -4.83682752e-03  1.05205476e-02\\n -5.85873378e-03  5.20687141e-02 -8.85277614e-02  4.50116247e-02\\n  1.08122220e-03 -5.47290919e-03 -3.33081521e-02  1.21016484e-02\\n  4.92520705e-02 -5.41802794e-02  1.47025986e-02 -1.66923436e-03\\n  2.58104391e-02  2.34372951e-02  2.57104728e-02  3.83085124e-02\\n  5.51848300e-02 -1.45613896e-02  3.32247205e-02  6.28602051e-04\\n -8.71854797e-02  5.07029742e-02  3.93777229e-02 -2.01981422e-03\\n  2.47269887e-02  1.84919778e-02  3.58008617e-03 -5.95241450e-02\\n  6.31292015e-02 -4.36282568e-02  6.09758496e-02 -4.31468524e-02\\n  1.24010688e-03 -7.16072833e-03  4.82369065e-02 -2.97320839e-02\\n  2.15177611e-02 -1.74574498e-02  1.86292063e-02  1.38889393e-02\\n  1.95412748e-02  4.90121637e-03 -4.78544608e-02 -1.58096272e-02\\n  7.53753483e-02  1.83848781e-03 -4.65091458e-03 -7.42519693e-03\\n -1.43394526e-02 -1.84963122e-02 -4.90987450e-02 -5.20742163e-02\\n -8.31171498e-03 -5.77401975e-03  4.95219938e-02  6.03165803e-03\\n -5.16859489e-03  1.62731595e-02 -3.89035419e-03 -5.35977036e-02\\n  2.17603333e-02 -1.09902779e-02  1.68200873e-03  6.51186779e-02\\n -1.12876445e-02  1.58013981e-02 -2.58111451e-02 -3.85037847e-02\\n -4.65043401e-03  4.10776064e-02  2.02377439e-02 -4.15525474e-02\\n -7.89869800e-02 -5.28971441e-02  1.18788062e-02  2.49539446e-02\\n  1.84943806e-02 -9.56437141e-02  1.63740590e-02 -7.84288067e-03\\n  5.67870252e-02  8.23984444e-02  4.63061221e-02  4.63627838e-02\\n -4.15198971e-03 -4.91365185e-03 -1.44581301e-02  2.86799204e-02\\n -5.64832054e-02  4.80040628e-03 -3.36515307e-02  5.37802977e-03\\n -1.97342271e-03  9.77179557e-02 -5.55393891e-03 -2.18250547e-02\\n -8.39389190e-02  4.03836137e-03  8.42815824e-03 -9.21517052e-03\\n  5.12587838e-02 -3.14068375e-03 -7.49684572e-02  2.42900592e-03\\n -2.36867904e-03 -2.90717352e-02  4.50628735e-02 -2.88135484e-02\\n -8.61925422e-04 -3.35239507e-02  5.02386782e-03 -2.31191684e-02\\n -5.89922350e-03  2.11090539e-02  1.69852749e-02 -2.63755079e-02\\n -7.09877461e-02  3.20992991e-02  9.08178464e-03 -3.59763764e-02\\n -1.60888992e-02  4.10640128e-02  1.85260531e-02 -3.90897319e-02\\n -1.34715326e-02  1.13530578e-02  2.01946013e-02  4.77627106e-02\\n  7.83290016e-04  1.48328086e-02  4.17277142e-02 -6.96407724e-03\\n -4.61153947e-02  2.53112372e-02  4.22288813e-02  1.81691013e-02\\n -6.17105104e-02  4.25060131e-02 -4.80994545e-02 -2.20236275e-02\\n -1.50910690e-02  4.93379729e-03  2.46869642e-02 -7.21987709e-03\\n -1.22121181e-02  2.67165434e-02 -1.11787394e-03 -2.17227563e-02\\n -1.64665263e-02  7.59681221e-03  5.75602464e-02 -5.98508753e-02\\n -4.29108553e-02  3.45189571e-02  7.67120868e-02 -4.72265854e-02\\n  1.26691498e-02  9.56213027e-02 -2.94746384e-02 -1.04058124e-01\\n  3.27910110e-02 -5.46051115e-02  9.75793600e-03  4.09056321e-02\\n -4.35942225e-03 -3.12786400e-02 -1.92116238e-02  1.20133813e-02\\n  2.51682498e-03 -3.48187983e-03 -4.68134768e-02  1.38981603e-02\\n -2.95475498e-02  1.79898273e-02  1.63607299e-02 -9.74822491e-02\\n -7.05319345e-02  3.65980831e-03 -1.08490214e-02  2.80084275e-02\\n  3.89088094e-02  2.96125002e-02 -2.98162427e-04  5.62849222e-03\\n  3.57276201e-02 -1.83136261e-03  2.33185440e-02 -1.77533478e-02\\n -3.49326022e-02  1.29231112e-02  6.65355921e-02  8.32712185e-03\\n -6.16484024e-02 -8.91611129e-02  5.67957796e-02 -1.26148080e-02\\n  1.45210857e-02 -2.82069966e-02 -2.30058152e-02  3.15087987e-03\\n -3.20279188e-02  8.43963306e-03  2.11283728e-03 -1.24678193e-02\\n  1.19159240e-02 -3.42375077e-02  1.10262156e-01 -8.79170373e-03\\n -2.44737957e-02 -1.23812603e-02 -3.58906277e-02 -8.16447102e-03\\n  2.92237252e-02  4.82453927e-02 -3.36726718e-02 -5.26389442e-02\\n -5.13135083e-02  1.84180625e-02 -2.20488012e-02 -2.35194117e-02\\n -2.71942485e-02 -1.04627814e-02 -5.22606857e-02 -5.04630283e-02\\n -3.08596101e-02  9.76334698e-03  9.04618017e-03  2.10731160e-02\\n  2.28276495e-02  1.13826571e-02 -3.67958024e-02 -5.53788766e-02\\n  3.34055200e-02  2.55387109e-02 -1.41410399e-02 -7.13597089e-02\\n -2.81449780e-02 -6.31294167e-03 -3.55496928e-02 -1.49663389e-02\\n -8.62339884e-03 -3.72193456e-02 -3.37072760e-02  2.22082715e-02\\n  1.84311625e-02  2.03814134e-02 -2.21277606e-02 -6.60021091e-04\\n -5.63961267e-02  2.06622295e-02 -2.22440735e-02 -2.23273933e-02\\n  2.85751894e-02  2.66863089e-02 -4.14199680e-02  1.05652781e-02\\n  6.77709514e-03  1.30904675e-03 -2.06706231e-03  1.11051081e-02\\n  2.17236523e-02  4.15768325e-02 -1.51275676e-02  3.59493797e-03\\n  8.91576987e-03  5.52861542e-02  1.39223039e-02  2.83368006e-02\\n  6.51297579e-03  4.24365401e-02  4.16147755e-03  3.10006384e-02\\n -4.12123427e-02 -9.08912905e-03  3.12567092e-02  1.02216955e-02\\n -7.99728930e-03 -4.70665060e-02  2.25170632e-04 -2.68264301e-02\\n  9.53102205e-03  5.88156544e-02 -6.30678013e-02  2.70450301e-02\\n -8.49257596e-03  1.39065767e-02 -3.31452675e-02 -4.73176651e-02\\n -2.31342204e-02  1.89040862e-02 -3.90706919e-02  2.30523422e-02\\n  2.80805826e-02  5.11334911e-02  5.11269737e-03  4.29049917e-02\\n -2.80415006e-02  1.04001351e-02 -2.03485508e-02 -1.73747949e-02\\n  3.02876700e-02 -2.27511022e-02  3.69199067e-02  8.99753626e-03\\n -1.37365935e-02 -6.96176756e-03  1.93160423e-03 -1.63698960e-02\\n  1.43552814e-02 -1.06586348e-02  2.42597908e-02 -7.11541949e-03\\n -4.47900593e-02  5.35786860e-02  7.13337399e-03 -1.21486075e-02\\n -1.81674939e-02 -4.04387191e-02  2.52362322e-02  7.74809346e-03\\n  3.56077813e-02  3.73874083e-02  6.87863911e-03 -1.26411142e-02\\n -1.36131095e-02 -3.33304182e-02  3.48340124e-02  5.98038686e-03\\n  2.97502968e-02 -6.45687282e-02  2.03283448e-02  3.41689996e-02\\n -3.56273074e-03 -5.30376248e-02 -1.39744524e-02  3.65523249e-02\\n  2.39526816e-02 -2.51336460e-04 -2.63614673e-03 -7.34214800e-33\\n -1.42442221e-02 -5.56150079e-02 -6.59608608e-03 -1.30599672e-02\\n -4.33878787e-02  1.71561483e-02 -4.19733953e-03 -2.93564200e-02\\n  2.85427291e-02  7.97908381e-03 -1.75058935e-02  2.03116331e-03\\n  8.26388504e-03  2.35118549e-02  2.80148815e-02 -2.45239474e-02\\n -1.63673959e-03 -1.42710225e-03  2.78032031e-02  2.87748631e-02\\n -9.46818572e-03  2.46764645e-02  1.13399059e-01  1.55895129e-02\\n  3.12260780e-02 -4.48393449e-03  2.47282311e-02 -2.54330002e-02\\n -2.22573578e-02  4.35647257e-02 -1.83221716e-02  2.93491525e-03\\n  2.15895642e-02 -1.08747467e-01 -2.88564395e-02  3.66899706e-02\\n -4.79342937e-02 -5.62508926e-02  5.66562228e-02  1.50737893e-02\\n -3.52092683e-02 -6.81875125e-02  6.48439676e-02  2.91948882e-03\\n -2.75132935e-02  2.11324706e-03  4.78808826e-04 -3.17815244e-02\\n -4.38280813e-02 -2.20558629e-03 -3.59474234e-02  8.38608295e-03\\n -1.21371252e-02  6.48154542e-02  4.27242406e-02 -3.72885577e-02\\n -4.23998050e-02  7.38989338e-02 -5.88120893e-02 -1.89175596e-03\\n  4.97205183e-02  9.16346610e-02  1.98545251e-02  3.47347371e-02\\n  2.78676841e-02 -1.57040020e-03  2.99211629e-02  3.13881505e-03\\n -4.11133505e-02  1.97096858e-02 -3.97300301e-03  8.90319608e-03\\n  8.78111050e-02 -7.30146170e-02  1.21402577e-01 -2.03204211e-02\\n -4.72449437e-02 -2.43754648e-02  1.54995744e-03  7.23294765e-02\\n  6.18229173e-02  2.33357847e-02  1.91805214e-02 -4.09556180e-03\\n  4.69711889e-03 -6.25319481e-02 -3.62215862e-02 -5.95237985e-02\\n -2.43638037e-03 -2.52958257e-02  6.81256363e-03  4.24602330e-02\\n -1.28019383e-04 -1.77177787e-02  3.28297168e-02 -7.50746753e-04\\n  1.82378516e-02 -1.01433359e-02  1.11441151e-03 -1.18596700e-03\\n -4.45213988e-02 -8.40533804e-03 -6.56528922e-04 -1.19872633e-02\\n  2.07879208e-02 -2.67024734e-03  4.68190480e-03  3.02699711e-02\\n -4.82181273e-02 -1.79744642e-02 -1.99282560e-02  5.56228822e-03\\n -1.01944776e-02  3.14387046e-02  2.05787960e-02 -5.89851886e-02\\n  1.18147405e-02  3.91975157e-02  6.49916986e-03 -3.76347825e-02\\n -2.71542035e-02  2.22681724e-02  3.19145597e-03  5.09076491e-02\\n -5.81590123e-02  7.56172556e-03 -3.40090171e-02  1.08197376e-01\\n  4.58620638e-02 -3.50988805e-02 -2.99011152e-02  5.25753684e-02\\n  3.25660835e-07  2.82395743e-02  7.00132698e-02  5.74905388e-02\\n  1.03465365e-02  4.51223142e-02  7.64646684e-04  9.96609777e-03\\n  6.99307173e-02  4.00596410e-02 -6.34890748e-03  1.59042422e-02\\n  1.69634465e-02 -1.74467955e-02 -3.15571018e-02 -7.61011839e-02\\n -7.16333184e-03 -4.78122197e-02  1.76035389e-02 -1.84074547e-02\\n  4.61913198e-02 -3.26243136e-03  9.75610837e-02 -2.59924065e-02\\n  1.82542391e-02 -1.37654739e-02 -3.36953811e-02  1.33240432e-03\\n -8.87904689e-03  5.74962683e-02 -3.21922675e-02  8.30278248e-02\\n -6.40278356e-03 -2.00662389e-02  1.05567009e-03  1.75644120e-03\\n  2.15924885e-02  2.46849861e-02  3.93833034e-02 -3.26932594e-02\\n -6.84203506e-02 -8.66641384e-03 -2.69154646e-02  1.29704140e-02\\n -6.66326284e-02  3.43963690e-02 -3.51958685e-02 -4.38013151e-02\\n -9.82358493e-03  2.57770438e-02 -2.23344155e-02 -2.29481682e-02\\n -4.02463134e-03 -2.69768946e-03  1.60688125e-02  1.80657767e-02\\n -1.77059751e-02 -2.46090419e-03 -6.98880255e-02 -3.39500210e-03\\n  1.20139122e-03 -5.80776185e-02 -1.46736987e-02 -7.28869364e-02\\n -3.10399663e-02  3.81251760e-02 -1.66654084e-02 -4.42832373e-02\\n  3.09301790e-34  1.99058764e-02 -5.56866303e-02  2.74243876e-02\\n  4.16939035e-02  1.13413250e-02 -2.33023036e-02 -7.50643993e-03\\n  3.88310067e-02 -3.65845207e-03 -4.89426218e-02 -6.40356913e-02]'},\n",
       " {'page_number': 7,\n",
       "  'sentence_chunk': 'Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state. Transformers In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets. The modern version of the transformer was proposed in the 2017 paper “Attention Is All You Need” by researchers at Google. Transformers were first developed as an improvement over previous architectures for machine translation,but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,audio,multimodal learning, robotics,[9] and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers). Attention Mechanisms Before we dive into the self-attention mechanism at the heart of LLMs, let’s consider the problem with pre-LLM architectures that do not include attention mechanisms.',\n",
       "  'sentence_chunk_size': 1926,\n",
       "  'sentence_chunk_word_count': 277,\n",
       "  'sentence_chunk_tokens': 481.5,\n",
       "  'embedding': '[ 1.18529797e-02  4.83752321e-03 -1.45901237e-02  3.28758843e-02\\n -3.10258362e-02 -1.00410404e-02 -2.15028245e-02 -1.02840066e-02\\n -5.37868887e-02 -5.24385460e-02 -3.38928550e-02 -4.19208370e-02\\n  1.01928795e-02 -2.48766355e-02  4.27839831e-02 -4.97372262e-02\\n  6.01240918e-02 -6.70253066e-03 -2.79400256e-02  2.40977295e-03\\n -1.33810285e-02 -2.17277110e-02  7.28399353e-03  5.69141991e-02\\n -2.09125206e-02  3.74381407e-03 -4.15651686e-03 -1.18574742e-02\\n -1.71483997e-02 -2.04520989e-02 -1.32703912e-02  1.48437833e-02\\n  4.63419966e-03  6.25935793e-02  2.21496998e-06 -4.14193831e-02\\n  1.95709653e-02 -1.95090175e-02  1.30115785e-02  1.38363428e-02\\n  1.24578113e-02 -1.31828347e-02 -7.83062261e-03 -6.64476817e-03\\n -1.50993643e-02 -1.21712992e-02  5.60807884e-02  8.21160227e-02\\n  3.33318189e-02  6.65704459e-02  8.00027512e-03 -5.29589988e-02\\n  1.99189447e-02 -2.32108720e-02  8.43139086e-03 -4.87387218e-02\\n  1.73184276e-02 -1.07042432e-01 -1.85585953e-02  2.04478428e-02\\n -5.63038811e-02  1.39867216e-02  2.54446696e-02  8.88883881e-03\\n  1.81858074e-02  5.40063046e-02 -3.64518538e-02 -1.10578556e-02\\n -2.91224346e-02  3.07757799e-02 -5.36325835e-02 -4.91498457e-03\\n  1.51960750e-03 -1.00613376e-02  4.31794254e-03 -2.11599097e-02\\n -4.18571159e-02  4.14672457e-02  4.14074287e-02  2.66446862e-02\\n  1.12791173e-02 -1.51200052e-02  2.38771625e-02 -9.78617091e-03\\n -5.04116341e-02  4.22360152e-02  1.22482711e-02 -3.51956114e-02\\n -7.70387612e-03  7.95197859e-03  8.15728866e-03 -5.52614778e-02\\n  2.87093921e-03  7.17407763e-02  4.12496412e-03  7.82658532e-03\\n -7.68127805e-03 -4.51233052e-02  2.03069001e-02 -4.61366996e-02\\n  1.42762130e-02  3.29691209e-02 -6.52475506e-02 -9.52375866e-03\\n -3.38697247e-02  6.68987110e-02 -7.99736902e-02  5.66759109e-02\\n -3.23411077e-02 -7.92406220e-03  1.61009710e-02 -2.63781901e-02\\n -3.24558392e-02  4.53229062e-02 -2.93329023e-02 -2.31635235e-02\\n -4.58781049e-02  4.90844250e-02  2.88339257e-02 -4.77124080e-02\\n -3.68851237e-03  2.42833290e-02 -3.19253393e-02  2.75506545e-02\\n  1.35529123e-03  6.02843314e-02 -1.78782735e-02 -8.90134461e-03\\n  1.08191548e-02 -5.03625050e-02 -4.90767136e-02  2.61442140e-02\\n  1.10356882e-02 -3.89662012e-02 -4.08325018e-03  7.31869563e-02\\n  2.57472135e-03 -4.50036041e-02 -7.06256740e-03  1.37668736e-02\\n -4.66486812e-02 -5.59707955e-02  4.24999557e-02 -1.95087679e-02\\n -4.50948579e-03  1.97815411e-02 -3.21582071e-02  7.36253569e-03\\n  2.28872001e-02  2.69825161e-02 -3.63943502e-02  2.80037504e-02\\n  4.69745621e-02 -1.15882177e-02  1.66909327e-03 -6.01948239e-03\\n  2.54271179e-02  2.18694285e-02  1.73414592e-02  3.21663208e-02\\n -7.15074502e-03 -2.31597424e-02  4.47918056e-03  2.59136148e-02\\n  1.23379538e-02 -1.77843571e-02 -1.51053607e-03 -2.28927378e-02\\n -1.68442428e-02  5.08655980e-02  3.92083786e-02  4.91023846e-02\\n  2.69529340e-03 -8.77895765e-03  8.27829540e-02  3.59824635e-02\\n  3.23899910e-02  7.49293640e-02  1.39974011e-02 -3.94738326e-03\\n -3.55780404e-03  4.69112471e-02 -6.60795495e-02  4.75785360e-02\\n  2.89945919e-02  9.02563985e-03 -1.25262002e-03  3.17002386e-02\\n -1.61903743e-02 -3.71132456e-02  8.17243103e-03  9.27388016e-03\\n -2.71924380e-02 -9.32486914e-03  3.69275622e-02  2.54580975e-02\\n -7.08856285e-02  1.09394871e-01 -1.03525314e-02 -5.75764813e-02\\n  1.74888801e-02 -4.88612391e-02 -2.98182294e-02  1.79357026e-02\\n  4.58175968e-03 -5.35242036e-02 -3.58355455e-02 -1.00185387e-02\\n  7.42711779e-03  5.52281439e-02 -9.61362943e-03  5.89577071e-02\\n  6.07752288e-03 -6.16988679e-03  1.85826141e-03 -2.64489073e-02\\n -3.92480753e-03  2.82054115e-03 -2.69683357e-02  1.89554654e-02\\n -9.74537991e-03  8.14035651e-04  5.42742433e-03  4.89809327e-02\\n  1.43399052e-02 -4.62121069e-02  1.17812781e-02  2.18020659e-02\\n  6.25610678e-03  1.80465579e-02  8.76187161e-03  2.75670718e-02\\n  6.90407231e-02 -4.60628271e-02 -1.05957957e-02  4.95651439e-02\\n -9.62582827e-02  2.53789444e-02  3.80844399e-02 -3.34065966e-02\\n  5.27873076e-03  4.58554029e-02 -8.40566866e-03 -4.50822450e-02\\n  4.75525670e-02 -4.10299003e-02  8.91597122e-02 -1.37216058e-02\\n  2.20063459e-02 -6.01527886e-03  5.97589463e-02 -1.96487531e-02\\n  2.65867878e-02 -3.00773121e-02 -1.76948328e-02 -7.31799006e-03\\n -2.45299004e-03 -2.02329420e-02 -1.14684098e-03 -2.73465663e-02\\n  3.10180858e-02 -2.50969958e-02  1.11468136e-02  2.74884533e-02\\n -2.73263976e-02 -3.74385975e-02 -3.99133451e-02 -5.15115112e-02\\n -1.37388995e-02 -3.86968590e-02  2.11761538e-02  1.13322549e-02\\n -8.46005604e-03  1.72819067e-02  1.18344520e-04 -2.38808747e-02\\n -1.34273584e-03 -1.95619045e-03  3.12765352e-02  8.24031979e-02\\n -4.18419484e-04 -1.52358450e-02 -4.44857515e-02 -2.26587337e-02\\n -5.26176393e-03  5.69175184e-02  2.08412874e-02 -6.02378452e-04\\n -5.72012439e-02 -2.20642723e-02  2.93902587e-02  2.58888490e-02\\n -1.26150269e-02 -6.82659447e-02  6.71131362e-04  1.36931119e-02\\n  9.07974541e-02  6.82252049e-02  4.09009643e-02  4.02887911e-02\\n  1.34943221e-02 -3.75419557e-02 -7.91340321e-03  2.56782603e-02\\n -6.59294054e-02 -2.78033838e-02 -5.83159961e-02  2.29982403e-03\\n  2.58734934e-02  8.83055925e-02 -9.32237599e-03 -2.13927515e-02\\n -2.85444781e-02 -2.16001887e-02  3.97324236e-03 -1.76473893e-02\\n  4.07370888e-02 -2.82895789e-02 -7.06933960e-02 -2.27239056e-04\\n -5.71211334e-03 -5.10844868e-03  3.29093710e-02 -3.20573226e-02\\n -1.41204260e-02 -1.19775953e-02  4.83541116e-02 -5.89548424e-02\\n  3.27947251e-02  2.69855466e-02  3.86199099e-03 -5.20590460e-03\\n -5.59669547e-02 -3.14362496e-02  1.03621297e-02 -1.75916720e-02\\n  6.34993846e-03  1.05149038e-01  2.39251684e-02 -2.22432539e-02\\n  1.80021278e-03 -2.83091739e-02 -4.70683910e-02 -5.61658619e-03\\n -1.28179714e-02 -6.93905586e-03  5.10117114e-02 -3.18703279e-02\\n -9.03418958e-02 -3.18359099e-02  3.96311507e-02  4.02946733e-02\\n -2.34021470e-02  1.20282313e-03 -2.85653379e-02  2.96897963e-02\\n  1.93863537e-03  4.82233204e-02  4.61413115e-02 -1.80560052e-02\\n  4.34784219e-03  6.10675141e-02  3.82764935e-02 -3.35837007e-02\\n -3.20192762e-02 -2.34127548e-02  6.33938015e-02 -8.26833397e-02\\n -2.75839604e-02  2.94266064e-02  4.63730469e-02 -7.65777156e-02\\n -2.03108210e-02  7.78265893e-02 -2.89500784e-02 -3.96798700e-02\\n  2.07141191e-02 -2.22876016e-02 -9.82108992e-03  4.29307409e-02\\n  1.86877605e-02 -5.68960197e-02 -1.74092110e-02  1.05950730e-02\\n -5.96449012e-03  2.27189697e-02 -2.92089414e-02  2.12843204e-03\\n -6.72416016e-02  2.61149444e-02 -1.76525414e-02 -8.87526199e-02\\n -1.04555236e-02  1.69767197e-02 -1.55764762e-02  1.43961934e-02\\n  5.03186472e-02  1.95905752e-02 -3.03753410e-02 -4.65526544e-02\\n  3.02530359e-02 -2.83217374e-02  2.04478558e-02  1.41361030e-03\\n -6.80135787e-02 -1.08683237e-03  7.58493245e-02  4.02150303e-02\\n -3.90969291e-02 -5.01303300e-02  3.80812511e-02 -1.47644030e-02\\n  3.68451253e-02  3.43670836e-03  3.71643491e-02 -4.24450934e-02\\n -8.91927572e-04  8.15152377e-03  1.08807227e-02 -2.10869662e-03\\n -1.49924271e-02 -1.17184911e-02  7.14317337e-02 -7.12586707e-03\\n -3.36449146e-02 -2.47636922e-02  9.54047404e-03 -1.81890987e-02\\n  3.21243219e-02  4.24329713e-02 -3.02827395e-02 -4.38115373e-02\\n  1.49690779e-04  2.72866040e-02 -4.67703268e-02 -6.61370531e-02\\n  4.71336395e-03  3.57549712e-02 -7.14241639e-02 -5.98432347e-02\\n -2.43665706e-02 -1.14498977e-02 -3.36968452e-02  2.00400557e-02\\n -4.34767175e-03  1.03041045e-02  1.31535775e-03 -4.10222672e-02\\n  3.91757339e-02  4.52920655e-03 -1.80954970e-02 -9.66670215e-02\\n  1.37862749e-03  2.11786255e-02 -5.40074967e-02 -1.25776203e-02\\n -6.95416657e-03 -8.17318633e-02 -3.09734587e-02  8.11903253e-02\\n  3.15910876e-02  6.19561598e-03  3.69827077e-02  1.03923341e-03\\n -4.56866510e-02  3.72095443e-02  1.81469414e-02 -2.13421956e-02\\n  2.04728432e-02  3.96038778e-02 -1.88854467e-02  5.09160338e-03\\n  1.96063109e-02 -2.45429073e-02  4.59094681e-02  4.19918038e-02\\n  2.01666821e-02  3.70994397e-02 -2.83970609e-02  1.22361286e-02\\n -1.48227438e-02  1.07184269e-01 -1.27859146e-03  5.17847650e-02\\n  5.62252663e-03  2.23358367e-02 -2.58311932e-03  1.73113961e-02\\n -1.56869013e-02 -1.49092386e-02 -5.03750332e-03  1.66877769e-02\\n -4.29431908e-03  2.87956093e-02  1.58005382e-03 -5.46510145e-03\\n -3.26959565e-02  7.46111944e-02 -3.04377433e-02 -3.59800900e-03\\n  6.30153529e-03  4.20511775e-02 -2.03085714e-03 -1.04443673e-02\\n  2.72410060e-03  3.05663552e-02 -3.58060561e-03  1.83566771e-02\\n  3.79277728e-02 -4.05440200e-03 -1.71066225e-02  2.62720957e-02\\n -7.99717531e-02  9.13002342e-03  1.22970659e-02 -1.90881565e-02\\n  4.97907996e-02 -3.33688222e-02  3.94040532e-02  1.01603549e-02\\n -7.82335084e-03 -4.56011854e-03 -5.70326438e-03 -1.79596432e-02\\n -7.67689617e-03 -1.34024862e-02  1.92040694e-03 -7.39771780e-03\\n  4.23764391e-03  4.37959507e-02 -6.04741182e-03  8.06682091e-03\\n  3.97942327e-02  5.17801149e-04 -7.73758767e-03 -3.28988731e-02\\n -2.58987118e-03 -1.05626937e-02 -2.87896837e-03 -1.19533706e-02\\n -6.07909337e-02 -1.68405082e-02  7.16131851e-02 -2.25275680e-02\\n -8.69433396e-03 -5.90401888e-02  2.61940863e-02  5.35177886e-02\\n  2.25392040e-02 -6.09872192e-02 -2.22880131e-04  5.21171577e-02\\n  2.89245807e-02 -1.77650545e-02  4.67217248e-03 -6.75668349e-33\\n  1.71459094e-02 -2.81077623e-02  8.44538957e-03 -1.01228608e-02\\n -6.86863288e-02  3.07068462e-03 -2.36472860e-02 -1.91959366e-02\\n -1.22462446e-02  8.11781082e-03 -5.11406846e-02  3.33080092e-03\\n  7.93227088e-03  4.73974310e-02  3.23575698e-02 -6.68347850e-02\\n  5.69214374e-02  5.35120722e-03 -9.22522787e-03  2.28365380e-02\\n  1.88176073e-02  6.27252162e-02  1.11646183e-01  1.31068574e-02\\n -1.36923988e-03 -2.16705333e-02  5.33540174e-03  1.07394895e-02\\n -1.46511076e-02  3.51908356e-02 -1.42149283e-02  5.65692894e-02\\n  1.23963561e-02 -9.36990678e-02 -3.68388221e-02  5.47734201e-02\\n -9.36695486e-02 -2.70169768e-02  1.90427080e-02  4.47476842e-02\\n -5.97252659e-02 -4.20210287e-02  9.42961127e-02 -2.23936117e-03\\n -2.18571704e-02 -1.23193879e-02  2.57058330e-02 -6.51755603e-03\\n -3.92664708e-02 -2.34807488e-02 -6.86835647e-02 -1.20549537e-02\\n -9.01315361e-03 -8.09491496e-04  1.34069063e-02  1.51017858e-02\\n -1.05428314e-02  7.45470375e-02 -9.79808718e-02  1.16627784e-02\\n  2.60665268e-02  6.75556436e-02  2.88080461e-02  5.23199961e-02\\n  1.88179489e-03  1.84150599e-02  5.17249256e-02 -4.10360284e-03\\n -6.44809157e-02 -3.05803437e-02  3.61585505e-02  3.82349230e-02\\n  4.96101566e-02 -2.87157204e-02  5.20016924e-02 -2.51661316e-02\\n -4.64828275e-02 -5.04453145e-02 -2.70535070e-02  7.20010474e-02\\n  6.19782098e-02 -8.52782931e-03 -4.39906819e-03 -1.12805422e-02\\n -6.17477577e-03  5.54706994e-03 -3.19663920e-02 -1.02042714e-02\\n -3.55307851e-03  6.19509164e-03  9.97097790e-03  2.23517530e-02\\n  1.22078732e-02 -1.47457151e-02  2.53513698e-02  8.19770340e-03\\n  4.31089709e-03 -2.78412621e-03 -9.77761578e-03  1.07053882e-02\\n -3.78594622e-02  2.26891302e-02 -1.65217128e-02  9.45219863e-03\\n  1.07771177e-02 -6.35491125e-03  2.82985028e-02  5.14571890e-02\\n -6.09783754e-02 -1.93433855e-02 -3.85080688e-02  1.76063310e-02\\n -1.17605524e-02 -1.48851452e-02  2.93137562e-02  6.50466536e-05\\n -1.85389165e-02  6.51456416e-02  1.40799172e-02  1.26637453e-02\\n -3.63933630e-02  3.65650579e-02  1.92069709e-02  2.94539891e-02\\n -3.11251096e-02 -2.18922927e-04 -3.78241464e-02  7.13351890e-02\\n  1.76370665e-02 -1.00074776e-01 -3.44732963e-02  8.40924401e-03\\n  3.09069293e-07  4.42702211e-02  8.26867446e-02  5.67672960e-02\\n -1.28676705e-02  3.00796423e-02  1.44064687e-02  2.07150355e-02\\n  3.70368063e-02  4.03734902e-03  2.40022317e-02  1.18941711e-02\\n  3.60926278e-02  7.37376558e-03 -9.44205653e-03 -8.50003734e-02\\n -2.14860663e-02 -1.10596694e-01  2.55681220e-02 -2.03914046e-02\\n  1.37020517e-02  1.27346953e-02  1.19026877e-01 -3.71371373e-03\\n -1.34548200e-02 -1.97173320e-02 -1.52878892e-02  3.22157592e-02\\n -2.50743050e-02  2.42781192e-02  5.09189907e-03  3.10401861e-02\\n  1.32311729e-03 -1.46313319e-02 -2.11306885e-02 -4.74919612e-03\\n  2.71136314e-02 -3.34609896e-02  1.82410311e-02  3.09753959e-04\\n -6.74619302e-02 -5.20107523e-03  6.02501910e-03  3.73303029e-03\\n -4.04605418e-02  5.32055907e-02 -1.20174408e-01 -4.17100154e-02\\n -2.10172012e-02 -3.06111248e-03 -4.44175303e-03  1.35454265e-02\\n -2.17473321e-03 -5.83887100e-03 -9.77159827e-04  9.50425398e-03\\n -1.49043445e-02 -3.74628557e-03 -1.08406372e-01 -2.78340504e-02\\n  2.72655860e-03 -5.19589707e-02  1.25002740e-02 -5.39487377e-02\\n -3.81720625e-02  3.92282978e-02 -4.38491162e-03 -4.23327237e-02\\n  2.70892926e-34  2.42378470e-02 -3.00700925e-02  3.98077909e-03\\n  4.79425266e-02 -7.01565016e-03 -7.65753072e-03  2.79445406e-02\\n  7.13884179e-03  8.51583853e-03 -4.14606035e-02 -4.31307927e-02]'},\n",
       " {'page_number': 7,\n",
       "  'sentence_chunk': 'Suppose we want to develop a language translation model that translates text from one language into another. We can’t simply translate a text word by word due to the grammatical structures in the source and target language. To address this problem, it is common to use a deep neural network with two submodules, an encoder and a decoder. The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text.',\n",
       "  'sentence_chunk_size': 460,\n",
       "  'sentence_chunk_word_count': 81,\n",
       "  'sentence_chunk_tokens': 115.0,\n",
       "  'embedding': '[ 3.91137088e-03  7.79452547e-02  7.20735453e-03  8.51027295e-02\\n -3.96391936e-02  1.87129769e-02  2.93527003e-02 -2.06745639e-02\\n -7.81520382e-02 -7.71523407e-03 -2.56101657e-02 -5.93048446e-02\\n  1.44688366e-02  1.73291974e-02  1.22149447e-02 -4.92437109e-02\\n  2.08908971e-02 -3.70463990e-02 -4.50799353e-02  3.59260663e-03\\n -2.07963884e-02 -2.46157451e-03  2.49387063e-02  2.23479997e-02\\n  1.77648924e-02 -1.38387028e-02 -2.00134274e-02  6.23020120e-02\\n  7.52169313e-03  3.60843539e-02 -5.78600634e-03 -2.09865179e-02\\n  2.98563112e-02  1.02306731e-01  1.87860485e-06  1.64539311e-02\\n -2.50371713e-02 -3.52306548e-03 -1.00687044e-02  2.62369160e-02\\n  5.27040027e-02 -7.91199356e-02 -3.64616588e-02  1.33855334e-02\\n -1.96381621e-02 -2.61320677e-02  4.91598509e-02  6.10898715e-03\\n  6.66385740e-02  8.60271454e-02 -2.54539642e-02 -6.63289204e-02\\n  2.87732538e-02 -3.91638614e-02 -2.87595931e-02 -6.50699958e-02\\n  8.44852254e-03 -2.62066890e-02 -6.89409450e-02  1.70498639e-02\\n -4.21101600e-02  6.96160719e-02  4.58294228e-02 -1.23749441e-02\\n  1.53643247e-02 -2.62787603e-02  3.81833352e-02 -7.16613904e-02\\n  7.70833157e-03  3.59551422e-02 -7.22811967e-02  1.11964801e-02\\n -3.51259373e-02 -3.01305745e-02 -2.00869702e-02 -8.30526743e-03\\n -3.01028248e-02  4.98272385e-03  2.15823017e-02  2.73220893e-02\\n  1.65563934e-02  2.61372942e-02  3.70977744e-02 -1.00646596e-02\\n -7.62878656e-02  3.21274437e-02  1.15606422e-02 -8.83690547e-03\\n -2.10939869e-02  2.89235879e-02  3.64859663e-02 -2.81645562e-02\\n  8.10466334e-02  2.94580329e-02  1.80946644e-02  1.22753652e-02\\n -3.63789946e-02 -7.59403929e-02 -4.50591147e-02 -8.49229991e-02\\n  2.92151468e-04  4.92549874e-02 -2.78531648e-02  2.51099076e-02\\n -2.20919494e-02  5.55795133e-02 -6.43246993e-02  1.59721524e-02\\n -7.52727464e-02 -6.24636747e-02  2.58301361e-03 -1.21461824e-02\\n -5.82345836e-02 -1.00001078e-02 -4.68262620e-02 -2.74725072e-02\\n -2.60566566e-02 -2.20434129e-04  2.76077110e-02  9.20965336e-03\\n -2.36141887e-02 -1.39579903e-02 -8.67390856e-02 -1.27506014e-02\\n  1.46846625e-03  7.67413080e-02 -1.51115954e-02 -7.21153570e-03\\n  7.66842626e-03 -4.82020862e-02  1.83757860e-04  5.09641692e-03\\n  2.02841423e-02 -3.79516296e-02 -9.19020642e-03  4.88877967e-02\\n -2.69445628e-02 -9.31054819e-03 -2.15331092e-02  4.37255669e-03\\n  5.68618998e-03 -4.57644500e-02  1.87917296e-02 -6.29710928e-02\\n  5.61798587e-02 -1.71621586e-03  8.84256139e-03 -6.64001927e-02\\n -5.92715526e-03  1.96186192e-02 -3.31545994e-02  7.14783147e-02\\n  6.76687108e-04 -2.20500734e-02  3.26613970e-02 -8.30391981e-03\\n  4.30718809e-02  3.55996080e-02  3.92775722e-02  4.96721379e-02\\n  1.64263602e-02 -2.03145538e-02 -2.67217848e-02  6.11729622e-02\\n  1.56301179e-03 -1.20604318e-02 -4.18218076e-02 -2.75063552e-02\\n  3.00273998e-03  4.24390584e-02  3.19803087e-03  5.79237193e-02\\n -2.38717254e-02  3.48245278e-02  4.21219505e-02  3.35545242e-02\\n  4.79381420e-02  9.90041867e-02  4.70793545e-02  1.85696352e-02\\n -1.37559501e-02  3.65342535e-02  2.76172198e-02  2.27942094e-02\\n  3.56411524e-02 -3.77219953e-02  8.91598221e-03  5.14380373e-02\\n -3.10640018e-02 -5.02287149e-02  3.10647544e-02 -2.39160787e-02\\n  3.53818685e-02 -2.67960839e-02  4.48889695e-02  2.24835407e-02\\n -6.70892596e-02  2.28746105e-02 -1.05342790e-02 -4.55148332e-02\\n  6.73045963e-02 -1.81250386e-02 -7.48655424e-02  5.93744889e-02\\n  3.31146978e-02  1.44233648e-02  8.25023837e-03  3.65396409e-04\\n  6.79022260e-03  5.28448299e-02  2.69485284e-02 -6.16964092e-03\\n  3.63724567e-02 -6.14271984e-02 -9.25275870e-03  1.70141049e-02\\n  9.20063257e-03  1.38418088e-02 -4.13987488e-02  4.34432551e-02\\n  4.07161564e-02  1.54867824e-02 -5.26306666e-02  1.96378529e-02\\n -1.70389116e-02 -2.79159863e-02  3.44660394e-02  2.88490504e-02\\n -4.22914606e-03  2.79994216e-02  1.32608730e-02 -5.94494771e-03\\n  2.53959186e-02 -6.99742064e-02 -3.88830639e-02  2.68435460e-02\\n -6.58817366e-02 -3.18418443e-02 -1.25798932e-03 -8.51236656e-02\\n  3.65058891e-02  1.73325427e-02 -3.12980749e-02 -6.44431114e-02\\n  1.82500842e-03 -2.27618851e-02  1.75272878e-02 -1.07702604e-02\\n  1.73192099e-02 -2.70441752e-02  1.09895822e-02 -2.57527977e-02\\n  5.85761014e-03  2.43629385e-02 -4.40592598e-03  2.27978528e-02\\n -1.23756481e-02  1.18153505e-02  1.42881915e-03 -5.26012033e-02\\n  6.68243840e-02  1.94452947e-03  3.50903124e-02  2.45771371e-02\\n  1.73336770e-02  2.24076696e-02 -4.36176918e-02 -7.78645556e-03\\n  1.05813686e-02 -3.20747979e-02  2.41080411e-02  4.36320156e-02\\n  5.92735857e-02  3.91213559e-02  2.04121857e-03 -2.71409396e-02\\n  4.90555651e-02 -1.15650445e-02 -1.85470078e-02  2.50381865e-02\\n  2.02568565e-02 -6.42714137e-03 -3.06466781e-02 -4.82670851e-02\\n  4.13555615e-02  7.09340796e-02  2.66727116e-02 -4.03094813e-02\\n -6.70348406e-02  4.33090236e-03  1.93723645e-02  4.07620035e-02\\n  3.54786916e-03 -8.60444829e-02 -1.66624524e-02 -4.42755502e-03\\n -1.40309697e-02  4.64789420e-02 -2.36397758e-02  4.16993909e-02\\n  2.54413560e-02  2.02616584e-02 -4.06093039e-02 -1.10213431e-02\\n -1.22774281e-02 -4.06357385e-02 -4.07892428e-02 -2.59109624e-02\\n -3.79301496e-02  8.97738189e-02 -2.53057964e-02 -5.15207299e-04\\n -3.66974920e-02 -1.64078306e-02 -3.69517356e-02 -1.80183630e-02\\n -5.91700384e-03 -1.94530357e-02 -1.97290406e-02  6.46518618e-02\\n -1.22703668e-02 -1.94937456e-02  3.71147878e-02 -2.75075808e-02\\n -1.06093427e-02 -3.02565899e-02 -4.98240162e-03 -2.17793360e-02\\n  1.87594462e-02  1.90187022e-02  2.52974574e-02 -3.66649404e-02\\n -4.76305336e-02 -2.87727267e-03  3.44440378e-02  4.99051297e-03\\n -1.11671574e-02  4.94777448e-02 -5.30678555e-02 -8.05299357e-03\\n -1.51368501e-02 -8.35867599e-03 -3.34497355e-02  3.75523083e-02\\n -4.00953973e-03  8.35705735e-03  1.87614094e-02 -5.22696972e-03\\n -5.22811711e-02 -1.70551222e-02  1.81155279e-02  7.36484677e-02\\n -3.99460224e-03  1.62904449e-02 -5.21075688e-02  2.67703608e-02\\n -2.37846021e-02 -6.90766750e-03 -1.56794935e-02 -5.19194305e-02\\n  1.86073259e-02 -9.97640018e-05 -1.56034548e-02  7.67468428e-03\\n -5.46500869e-02 -1.46660712e-02  5.16329184e-02  1.87354013e-02\\n -1.19423280e-02 -5.24747046e-03  3.53664532e-02 -4.81455475e-02\\n -3.18150707e-02  8.34739655e-02  3.80131067e-03 -8.64196271e-02\\n -1.31415753e-02 -2.63157021e-02  2.16556136e-02  6.24620775e-03\\n  1.91759374e-02 -7.29177371e-02 -2.27588117e-02  1.09978020e-02\\n  1.74267553e-02  3.99876088e-02  1.71216913e-02 -3.47421010e-04\\n -4.22977954e-02 -5.78749925e-03  4.64831516e-02 -5.15452959e-02\\n -6.73882142e-02  1.18590072e-02 -4.28678021e-02 -1.71890575e-02\\n  2.72078440e-02  9.40965116e-03  1.95413027e-02 -3.31609920e-02\\n  6.16184343e-03  1.54631399e-02 -5.75805269e-02 -3.33073772e-02\\n -6.53756410e-02 -2.58294642e-02  8.23389888e-02  2.31389422e-02\\n -2.26979777e-02  9.64074023e-03  2.77186707e-02 -3.88267636e-02\\n -1.35601331e-02 -3.20945255e-04  1.51313813e-02 -4.45756176e-03\\n -2.94080619e-02  1.90256275e-02  2.42719296e-02 -6.78844154e-02\\n  1.78788006e-02 -1.27150426e-02  4.68476526e-02  4.30352986e-03\\n -1.39299426e-02  1.42490724e-02 -3.73308957e-02  1.83573421e-02\\n  2.90127732e-02  4.01448570e-02 -4.66392934e-02 -2.65640728e-02\\n -3.38418074e-02  1.58576900e-03 -7.27955848e-02 -3.73633914e-02\\n -9.00997431e-04 -7.62149785e-03 -5.77320233e-02 -7.06292763e-02\\n -3.68282348e-02  1.56886093e-02  3.31732035e-02  4.05941792e-02\\n -1.58836637e-02  6.45248666e-02 -3.78729887e-02 -2.75077596e-02\\n -2.90895021e-03 -1.90351140e-02  3.24676782e-02 -1.67704392e-02\\n  1.02960528e-03  4.67030257e-02 -6.71140943e-03  4.65531182e-03\\n  1.66894849e-02 -6.75493777e-02 -3.82679254e-02  9.29244421e-03\\n  1.32308435e-02  2.76880544e-02  7.04958737e-02 -2.46325731e-02\\n -6.66329861e-02  1.81130460e-03 -1.41260279e-02 -1.32291391e-02\\n -1.10232094e-02  4.27015647e-02 -4.02763151e-02 -1.91629063e-02\\n  4.66156043e-02 -1.20939771e-02 -4.85465862e-04 -2.18859664e-03\\n  1.35584446e-02 -1.94152119e-03 -6.10246100e-02  3.05796806e-02\\n  2.46826606e-03  3.48810595e-03  1.66537222e-02  8.76201037e-03\\n -3.59594040e-02  3.37771289e-02  9.42982361e-03  2.18578447e-02\\n -7.67522976e-02  5.01778768e-03  6.84360787e-02  1.24959964e-02\\n  1.75233856e-02  2.38983817e-02  2.30006650e-02 -4.92349267e-03\\n  1.19283265e-02  2.64267717e-02 -3.10566090e-03  5.31812236e-02\\n  3.02924477e-02  3.39411758e-02 -2.11986396e-02  2.82016229e-02\\n -7.33973598e-03  2.40423195e-02 -4.37953696e-02 -1.51201421e-02\\n  2.67469417e-02  5.60370274e-02 -6.57589408e-04  5.57935517e-03\\n -1.15309628e-02 -4.05165106e-02 -2.16666218e-02  1.54577829e-02\\n  8.81200433e-02 -2.81956838e-03  8.12716112e-02 -1.13708060e-02\\n -8.34335480e-03 -5.36758173e-03  4.95176427e-02 -1.21238893e-02\\n -2.77795438e-02 -2.64300033e-03 -5.97934891e-03  2.36817151e-02\\n -1.66274663e-02  3.78269590e-02  1.58024505e-02  2.91544646e-02\\n -1.10866847e-02  4.87683667e-03 -4.39850846e-03 -1.16023431e-02\\n  2.53817462e-03 -5.08238701e-03 -3.91282439e-02 -2.07203235e-02\\n  3.08566783e-02 -5.05104437e-02 -2.24911831e-02  2.55146567e-02\\n  2.37696078e-02 -3.08891255e-02  1.90386847e-02  6.92614391e-02\\n -3.36806215e-02  4.96028282e-04 -1.79821271e-02  3.60485055e-02\\n  1.76621433e-02 -1.71118807e-02 -4.68388945e-03 -5.80945361e-33\\n -4.36170846e-02 -3.85457580e-03 -6.08222326e-03  8.71418230e-03\\n -5.46852499e-02 -2.48958860e-02 -2.00160630e-02 -2.20708437e-02\\n  8.06053821e-03  6.15268713e-03 -4.27755946e-03 -4.36669029e-03\\n  3.36633585e-02  4.04765941e-02 -3.16799362e-03 -1.10281873e-02\\n  7.02631846e-02 -1.51876463e-02  1.17378719e-02  2.20540557e-02\\n  4.36212355e-03  4.17974107e-02  8.65492597e-02 -2.52802726e-02\\n  2.17563864e-02 -4.56249639e-02  3.51813156e-03  4.08496149e-02\\n  4.33387142e-03 -4.16760007e-03 -2.32128752e-03  1.61035657e-02\\n  6.80455659e-03 -3.70688215e-02 -3.51658911e-02  2.31892448e-02\\n -1.00556992e-01  4.75949002e-03 -1.10885631e-02  9.66673344e-02\\n -8.10862556e-02 -8.49499181e-02  1.08090147e-01 -2.43041757e-02\\n  1.25411479e-02 -8.14777520e-03  1.09876730e-02 -2.22283695e-02\\n -4.24595922e-02 -1.48430252e-02 -8.21312815e-02  3.78212356e-03\\n -1.14030624e-02  2.91038887e-03  6.03170954e-02  5.84974959e-02\\n -1.69857256e-02 -2.62776259e-02 -8.25655088e-02 -8.94381478e-03\\n  1.47481880e-03  5.61974607e-02  6.80842325e-02  2.25793477e-02\\n -2.92377081e-02  1.44239876e-03  3.35976593e-02 -2.17602365e-02\\n -3.38893966e-03  3.25925760e-02  5.78581356e-02  7.76205817e-03\\n  5.03639057e-02  2.56448928e-02  8.17306712e-02 -3.99536192e-02\\n -5.70321195e-02 -1.53640402e-03 -3.19402218e-02  1.65215749e-02\\n  2.86630038e-02  3.89961638e-02  2.13076137e-02 -3.18679959e-02\\n -4.22535874e-02 -5.32487892e-02 -1.30036464e-02 -3.07918154e-02\\n  6.01712242e-03  5.44616859e-03 -5.02807572e-02 -1.33243892e-02\\n  7.27215484e-02 -4.49486524e-02  4.54448164e-02  3.24615203e-02\\n -2.58123688e-02  3.23441736e-02 -2.90308744e-02  6.67305589e-02\\n -6.64139586e-03 -5.68355992e-02 -2.80844178e-02  1.72451548e-02\\n  2.98296064e-02 -1.00169273e-04  1.32112689e-02  3.26236151e-02\\n -5.64448885e-04 -5.07795671e-03 -5.42719327e-02 -5.17150573e-03\\n  1.77222362e-03  8.03601891e-02  1.40522020e-02 -4.83340165e-03\\n -1.78648476e-02  3.69300917e-02 -1.50580490e-02 -1.18563324e-02\\n -9.61262826e-03  1.38677331e-03  4.98921238e-02  4.27002944e-02\\n -8.20546374e-02  1.27635605e-03 -5.97616211e-02  2.09463499e-02\\n  6.19524010e-02 -6.25282750e-02 -2.04415210e-02  2.81013828e-02\\n  2.62029403e-07  5.72036617e-02  5.09165935e-02  5.28268777e-02\\n  2.54681148e-02  2.60881931e-02  1.05643291e-02 -2.33606286e-02\\n  4.81569469e-02  7.46869203e-03 -7.70809734e-03  3.03233042e-02\\n  2.56115012e-02  4.76096012e-02  2.84005906e-02 -4.78442311e-02\\n -2.50207596e-02 -2.50070840e-02  3.47497724e-02 -2.92207226e-02\\n -5.97768277e-03  3.34014930e-02  9.05050710e-02 -3.64041924e-02\\n -1.98956579e-02  1.70688685e-02 -2.81954277e-02  5.70156164e-02\\n  2.21881103e-02  1.44984042e-02  1.55443484e-02  2.80455519e-02\\n  4.45119515e-02 -5.11322804e-02 -1.37861958e-03 -4.85164439e-03\\n  2.65682302e-03  6.74427226e-02  8.34463239e-02  4.96402523e-03\\n -1.93946145e-03 -1.19117834e-02 -9.58145689e-03 -3.15767787e-02\\n -2.86720004e-02  3.90804447e-02 -6.92959279e-02 -1.21376766e-02\\n  1.33086406e-02 -7.90155551e-04  1.52630880e-02 -6.02091022e-04\\n -2.45148018e-02 -6.31330244e-04  1.28468499e-02  5.25748655e-02\\n -2.40474492e-02 -1.36919497e-02 -1.55772073e-02 -1.07277064e-02\\n -2.99452115e-02 -4.68111895e-02 -6.15504291e-03 -5.20250089e-02\\n  2.17684210e-04  1.68666616e-02 -6.57432666e-03 -2.64519732e-03\\n  1.99408841e-34  3.72692570e-02 -9.97918774e-04  5.70585253e-03\\n  1.42054623e-02  1.36625096e-02 -1.48403188e-02 -2.75286138e-02\\n  2.00862028e-02 -4.09760289e-02 -4.28571515e-02 -5.00870831e-02]'},\n",
       " {'page_number': 8,\n",
       "  'sentence_chunk': 'Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder–decoder architecture for language translation. An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text. If you are unfamiliar with RNNs, don’t worry—you don’t need to know the detailed workings of RNNs to follow this discussion; our focus here is more on the general concept of the encoder–decoder setup. In an encoder–decoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence in the final hidden state. The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction. While we don’t need to know the inner workings of these encoder–decoder RNNs, the key idea here is that the encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in this hidden state to produce the output. You can think of this hidden state as an embedding vector.',\n",
       "  'sentence_chunk_size': 1340,\n",
       "  'sentence_chunk_word_count': 224,\n",
       "  'sentence_chunk_tokens': 335.0,\n",
       "  'embedding': '[ 2.19679438e-02 -3.29127237e-02 -2.12686881e-02  4.54190820e-02\\n -3.06133498e-02  3.30651775e-02 -3.00334161e-03 -6.88261399e-03\\n -8.60312730e-02 -2.48661581e-02  2.10364554e-02 -3.40420343e-02\\n  6.31175041e-02 -2.32518632e-02  3.97391282e-02 -2.31192540e-02\\n  2.77366843e-02 -1.01534314e-02 -3.96025479e-02 -1.89958268e-03\\n -2.68955473e-02 -1.72236878e-02 -1.95630081e-03  3.71586606e-02\\n  2.19266135e-02 -5.14406804e-03 -4.86803353e-02  1.06365569e-02\\n -4.58441526e-02 -2.26253597e-03 -3.87101136e-02 -1.26109188e-02\\n  6.13152906e-02  4.57818396e-02  2.07098378e-06 -2.10337993e-02\\n  1.27238464e-02 -2.43172254e-02  1.33408112e-02 -1.19107822e-02\\n  2.08552610e-02 -5.86413741e-02 -2.74522938e-02  2.49379855e-02\\n -1.00156723e-03 -3.75851505e-02  7.13635013e-02  3.44420597e-02\\n  3.47043499e-02  5.01811840e-02  1.18597725e-03 -1.01284668e-01\\n  8.94307345e-03 -1.30210761e-02  1.82065088e-02 -2.23060250e-02\\n  4.32957970e-02 -5.87224662e-02 -4.55590151e-02  1.17066428e-02\\n -7.48566687e-02  3.40119153e-02  3.08789741e-02 -5.31720184e-03\\n -1.76381860e-02  7.74459913e-03  3.41381058e-02 -5.02826013e-02\\n -1.84752122e-02 -1.46915019e-02 -5.03222197e-02 -2.42199097e-03\\n -8.32283497e-03  2.17729993e-02 -2.88756844e-02  6.35020481e-03\\n -2.30273195e-02 -1.04166819e-02  2.21656337e-02  3.77480797e-02\\n  1.02543943e-02  2.17961725e-02  1.61109772e-02 -4.46163528e-02\\n -5.16105145e-02 -6.79127406e-03  1.03502600e-02 -4.02693003e-02\\n -2.81632319e-02  2.31549051e-02  6.90333322e-02 -4.70852293e-02\\n  6.78728521e-02  5.60460910e-02  6.76762760e-02  3.76944281e-02\\n -5.98681718e-03 -5.30544333e-02 -1.90302208e-02 -7.90029541e-02\\n -2.10544690e-02  4.01102789e-02 -2.05360297e-02 -5.53280406e-04\\n -1.61278099e-02  1.03045352e-01 -5.83210886e-02  2.70732306e-02\\n -6.17211238e-02 -8.77014734e-03 -2.45593563e-02 -4.45558019e-02\\n -5.94271384e-02  1.09127285e-02 -2.46619415e-02 -4.03701700e-02\\n -3.08112558e-02  2.39050575e-02  4.07356098e-02  1.71691589e-02\\n -1.96959060e-02  6.83515007e-03 -5.67337982e-02  6.26362115e-02\\n -5.03681786e-02  6.72202259e-02  1.20424235e-03 -5.68255782e-02\\n  9.76922270e-03  1.09184848e-03 -2.13594735e-02 -4.03762516e-03\\n  2.02589389e-02 -1.66162793e-02 -4.41510417e-02  2.33563073e-02\\n  4.58072452e-03  8.06513987e-03 -1.08167250e-02  1.86847784e-02\\n -6.28329860e-03 -4.78933640e-02  4.85893413e-02 -1.71812642e-02\\n  2.99095083e-02  2.02164687e-02 -4.07895632e-02 -4.23028395e-02\\n  3.28982137e-02  3.96322608e-02 -4.07480896e-02  5.45089617e-02\\n  1.93480160e-02 -4.46444973e-02  2.90351082e-02  1.07743824e-02\\n  6.56794235e-02  1.00177396e-02  3.88763510e-02  2.71738376e-02\\n -3.40497075e-03 -1.50482845e-03 -4.20419453e-03  1.75946131e-02\\n  6.41959766e-03 -2.71581672e-02 -5.41814640e-02 -4.99802716e-02\\n  1.98640060e-02  7.37495571e-02  1.41616380e-02  2.73332037e-02\\n -1.32682044e-02  1.77624933e-02  5.32592684e-02  4.88543101e-02\\n  4.49537002e-02  9.84537303e-02  1.80792809e-02  8.82275868e-03\\n  9.90828499e-03  6.88936859e-02  3.97186587e-03  5.61619736e-02\\n  4.48344648e-03  9.25621949e-03  1.34454910e-02  1.03580179e-02\\n -1.08066443e-02 -3.70977260e-02  2.26962175e-02  7.79408589e-03\\n  4.53027077e-02 -7.33363554e-02  3.82613880e-03  3.80491205e-02\\n -7.73208961e-02  3.30919772e-02 -5.05665354e-02 -1.04635572e-02\\n  4.82304506e-02  6.85814256e-03 -6.49308935e-02  5.52076809e-02\\n  2.87338626e-02  2.67466512e-02 -1.04312925e-02 -2.49642525e-02\\n -6.67855218e-02  4.91681136e-02 -2.24572979e-02  4.58920486e-02\\n -2.15170886e-02 -3.84096690e-02 -4.68460436e-04  4.11999598e-02\\n -8.86244047e-03  4.08691540e-02 -6.40771762e-02  3.34994905e-02\\n -2.09700479e-03  1.39517942e-02 -3.76937836e-02 -6.48598466e-03\\n -4.11564261e-02 -6.64014649e-03 -1.50745024e-03  3.22416350e-02\\n -3.54324766e-02  2.58704592e-02  1.21897636e-02  3.26829776e-02\\n  5.03957272e-02 -4.97308336e-02 -2.96827797e-02  3.74492668e-02\\n -1.10454254e-01 -5.52047342e-02  1.83520429e-02 -2.11927909e-02\\n  4.15255241e-02  1.58068277e-02  2.91981418e-02 -5.32518439e-02\\n  5.90525903e-02 -1.29183521e-02  7.36691579e-02 -1.59955248e-02\\n  9.17023234e-03 -1.46496203e-02  1.53754530e-02  7.16386177e-03\\n  2.65842956e-02 -1.40476860e-02  1.95984240e-03  3.29104140e-02\\n  7.41474610e-03 -5.98495780e-03 -1.05316257e-02 -3.13384868e-02\\n  9.00798365e-02 -5.73379267e-03  4.24649334e-04 -2.24553142e-02\\n -3.71708423e-02  1.40561340e-02 -4.51376550e-02  1.10707954e-02\\n  5.18129673e-03 -8.33841413e-03  1.67568866e-02  3.61278281e-02\\n -8.07927921e-03  4.65817265e-02  5.40647714e-04 -3.26708481e-02\\n  4.04158086e-02 -1.95030719e-02 -1.11155240e-02  6.02552071e-02\\n  4.35983166e-02 -3.20323855e-02 -2.62713805e-02 -1.95061155e-02\\n -1.75675040e-03  8.62160921e-02  2.33203899e-02 -4.21444848e-02\\n -8.46393034e-02 -2.51375604e-02  2.11128425e-02  1.95952877e-02\\n  3.67326615e-03 -1.06600009e-01 -4.06117085e-03  7.22196139e-03\\n  4.24019471e-02  1.55450627e-02  8.94131418e-03  6.84811100e-02\\n -1.97788165e-03 -3.65860015e-02 -2.71629393e-02  6.96977181e-03\\n -6.10758327e-02  3.26947402e-03 -6.58037663e-02  2.13175127e-03\\n -3.72390412e-02  9.15663391e-02  3.50048617e-02  1.40454397e-02\\n -6.60073534e-02  1.22098206e-02 -1.26569057e-02  2.01272793e-04\\n -7.17974547e-03 -6.44891039e-02 -5.00262305e-02  1.89238675e-02\\n -3.03287804e-02  2.60006040e-02  3.62190977e-02 -3.13633643e-02\\n -1.50824524e-02 -5.18408641e-02  3.86582650e-02 -1.45093007e-02\\n  1.05468724e-02  2.14155782e-02  1.27449781e-02 -2.51024752e-03\\n -2.98311282e-02  3.90747115e-02  2.81694401e-02 -1.09844012e-02\\n -1.92784220e-02  8.63992237e-03 -2.04503117e-03 -4.26918976e-02\\n -8.34125734e-04  2.56230473e-03 -1.37336357e-02  3.78702357e-02\\n  2.20983662e-02 -1.62264574e-02  3.52187641e-02 -1.57883801e-02\\n -2.94244643e-02 -2.30150335e-02  3.27704363e-02  4.88377251e-02\\n -3.29376794e-02  1.21351387e-02 -4.82663922e-02  4.86819036e-02\\n -3.58902924e-02 -1.66555382e-02  3.11465710e-02 -2.07959004e-02\\n  4.19672169e-02  1.30662427e-03 -5.68457693e-02 -2.42348127e-02\\n -4.68048155e-02  3.46562034e-03  4.36043404e-02 -4.27709101e-03\\n -2.35936474e-02  3.26891765e-02  1.15612065e-02 -4.99202013e-02\\n -3.49745750e-02  7.11530894e-02 -1.91306006e-02 -3.95630412e-02\\n -3.13175307e-03 -6.19007507e-03  1.51948631e-02  4.39784303e-02\\n  2.93027554e-02 -5.09663522e-02  1.93735654e-03  4.57117939e-03\\n -3.00168674e-02  4.39982004e-02 -1.87548231e-02 -2.81838980e-02\\n -3.79505008e-02  1.10657541e-02  8.36275704e-03 -8.48941281e-02\\n -6.35501519e-02  1.61491595e-02 -5.30077405e-02  5.71034662e-02\\n  1.97739862e-02  1.63698941e-02  2.15794761e-02 -6.78877160e-03\\n  2.80743185e-02 -2.48379633e-02  1.24846995e-02 -4.76810858e-02\\n -4.91393954e-02  1.69246420e-02  9.78292897e-02  4.70603704e-02\\n -4.77744676e-02 -3.99801470e-02 -4.65659751e-03 -4.53144461e-02\\n  3.75302583e-02 -1.21020339e-03  3.53397131e-02  2.00039316e-02\\n -1.01931179e-02  1.78822689e-02 -8.37978628e-03 -3.32652964e-02\\n -3.80311068e-03 -3.40640768e-02  4.02615592e-02 -3.19363251e-02\\n -1.34108560e-02  2.02116426e-02 -3.57941817e-03  4.21899036e-02\\n  1.76337566e-02  3.43828909e-02 -3.18895876e-02 -3.29215713e-02\\n -4.14609462e-02  4.84683588e-02 -6.30089268e-02 -4.86038439e-02\\n -1.60219315e-02  1.77527163e-02 -5.04998118e-02 -6.46047518e-02\\n -1.94088053e-02 -9.14306194e-03 -1.41356494e-02  5.17655648e-02\\n -4.60756198e-03  7.95451775e-02 -3.87862399e-02 -3.92249748e-02\\n -3.69899087e-02  4.19805497e-02  1.22951791e-02  8.66958499e-03\\n -2.98005491e-02  3.65361646e-02 -2.27839444e-02 -3.97591339e-03\\n -7.20385909e-02 -6.51961565e-02 -5.27612446e-03  5.38667180e-02\\n  2.39295000e-03  1.37528889e-02  2.62818113e-02 -7.65051402e-04\\n -3.16308439e-02  4.97778170e-02 -4.55901325e-02 -1.23644965e-02\\n  1.32276746e-03  3.76739278e-02 -1.39677001e-03  1.05728311e-02\\n  1.14687579e-02 -1.05385194e-02 -8.99710134e-03 -2.89620124e-02\\n -1.88419446e-02  2.91818399e-02 -6.24765195e-02  2.67783068e-02\\n -4.33464609e-02  3.93966474e-02  6.72954088e-03  2.77884547e-02\\n  1.74843683e-03  3.23027633e-02 -1.41824903e-02  1.97292101e-02\\n -3.72371413e-02 -1.72574688e-02  5.02652377e-02  3.46568935e-02\\n -3.79805895e-03  8.46530683e-03  5.39922416e-02 -5.86520061e-02\\n  1.83030460e-02  2.34484952e-02  1.72617827e-02  1.82687044e-02\\n -6.84315572e-03  5.57980873e-02 -1.63949525e-03  3.97239700e-02\\n -2.14514770e-02  2.49169692e-02 -5.94301187e-02  2.89579891e-02\\n  3.01101897e-02  5.16091213e-02 -1.24802003e-02  1.19195096e-02\\n -2.18210611e-02  1.47897722e-02  2.86317449e-02 -2.24334355e-02\\n  8.03497136e-02 -4.09591896e-03  4.69316095e-02  5.80760418e-03\\n  1.73677206e-02  3.13322153e-03 -1.29938573e-02 -2.87985764e-02\\n -3.68992006e-03  2.19805958e-03 -7.13852653e-03  1.02928597e-02\\n -6.16914174e-03  4.22916785e-02 -1.11225126e-02  2.88100373e-02\\n -8.05207808e-03 -7.58495554e-03  2.52746679e-02 -3.43438913e-03\\n  4.53084782e-02 -1.28900819e-02 -2.06778497e-02 -2.03387793e-02\\n  1.38971191e-02 -1.70742553e-02  6.31688610e-02  2.42204145e-02\\n  1.90293032e-03 -5.89457788e-02  2.19085868e-02  2.88182013e-02\\n -1.52359223e-02 -7.63207348e-03 -8.26031808e-03  5.67414723e-02\\n  1.06983597e-03 -1.71806868e-02 -3.80313909e-03 -5.88016841e-33\\n -2.35552564e-02 -4.83237654e-02  3.75262871e-02 -1.79276243e-02\\n -2.83705089e-02 -3.41087803e-02 -2.39262972e-02 -5.24435230e-02\\n  2.84365863e-02  8.85820575e-03 -6.82592392e-03 -5.26041444e-03\\n  5.83562255e-03  3.04676257e-02  1.24554029e-02 -5.08022122e-02\\n  4.07095365e-02  2.02134345e-03  2.51368992e-02 -6.94732601e-03\\n  3.37386318e-02  5.19113094e-02  5.33515960e-02 -1.05093978e-02\\n  2.78675128e-02 -4.35149036e-02  1.44911697e-02  2.23781206e-02\\n  1.73860863e-02  9.26213153e-03 -1.45620052e-02  3.24133132e-03\\n -7.35271547e-04 -1.01405166e-01 -3.74256484e-02  7.18369409e-02\\n -7.79782012e-02 -2.83995904e-02  2.06562802e-02  7.48436674e-02\\n -9.56890211e-02 -6.22829609e-02  6.95598796e-02 -9.31237638e-03\\n -2.28600074e-02  3.91920581e-02 -1.41936995e-03 -3.50705050e-02\\n -2.51350980e-02 -4.21836451e-02 -4.71774526e-02 -4.30854671e-02\\n -1.43306777e-02  3.36268917e-03  4.87138964e-02 -6.49598707e-03\\n -3.08009591e-02 -3.66998580e-03 -1.30559847e-01 -5.88055467e-03\\n -2.92847157e-02  8.06509703e-02  4.38992586e-03  4.20592166e-03\\n -1.94377340e-02 -3.32780345e-03  4.18448597e-02 -5.36872000e-02\\n -5.40443510e-03  4.96360995e-02  2.21536066e-02 -1.96727924e-02\\n  5.64771257e-02 -2.31803171e-02  6.73228726e-02 -5.35991378e-02\\n -2.37868614e-02  4.32706525e-04 -4.77026440e-02  2.62610484e-02\\n  5.68959452e-02  1.33974282e-02  3.48273627e-02 -2.24255081e-02\\n -2.48003863e-02 -3.53417471e-02 -1.09061096e-02 -4.25949916e-02\\n  1.79627873e-02  1.50081273e-02 -1.33140180e-02  2.34916378e-02\\n  2.45148763e-02 -4.75552008e-02  3.45476158e-02  3.26925218e-02\\n -1.45736234e-02 -1.51954498e-03 -1.16371112e-02  1.62465721e-02\\n -4.16271621e-03 -9.65885259e-03 -1.04422979e-02  9.78940632e-03\\n  1.60968471e-02  9.62178223e-03  7.48450402e-03  4.47533652e-02\\n -3.69210765e-02  5.95618691e-03 -3.69520895e-02  1.88951958e-02\\n -1.25369085e-02 -1.77061395e-03 -1.02110794e-02  6.18252484e-03\\n  3.64252599e-04  1.65384710e-02  1.99210253e-02  1.14465691e-02\\n -7.09140534e-03  7.71596422e-03  1.89118683e-02  1.57598630e-02\\n -7.49817267e-02  8.50839366e-04 -5.85587844e-02  7.52963647e-02\\n  4.62518334e-02 -5.80755249e-02 -1.10737076e-02  5.20996340e-02\\n  2.79220927e-07  2.62347255e-02  8.59279782e-02  8.68962556e-02\\n -2.53585516e-03  5.70976026e-02 -2.14841170e-03 -1.88377574e-02\\n  2.24267561e-02  4.82038446e-02 -9.21960641e-03  4.89773788e-03\\n  4.10980210e-02 -9.56137292e-03  1.47500178e-02 -8.32538530e-02\\n -2.14061048e-02 -6.42725676e-02  1.75200794e-02 -1.73302274e-02\\n  2.91016214e-02  6.27334937e-02  1.00830436e-01 -3.00078504e-02\\n  1.95825901e-02 -3.34948152e-02 -1.56625770e-02  3.01800426e-02\\n  4.25765179e-02  1.65691618e-02 -3.04560624e-02  2.75093899e-03\\n  2.13725530e-02 -2.71595158e-02 -3.61171663e-02 -1.51785575e-02\\n  1.16277551e-02  8.46483558e-03  5.39305694e-02  1.49620054e-02\\n -3.49544324e-02  3.98170343e-03  2.63061025e-03 -4.01455909e-02\\n -6.49408670e-03  1.83693692e-02 -9.68247578e-02 -3.22860591e-02\\n  1.78940166e-02  5.78870140e-02  3.11159100e-02  6.85881125e-03\\n  2.34148372e-02 -2.58891582e-02 -1.96103733e-02  3.69769968e-02\\n  9.89068672e-03  1.07757961e-02 -4.42217402e-02 -1.19597940e-02\\n -2.47631967e-02 -4.11945283e-02  3.97995859e-02 -7.71280453e-02\\n -1.28385555e-02  2.58996189e-02 -2.34134449e-03 -2.38294024e-02\\n  2.38504704e-34 -2.81681726e-03 -4.20571342e-02  5.91911236e-03\\n  2.93802600e-02 -3.74699500e-03 -1.90985594e-02  3.53594162e-02\\n  2.41994932e-02 -1.55911651e-02 -3.74074802e-02 -4.82829474e-02]'},\n",
       " {'page_number': 8,\n",
       "  'sentence_chunk': 'The big limitation of encoder–decoder RNNs is that the RNN can’t directly access earlier hidden states from the encoder during the decoding phase. Consequently, it relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances. Capturing data dependencies with attention mechanisms Although RNNs work fine for translating short sentences, they don’t work well for longer texts as they don’t have direct access to previous words in the input. One major shortcoming in this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder. Hence, researchers developed the Bahdanau attention mechanism for RNNs in 2014 (named after the first author of the respective paper), which modifies the encoder– decoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step. Interestingly, only three years later, researchers found that RNN architectures are not required for building deep neural networks for natural language processing and proposed the original transformer architecture including a self-attention mechanism inspired by the Bahdanau attention mechanism.',\n",
       "  'sentence_chunk_size': 1310,\n",
       "  'sentence_chunk_word_count': 196,\n",
       "  'sentence_chunk_tokens': 327.5,\n",
       "  'embedding': '[ 1.11495359e-02  6.77088946e-02 -5.77324955e-03  3.58568579e-02\\n -4.25800011e-02 -2.62281410e-02  3.72937392e-03 -9.38656833e-03\\n -4.90930118e-02 -1.32627562e-02 -9.53591894e-03 -5.43013960e-02\\n  4.02595066e-02  2.47322675e-02 -5.65034058e-03 -6.59554005e-02\\n  3.58035378e-02 -1.11702131e-02  8.58833827e-03  1.47194546e-02\\n  6.81589311e-03 -1.77978743e-02 -2.54532439e-03 -3.06922197e-03\\n -1.00090988e-02 -1.45488735e-02 -3.03740148e-02  8.60005151e-03\\n  1.77159421e-02 -3.38757448e-02  1.07034389e-02  2.57124286e-02\\n  2.36443663e-03  1.24724070e-02  2.05908782e-06 -3.00699230e-02\\n  9.81141906e-03 -3.95629890e-02 -1.36489077e-02  2.27343682e-02\\n  1.30610922e-02 -4.37428541e-02 -1.67832635e-02  1.52055454e-02\\n -9.10613034e-03 -2.24418249e-02  2.40668003e-02  7.53166005e-02\\n  5.83522096e-02  6.50235415e-02  1.18100177e-03 -7.07245693e-02\\n  3.10816262e-02 -8.96387622e-02  1.27121722e-02 -5.09483404e-02\\n  1.89558715e-02 -4.87653986e-02 -1.17067341e-02  1.11911064e-02\\n -6.68554977e-02  6.35154322e-02  7.73684531e-02  1.52477538e-02\\n  1.20285386e-02  5.68541959e-02  1.81529447e-02  6.99879834e-03\\n  3.97393218e-04  6.21450618e-02 -4.74254228e-02 -8.89568869e-03\\n -7.85868149e-03 -1.05772074e-02  7.81225180e-03 -7.18970224e-03\\n -3.23108211e-02  6.04703948e-02  1.43059588e-04  2.30019819e-02\\n  1.86889023e-02  4.63189594e-02  2.61070263e-02  5.99579792e-03\\n -4.13261093e-02  9.85962618e-03  2.99489778e-03 -3.12253628e-02\\n -2.29417644e-02 -8.93081538e-03  3.32886167e-02 -4.27698307e-02\\n  5.87791316e-02  5.63446842e-02  6.47060201e-02  1.14741242e-02\\n -3.81805003e-02 -2.51876730e-02 -1.09019792e-02 -4.99899015e-02\\n -3.23634222e-02  4.66891676e-02 -5.01044467e-02  5.85332559e-03\\n -3.43435481e-02  7.58256093e-02 -4.72339429e-02 -1.52685838e-02\\n -1.65569130e-02  1.15044191e-02 -1.19140996e-02  4.08198626e-04\\n -5.38234785e-02  4.06988710e-02 -4.43544164e-02 -3.44509706e-02\\n -3.82965766e-02  2.26133205e-02  5.21775475e-03 -1.61463600e-02\\n  1.01873465e-02  5.19443639e-02 -6.77514076e-02 -3.17152683e-03\\n  4.60733706e-03  4.45107743e-02 -3.72792035e-02 -1.07229576e-02\\n  3.32146627e-03 -4.47937511e-02 -3.56763490e-02  3.66109461e-02\\n  4.34679426e-02 -1.05355782e-02 -2.75253057e-02  3.56278941e-02\\n  2.30529364e-02 -2.64339577e-02 -3.40885855e-02  3.45322378e-02\\n -2.56273728e-02 -4.77288738e-02  3.75775881e-02 -5.25444141e-03\\n  3.90069410e-02 -2.93904059e-02 -1.83897689e-02 -5.49951494e-02\\n  2.87838802e-02  3.60979289e-02  4.22773790e-03  6.44391254e-02\\n  3.30457091e-02 -5.66368513e-02 -1.38437049e-03  1.66299120e-02\\n  5.79387210e-02  9.94482730e-03  3.46897393e-02  5.86587228e-02\\n -1.92183014e-02 -3.34403515e-02  4.02048789e-02 -7.40022911e-03\\n  3.07316128e-02 -3.57539803e-02 -4.43583447e-03 -1.09688677e-02\\n -1.73990037e-02  4.96687628e-02  7.09204227e-02  3.69890817e-02\\n -9.44636203e-03 -1.72536483e-03  4.64348160e-02  1.55850919e-02\\n  4.16122228e-02  9.38340649e-02  2.90141273e-02  1.40002044e-02\\n -1.65922586e-02  6.18287474e-02 -2.66377181e-02  7.77006820e-02\\n  1.80242676e-02  1.91603173e-02  1.82502139e-02  3.06612509e-03\\n -3.56796272e-02 -4.11282293e-02  4.34143394e-02 -1.90731455e-02\\n  1.05393687e-02 -2.55507752e-02  2.98546217e-02  5.30830100e-02\\n -6.45248145e-02  5.93391471e-02  7.48283556e-03 -2.87445877e-02\\n  2.86957920e-02  1.37108769e-02 -3.22971717e-02  6.26094043e-02\\n  1.02347462e-02 -3.58134918e-02 -1.68672763e-03 -1.75685044e-02\\n -3.57037522e-02  2.61321086e-02 -1.05679762e-02  3.50752622e-02\\n -7.16792839e-03 -5.30325994e-02  1.11364266e-02 -1.22639313e-02\\n -7.98782799e-03  5.12964800e-02 -4.79460098e-02  3.07502411e-02\\n  1.69272199e-02  5.57412133e-02 -3.80723272e-03  2.00704839e-02\\n -1.07098687e-02 -1.90572515e-02  2.00134348e-02  3.59487198e-02\\n  2.80061085e-02  7.24732829e-03 -6.90442603e-03  2.79627908e-02\\n  4.53038290e-02 -1.02708191e-01 -9.21062287e-03  7.35705625e-03\\n -1.09595761e-01 -7.09129721e-02 -1.98901538e-02 -3.38833928e-02\\n  1.33643467e-02  5.47769777e-02  4.06326959e-04 -7.43851736e-02\\n  5.97441718e-02 -4.33099791e-02  4.43241633e-02 -3.12351584e-02\\n  7.90426042e-03  3.23812179e-02  8.76724720e-02 -7.67807756e-03\\n  7.14256018e-02 -3.48125249e-02  4.20772098e-03 -1.61416028e-02\\n  3.79860327e-02 -1.02585061e-02 -2.93825567e-02 -1.78858526e-02\\n  2.52067149e-02 -1.07347760e-02 -5.21296449e-03  8.12943652e-03\\n -2.64993832e-02 -2.41995906e-03 -1.39866816e-02 -8.57022963e-03\\n -1.20530818e-02 -3.74446027e-02  3.08007114e-02  4.51035500e-02\\n  4.79896404e-02  1.23088565e-02 -1.80330705e-02 -1.92810912e-02\\n  9.21215117e-03 -4.42501158e-02 -1.65765528e-02  5.10149077e-02\\n -5.14604058e-03 -2.44067684e-02 -4.10742164e-02 -3.39960754e-02\\n  8.13138299e-03  5.39186820e-02  5.58328489e-03 -3.18687409e-02\\n -3.04556843e-02 -1.01651996e-02  3.80484052e-02  2.86662541e-02\\n -4.96015185e-04 -6.56016916e-02 -1.21698864e-02 -1.02994125e-02\\n  4.55432460e-02  6.22188747e-02  4.82614338e-03  1.51412236e-03\\n  3.37750949e-02 -1.17618022e-02 -1.72818713e-02  4.64727879e-02\\n -3.31924595e-02  3.35537940e-02 -8.87159072e-03  7.56505085e-03\\n -4.82361251e-03  4.66443934e-02  3.67173721e-04  3.69472429e-03\\n -4.90329340e-02 -4.88787070e-02  1.23153450e-02 -3.48966122e-02\\n  1.86540261e-02 -1.52831962e-02 -7.16928765e-02  1.65615659e-02\\n -4.86758677e-03  6.09580195e-03  3.03001925e-02 -2.23862678e-02\\n -1.27015440e-02 -5.04890867e-02  3.31660956e-02 -2.51582786e-02\\n  4.40083779e-02  4.21707779e-02  2.47559114e-03  1.33377956e-02\\n -6.72958568e-02  2.00958326e-02  1.81198735e-02 -2.73246542e-02\\n -9.27582849e-03  2.32774597e-02  2.16864725e-03  6.98246248e-03\\n -1.53060695e-02 -3.07034086e-02 -4.30825278e-02 -3.47223436e-03\\n -2.52529792e-02 -4.34571970e-03  2.22525019e-02  1.13202387e-03\\n -2.55752746e-02 -1.41309248e-02  3.10406275e-02  7.39104450e-02\\n -2.61375867e-02  1.91649329e-02 -3.96247581e-02  8.66414309e-02\\n  1.97971016e-02 -2.44245073e-03  1.93726961e-02  1.57020036e-02\\n  1.73014738e-02  4.98573743e-02  7.96437263e-03  4.48593982e-02\\n -4.79867645e-02 -4.99515701e-03  2.57256739e-02 -2.31773257e-02\\n  6.55305618e-03  1.09489001e-02  4.42094402e-03 -9.08413231e-02\\n -4.75361459e-02  8.45436379e-02 -1.48313828e-02 -3.57485004e-02\\n  8.22709221e-03 -4.40552365e-03  2.46793870e-02  1.22691607e-02\\n  5.70947479e-04 -5.03693074e-02 -2.52348017e-02 -1.00409193e-02\\n  1.75237784e-03  2.84149498e-02 -2.77790558e-02  1.01090008e-02\\n -6.86508343e-02 -1.66366138e-02  5.80143277e-03 -6.58906102e-02\\n -5.30330203e-02 -1.29227596e-03 -6.25112932e-03  2.03657728e-02\\n  3.01389191e-02  5.47304749e-03  1.16920657e-02 -1.88278370e-02\\n  1.69366207e-02 -4.85218912e-02 -2.37404127e-02 -3.03203240e-04\\n -5.68686388e-02  4.97123646e-03  6.58663288e-02  1.96235571e-02\\n -2.76048668e-02 -1.46128573e-02  1.04334122e-02 -1.71223804e-02\\n  2.13237889e-02  3.60776037e-02  4.50904369e-02 -4.79905080e-04\\n -2.54951231e-02 -3.18090469e-02  4.52680164e-04 -8.06387365e-02\\n  3.16972961e-04 -4.46291976e-02  4.88637462e-02 -2.43712589e-02\\n -2.07900461e-02 -1.65792424e-02 -1.57573950e-02  3.57384724e-03\\n  4.32011113e-02  4.00952250e-02 -6.46613091e-02 -5.96842021e-02\\n  6.20273000e-04  8.21720995e-03 -1.22084051e-01 -3.90994176e-02\\n  1.57969315e-02  4.32960428e-02 -8.87227431e-02 -8.30651522e-02\\n -7.12757036e-02 -2.46865097e-02  3.24550644e-02 -9.46158427e-04\\n  8.64731800e-03  8.42477083e-02  3.37841397e-04 -9.71451961e-03\\n  2.09671003e-03 -7.56226759e-03 -3.53273451e-02 -2.08298415e-02\\n  5.87638794e-03  4.37960438e-02 -2.17803884e-02  2.28102766e-02\\n  7.67620234e-03 -5.45576774e-02 -5.00964792e-03  5.83682843e-02\\n -2.83684698e-03  4.74280724e-03  5.52519262e-02 -2.00896449e-02\\n -4.16204259e-02  1.65309347e-02 -4.98845503e-02 -3.80469784e-02\\n  2.47621201e-02  2.92741824e-02 -6.45218743e-03 -4.92049707e-03\\n  4.03517485e-02 -7.26452004e-03  1.82831902e-02  1.67723335e-02\\n  2.80605406e-02 -9.68378503e-03 -6.21114625e-03  1.57563910e-02\\n -3.46968472e-02  3.33005004e-02  1.26339495e-02  1.93672795e-02\\n -7.45220995e-03  4.98222299e-02  3.45040811e-03  4.07247618e-02\\n -3.01333982e-02  2.06650365e-02  5.37029579e-02  1.26556596e-02\\n  1.07744299e-02 -1.08283935e-02  4.68733422e-02 -6.60861880e-02\\n  4.65753628e-03  4.51974235e-02  2.37082224e-02  5.12543805e-02\\n -3.66264619e-02  3.92620778e-03 -1.38650369e-02 -5.59724215e-03\\n  7.88273523e-04  3.50633264e-02 -4.58067283e-02  2.28002947e-02\\n  2.66282093e-02  1.56634580e-02 -1.29799619e-02  2.32844148e-03\\n -5.44863679e-02  2.60795727e-02 -7.78160058e-03 -3.56032550e-02\\n  8.30948278e-02 -1.32431341e-02  3.54688726e-02  2.82952189e-02\\n -7.03260629e-03  2.48400229e-05  1.33042941e-02 -1.26461051e-02\\n -1.47932339e-02 -1.52356019e-02  1.12499278e-02 -7.75646046e-03\\n  7.06963427e-03  3.26303728e-02  1.65044388e-03  2.90742982e-02\\n  3.84933352e-02  3.02422363e-02 -3.05127930e-02  1.70098501e-03\\n -2.68837437e-02  8.03811569e-03 -2.55556498e-02  3.71794514e-02\\n -3.93370213e-03 -3.94681282e-02  4.92227152e-02 -2.31438410e-02\\n  9.99453128e-04 -3.09665352e-02  5.88785335e-02  5.82043454e-03\\n  9.78170615e-03 -2.04108991e-02 -1.29855974e-02  7.40928352e-02\\n  1.22513212e-02 -9.98387393e-03 -2.84017958e-02 -6.55067737e-33\\n -1.32880826e-02  3.14149051e-03 -2.60549039e-02 -6.41590729e-02\\n -2.83775777e-02 -7.17309862e-03 -4.38901149e-02 -2.48293914e-02\\n -2.00303122e-02 -2.18792493e-03 -4.83407378e-02  4.77332156e-03\\n  7.99020380e-03  5.01363650e-02  5.86467013e-02 -5.41178584e-02\\n  3.82046402e-02 -5.39158517e-03 -2.66237417e-03  4.87154014e-02\\n  5.29823266e-02  6.62238747e-02  5.33456691e-02 -3.41310464e-02\\n -1.78096574e-02 -4.11411114e-02  1.61605142e-02  6.70700427e-03\\n -8.90495256e-03  1.73939783e-02 -3.90087068e-02  2.62473058e-02\\n  3.60818356e-02 -8.49833861e-02 -4.46008220e-02  5.35291806e-02\\n -1.34788796e-01 -4.43275794e-02 -3.34098078e-02  4.08980437e-02\\n -9.04361829e-02 -3.41784097e-02  9.60991755e-02  1.73647329e-02\\n  4.46053855e-02  5.20249503e-03 -1.70709658e-02 -1.92504022e-02\\n -3.36435921e-02 -4.46915952e-03 -5.12677617e-02 -4.09673713e-03\\n -2.48131640e-02 -4.34361259e-03  4.10508476e-02  1.07883364e-02\\n -2.81844325e-02  2.28786771e-03 -8.65795240e-02 -3.43765244e-02\\n -1.85852647e-02  7.80695230e-02  7.93604273e-03  1.94367357e-02\\n -1.76002420e-02  2.04545955e-04  5.79993539e-02 -3.01200282e-02\\n -4.08080555e-02  1.14716524e-02 -1.14024784e-02  7.65325315e-03\\n  3.20649333e-02 -2.16057841e-02  6.64586648e-02 -4.40529622e-02\\n -6.28891885e-02 -2.60281749e-02 -1.57093741e-02  8.27132910e-02\\n  4.36725691e-02  1.28703350e-02 -4.22947071e-02 -5.98230120e-03\\n  2.24513523e-02  1.06762378e-02 -1.90372877e-02 -6.86621992e-03\\n  1.48608526e-02  2.22414713e-02 -2.00936049e-02  3.10185328e-02\\n  1.32301021e-02 -2.88822763e-02  4.08541597e-02  3.63166146e-02\\n  3.58680449e-02  1.11287460e-02  1.16698919e-02  9.49945115e-03\\n -4.65560034e-02  2.52845325e-02 -2.02074815e-02  6.15356863e-03\\n  9.49032046e-03 -1.39671015e-02  1.39884567e-02  2.56859250e-02\\n -6.16071746e-02 -2.77903825e-02 -3.93377207e-02  2.55107880e-03\\n  7.82590639e-03  3.17590572e-02  3.07980366e-02 -1.07058408e-02\\n -5.18485857e-03  5.87650500e-02 -6.73080655e-03  6.48415135e-03\\n -4.94123809e-02  2.81959586e-02  2.02024747e-02  1.67383235e-02\\n -2.97066756e-02 -1.87112689e-02 -5.12989238e-02  6.11767918e-02\\n  3.36434618e-02 -6.50974363e-02 -1.74225587e-02  5.16502038e-02\\n  2.95117786e-07  3.53158563e-02  1.84149053e-02  5.98369054e-02\\n -5.32438420e-02  3.97105329e-02 -1.62408222e-03 -3.05993110e-02\\n  5.73930107e-02 -3.05372439e-02  2.23674602e-03  3.56143489e-02\\n  1.18662855e-02  8.42485111e-03 -2.33604014e-02 -9.51037109e-02\\n -2.75123697e-02 -4.45882902e-02  4.10267152e-03 -3.25396657e-03\\n -3.67158614e-02  3.35687660e-02  1.13757640e-01 -1.11093465e-02\\n  7.07933819e-03  1.80585645e-02 -5.37893223e-03  1.91164762e-02\\n  4.31531519e-02  2.74721459e-02  1.04668522e-02  1.19764032e-02\\n  1.44727984e-02 -6.23193122e-02 -1.47037376e-02  6.13775104e-03\\n  2.72096023e-02 -8.72900151e-03  3.80658917e-02  5.31912455e-03\\n -9.64617729e-02 -1.69865917e-02  1.31416731e-02 -1.17318230e-02\\n -1.13853393e-02  5.30642904e-02 -7.47642294e-02 -5.10018505e-02\\n  4.26047742e-02  4.58196417e-04  5.51457442e-02 -1.51369218e-02\\n  5.62011451e-02  4.29248810e-03 -1.28335394e-02 -2.24021263e-03\\n -2.82396725e-03 -4.26225588e-02 -9.79042202e-02 -1.19856419e-03\\n -1.07674961e-04 -5.24584353e-02  3.53963338e-02 -8.58948678e-02\\n -2.15667356e-02  6.70429170e-02 -3.80093232e-02 -1.31617906e-02\\n  2.03231769e-34  1.56858247e-02 -1.56746712e-02 -2.33718660e-03\\n  2.64627580e-02  2.04109144e-03 -3.82825844e-02 -1.21365199e-02\\n -4.69368591e-04  2.33491901e-02 -7.59772286e-02 -4.25595231e-02]'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d0b6135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [0.04973646, 0.04890698, -0.009988132, 0.07886...\n",
       "1     [0.05504632, 0.037693933, -0.021266555, 0.0655...\n",
       "2     [0.06285553, -0.012305287, 0.0018492913, 0.041...\n",
       "3     [0.029825618, -0.0035607142, -0.05730359, 0.01...\n",
       "4     [0.07315906, 0.020924203, -0.032348167, 0.0298...\n",
       "5     [0.036764275, -0.015411747, -0.00660755, 0.021...\n",
       "6     [0.020559799, -0.038756255, -0.02460149, 0.018...\n",
       "7     [0.03291103, -0.039424673, -0.020620914, 0.016...\n",
       "8     [0.059157982, -0.0071736774, -0.006206, 0.0478...\n",
       "9     [0.005167837, -0.056243856, -0.005987081, 0.05...\n",
       "10    [0.017086511, -0.057033505, -0.016533313, 0.04...\n",
       "11    [0.026564995, -0.026551738, 0.001147631, 0.009...\n",
       "12    [0.02036939, 0.0055063316, -0.0025264546, 0.02...\n",
       "13    [0.0513227, 0.0010891206, -0.021530451, 0.0456...\n",
       "14    [0.04133433, 0.048474442, -0.003493334, 0.0317...\n",
       "15    [0.06332711, -0.019899074, 0.005012609, 0.0350...\n",
       "16    [0.031515833, 0.024714928, 0.012086132, 0.0295...\n",
       "17    [0.042928156, 0.063535064, -0.020745669, 0.045...\n",
       "18    [0.016351195, -0.015552486, -0.0021876674, 0.0...\n",
       "19    [-0.059602223, 0.0005097713, -0.024471141, 0.0...\n",
       "20    [0.0069432943, -0.020497793, 0.006129227, 0.03...\n",
       "21    [-0.031230005, -0.014442275, -0.017461626, 0.0...\n",
       "22    [0.029565057, -0.051459692, -0.0018620156, 0.0...\n",
       "23    [-0.00093774986, -0.09749521, 0.01438818, 0.02...\n",
       "24    [-0.003280659, -0.06433356, -0.012688206, 0.03...\n",
       "25    [0.01185298, 0.004837523, -0.014590124, 0.0328...\n",
       "26    [0.003911371, 0.077945255, 0.0072073545, 0.085...\n",
       "27    [0.021967944, -0.032912724, -0.021268688, 0.04...\n",
       "28    [0.011149536, 0.067708895, -0.0057732495, 0.03...\n",
       "Name: embedding, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunk_data[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "415b31db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL User\\AppData\\Local\\Temp\\ipykernel_24364\\2561150522.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  embeddings_chunk = torch.tensor(text_chunk_data[\"embedding\"].to_list(), dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "embeddings_chunk = torch.tensor(text_chunk_data[\"embedding\"].to_list(), dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9298df1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_chunk.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f45465",
   "metadata": {},
   "source": [
    "#### Creating queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b88210a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8aafb9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Attention mechanism\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Attention mechanism\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "embed_query = embedding_model.encode(query, convert_to_tensor=True).to(device=device)\n",
    "embed_query.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "116530a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0.00265 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.5039, 0.4482, 0.4347, 0.3842, 0.3732], device='cuda:0'),\n",
       "indices=tensor([24, 25, 10, 28, 27], device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import perf_counter as timer\n",
    "from sentence_transformers import util\n",
    "\n",
    "start_time = timer()\n",
    "dort_prod = util.dot_score(a=embed_query, b=embeddings_chunk)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Total time taken: {end_time-start_time:.5f} seconds\")\n",
    "top_results = torch.topk(dort_prod, k=5)\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ec12dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 0,\n",
       "  'sentence_chunk': 'Building LLMs Understanding Large Langauge Models Large language models (LLMs), such as those offered in OpenAI’s ChatGPT, are deep neural network models that have been developed over the past few years. They ushered in a new era for natural language processing (NLP). Before the advent of LLMs, traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern recognition that could be captured with handcrafted rules or simpler models. However, they typically underperformed in language tasks that demanded complex understanding and generation abilities, such as parsing detailed instructions, conducting contextual analysis, and creating coherent and contextually appropriate original text. For example, previous generations of language models could not write an email from a list of keywords—a task that is trivial for contemporary LLMs. LLMs have remarkable capabilities to understand, generate, and interpret human language. However, it’s important to clarify that when we say language models “understand,” we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension. Enabled by advancements in deep learning, which is a subset of machine learning and artificial intelligence (AI) focused on neural networks, LLMs are trained on vast quantities of text data. This large-scale training allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches. As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more.',\n",
       "  'sentence_chunk_size': 1738,\n",
       "  'sentence_chunk_word_count': 248,\n",
       "  'sentence_chunk_tokens': 434.5},\n",
       " {'page_number': 0,\n",
       "  'sentence_chunk': 'Another important distinction between contemporary LLMs and earlier NLP models is that earlier NLP models were typically designed for specific tasks, such as text categorization, language translation, etc. While those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks. The success behind LLMs can be attributed to the transformer architecture that underpins many LLMs and the vast amounts of data on which LLMs are trained, allowing them to capture a wide variety of linguistic nuances, contexts, and patterns that would be challenging to encode manually. This shift toward implementing models based on the transformer architecture and using large training datasets to train LLMs has fundamentally transformed NLP, providing more capable tools for understanding and interacting with human language. The following discussion sets a foundation to accomplish the primary objective of this book: understanding LLMs by implementing a ChatGPT-like LLM based on the transformer architecture step by step in code. What is an LLM ? An LLM is a neural network designed to understand, generate, and respond to human-like text. These models are deep neural networks trained on massive amounts of text data, sometimes encompassing large portions of the entire publicly available text on the internet. The “large” in “large language model” refers to both the model’s size in terms of parameters and the immense dataset on which it’s trained. Models like this often have tens or even hundreds of billions of parameters, which are the adjustable weights in the network that are optimized during training to predict the next word in a sequence.',\n",
       "  'sentence_chunk_size': 1707,\n",
       "  'sentence_chunk_word_count': 264,\n",
       "  'sentence_chunk_tokens': 426.75},\n",
       " {'page_number': 0,\n",
       "  'sentence_chunk': 'Next-word prediction is sensible because it harnesses the inherent sequential nature of language to train models on understanding context, structure, and relationships within text. Yet, it is a very simple task, and so it is surprising to many researchers that it can produce such capable models. In later chapters, we will discuss and implement the nextword training procedure step by step. LLMs utilize an architecture called the transformer, which allows them to pay selective attention to different parts of the input when making predictions, making them especially adept at handling the nuances and complexities of human language.',\n",
       "  'sentence_chunk_size': 635,\n",
       "  'sentence_chunk_word_count': 97,\n",
       "  'sentence_chunk_tokens': 158.75},\n",
       " {'page_number': 1,\n",
       "  'sentence_chunk': 'Since LLMs are capable of generating text, LLMs are also often referred to as a form of generative artificial intelligence, often abbreviated as generative AI or GenAI. As illustrated in figure 1.1, AI encompasses the broader field of creating machines that can perform tasks requiring humanlike intelligence, including understanding language, recognizing patterns, and making decisions, and includes subfields like machine learning and deep learning. The algorithms used to implement AI are the focus of the field of machine learning. Specifically, machine learning involves the development of algorithms that can learn from and make predictions or decisions based on data without being explicitly programmed. To illustrate this, imagine a spam filter as a practical application of machine learning. Instead of manually writing rules to identify spam emails, a machine learning algorithm is fed examples of emails labeled as spam and legitimate emails. By minimizing the error in its predictions on a training dataset, the model then learns to recognize patterns and characteristics indicative of spam, enabling it to classify new emails as either spam or not spam. deep learning is a subset of machine learning that focuses on utilizing neural networks with three or more layers (also called deep neural networks) to model complex patterns and abstractions in data. In contrast to deep learning, traditional machine learning requires manual feature extraction. This means that human experts need to identify and select the most relevant features for the model.',\n",
       "  'sentence_chunk_size': 1562,\n",
       "  'sentence_chunk_word_count': 237,\n",
       "  'sentence_chunk_tokens': 390.5},\n",
       " {'page_number': 1,\n",
       "  'sentence_chunk': 'While the field of AI is now dominated by machine learning and deep learning, it also includes other approaches—for example, using rule-based systems, genetic algorithms, expert systems, fuzzy logic, or symbolic reasoning. Returning to the spam classification example, in traditional machine learning, human experts might manually extract features from email text such as the frequency of certain trigger words (for example, “prize,” “win,” “free”), the number of exclamation marks, use of all uppercase words, or the presence of suspicious links. This dataset, created based on these expert- defined features, would then be used to train the model. In contrast to traditional machine learning, deep learning does not require manual feature extraction. This means that human experts do not need to identify and select the most relevant features for a deep learning model. ( However, both traditional machine learning and deep learning for spam classification still require the collection of labels, such as spam or non-spam, which need to be gathered either by an expert or users.) Let’s look at some of the problems LLMs can solve today, the challenges that LLMs address, and the general LLM architecture we will implement later. Applications of LLMs Owing to their advanced capabilities to parse and understand unstructured text data, LLMs have a broad range of applications across various domains. Today, LLMs are employed for machine translation, generation of novel texts, sentiment analysis, text summarization, and many other tasks. LLMs have recently been used for content creation, such as writing fiction, articles, and even computer code.',\n",
       "  'sentence_chunk_size': 1649,\n",
       "  'sentence_chunk_word_count': 252,\n",
       "  'sentence_chunk_tokens': 412.25},\n",
       " {'page_number': 1,\n",
       "  'sentence_chunk': 'LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s ChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries and augment traditional search engines such as Google Search or Microsoft Bing. Moreover, LLMs may be used for effective knowledge retrieval from vast volumes of text in specialized areas such as medicine or law. This includes sifting through documents, summarizing lengthy passages, and answering technical questions. In short, LLMs are invaluable for automating almost any task that involves parsing and generating text. Their applications are virtually endless, and as we continue to innovate and explore new ways to use these models, it’s clear that LLMs have the potential to redefine our relationship with technology, making it more conversational, intuitive, and accessible. We will focus on understanding how LLMs work from the ground up, coding an LLM that can generate texts. You will also learn about techniques that allow LLMs to carry out queries, ranging from answering questions',\n",
       "  'sentence_chunk_size': 1058,\n",
       "  'sentence_chunk_word_count': 161,\n",
       "  'sentence_chunk_tokens': 264.5},\n",
       " {'page_number': 2,\n",
       "  'sentence_chunk': 'to summarizing text, translating text into different languages, and more. In other words, you will learn how complex LLM assistants such as ChatGPT work by building one step by step. Stages of building and using LLMs Why should we build our own LLMs? Coding an LLM from the ground up is an excellent exercise to understand its mechanics and limitations. Also, it equips us with the required knowledge for pretraining or fine-tuning existing open source LLM architectures to our own domain-specific datasets or tasks. Research has shown that when it comes to modeling performance, custom-built LLMs—those tailored for specific tasks or domains—can outperform general-purpose LLMs, such as those provided by ChatGPT, which are designed for a wide array of applications. Examples of these include BloombergGPT (specialized for finance) and LLMs tailored for medical question answering. Using custom-built LLMs offers several advantages, particularly regarding data privacy. For instance, companies may prefer not to share sensitive data with third-party LLM providers like OpenAI due to confidentiality concerns. Additionally, developing smaller custom LLMs enables deployment directly on customer devices, such as laptops and smartphones, which is something companies like Apple are currently exploring.',\n",
       "  'sentence_chunk_size': 1301,\n",
       "  'sentence_chunk_word_count': 190,\n",
       "  'sentence_chunk_tokens': 325.25},\n",
       " {'page_number': 2,\n",
       "  'sentence_chunk': 'This local implementation can significantly decrease latency and reduce server-related costs. Furthermore, custom LLMs grant developers complete autonomy, allowing them to control updates and modifications to the model as needed. The general process of creating an LLM includes pretraining and fine-tuning. The “pre” in “pretraining” refers to the initial phase where a model like an LLM is trained on a large, diverse dataset to develop a broad understanding of language. This pretrained model then serves as a foundational resource that can be further refined through fine-tuning, a process where the model is specifically trained on a narrower dataset that is more specific to particular tasks or domains. The first step in creating an LLM is to train it on a large corpus of text data, sometimes referred to as raw text. Here, “raw” refers to the fact that this data is just regular text without any labeling information. ( Filtering may be applied, such as removing formatting characters or documents in unknown languages.) This first training stage of an LLM is also known as pretraining, creating an initial pretrained LLM, often called a base or foundation model. A typical example of such a model is the GPT-3 model (the precursor of the original model offered in ChatGPT).',\n",
       "  'sentence_chunk_size': 1282,\n",
       "  'sentence_chunk_word_count': 207,\n",
       "  'sentence_chunk_tokens': 320.5},\n",
       " {'page_number': 2,\n",
       "  'sentence_chunk': 'This model is capable of text completion—that is, finishing a half-written sentence provided by a user. It also has limited few-shot capabilities, which means it can learn to perform new tasks based on only a few examples instead of needing extensive training data. After obtaining a pretrained LLM from training on large text datasets, where the LLM is trained to predict the next word in the text, we can further train the LLM on labeled data, also known as fine-tuning. The two most popular categories of fine-tuning LLMs are instruction fine-tuning and classification fine-tuning. In instruction fine-tuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text. In classification fine-tuning, the labeled dataset consists of texts and associated class labels—for example, emails associated with “spam” and “not spam” labels. Introduction to Transformers Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper “Attention Is All You Need” (https://arxiv.org/abs/1706. 03762). To understand LLMs, we must understand the original transformer, which was developed for machine translation, translating English texts to German and French. The transformer architecture consists of two submodules: an encoder and a decoder.',\n",
       "  'sentence_chunk_size': 1382,\n",
       "  'sentence_chunk_word_count': 206,\n",
       "  'sentence_chunk_tokens': 345.5},\n",
       " {'page_number': 2,\n",
       "  'sentence_chunk': 'The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input. Then, the decoder module takes these encoded vectors and generates the',\n",
       "  'sentence_chunk_size': 237,\n",
       "  'sentence_chunk_word_count': 37,\n",
       "  'sentence_chunk_tokens': 59.25},\n",
       " {'page_number': 3,\n",
       "  'sentence_chunk': 'output text. In a translation task, for example, the encoder would encode the text from the source language into vectors, and the decoder would decode these vectors to generate text in the target language. Both the encoder and decoder consist of many layers connected by a so-called self- attention mechanism. You may have many questions regarding how the inputs are preprocessed and encoded. These will be addressed in a step-by-step implementation in subsequent chapters. A key component of transformers and LLMs is the selfattention mechanism (not shown), which allows the model to weigh the importance of different words or tokens in a sequence relative to each other. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output. Later variants of the transformer architecture, such as BERT (short for bidirectional encoder representations from transformers) and the various GPT models (short for generative pretrained transformers), built on this concept to adapt this architecture for different tasks. BERT, which is built upon the original transformer’s encoder submodule, differs in its training approach from GPT. While GPT is designed for generative tasks, BERT and its variants specialize in masked word prediction, where the model predicts masked or hidden words in a given sentence.',\n",
       "  'sentence_chunk_size': 1431,\n",
       "  'sentence_chunk_word_count': 215,\n",
       "  'sentence_chunk_tokens': 357.75},\n",
       " {'page_number': 3,\n",
       "  'sentence_chunk': 'This unique training strategy equips BERT with strengths in text classification tasks, including sentiment prediction and document categorization. As an application of its capabilities, as of this writing, X (formerly Twitter) uses BERT to detect toxic content. GPT, on the other hand, focuses on the decoder portion of the original transformer architecture and is designed for tasks that require generating texts. This includes machine translation, text summarization, fiction writing, writing computer code, and more. GPT models, primarily designed and trained to perform text completion tasks, also show remarkable versatility in their capabilities. These models are adept at executing both zeroshot and few-shot learning tasks. Zero-shot learning refers to the ability to generalize to completely unseen tasks without any prior specific examples. On the other hand, fewshot learning involves learning from a minimal number of examples the user provides as input. Transformers vs. LLM Today’s LLMs are based on the transformer architecture. Hence, transformers and LLMs are terms that are often used synonymously in the literature.',\n",
       "  'sentence_chunk_size': 1134,\n",
       "  'sentence_chunk_word_count': 165,\n",
       "  'sentence_chunk_tokens': 283.5},\n",
       " {'page_number': 3,\n",
       "  'sentence_chunk': 'However, note that not all transformers are LLMs since transformers can also be used for computer vision. Also, not all LLMs are transformers, as there are LLMs based on recurrent and convolutional architectures. The main motivation behind these alternative approaches is to improve the computational efficiency of LLMs. Whether these alternative LLM architectures can compete with the capabilities of transformer-based LLMs and whether they are going to be adopted in practice remains to be seen. For simplicity, I use the term “LLM” to refer to transformer-based LLMs similar to GPT. Utilizing large datasets The large training datasets for popular GPT- and BERT-like models represent diverse and comprehensive text corpora encompassing billions of words, which include a vast array of topics and natural and computer languages. To provide a concrete example, table 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base model for the first version of ChatGPT. Dataset Name Dataset Description Number of tokens Proportion in training data Common Crawl    (filtered) Web crawl data 410 billion 60% WebText2 Web crawl data 19 billion 22%',\n",
       "  'sentence_chunk_size': 1162,\n",
       "  'sentence_chunk_word_count': 178,\n",
       "  'sentence_chunk_tokens': 290.5},\n",
       " {'page_number': 4,\n",
       "  'sentence_chunk': 'Dataset Name Dataset Description Number of tokens Proportion in training data Books1 Internet-based book corpus 12 billion 8% Books2 Internet-based book corpus 55 billion 8% Wikipedia High-quality text 3 billion 3% The table above, reports the number of tokens, where a token is a unit of text that a model reads and the number of tokens in a dataset is roughly equivalent to the number of words and punctuation characters in the text. The main takeaway is that the scale and diversity of this training dataset allow these models to perform well on diverse tasks, including language syntax, semantics, and context—even some requiring general knowledge. The pretrained nature of these models makes them incredibly versatile for further fine-tuning on downstream tasks, which is why they are also known as base or foundation models. Pretraining LLMs requires access to significant resources and is very expensive. For example, the GPT-3 pretraining cost is estimated to be 4.6 million USD in terms of cloud computing credits (https://mng.bz/VxEW). The good news is that many pretrained LLMs, available as open source models, can be used as general-purpose tools to write, extract, and edit texts that were not part of the training data. Also, LLMs can be fine-tuned on specific tasks with relatively smaller datasets, reducing the computational resources needed and improving performance. We will implement the code for pretraining and use it to pretrain an LLM for educational purposes. All computations are executable on consumer hardware. After implementing the pretraining code, we will learn how to reuse openly available model weights and load them into the architecture we will implement, allowing us to skip the expensive pretraining stage when we fine-tune our LLM.',\n",
       "  'sentence_chunk_size': 1772,\n",
       "  'sentence_chunk_word_count': 278,\n",
       "  'sentence_chunk_tokens': 443.0},\n",
       " {'page_number': 4,\n",
       "  'sentence_chunk': 'GPT-3 Dataset Details The Table above displays the dataset used for GPT-3. The proportions column in the table sums up to 100% of the sampled data, adjusted for rounding errors. Although the subsets in the Number of Tokens column total 499 billion, the model was trained on only 300 billion tokens. The authors of the GPT-3 paper did not specify why the model was not trained on all 499 billion tokens. For context, consider the size of the CommonCrawl dataset, which alone consists of 410 billion tokens and requires about 570 GB of storage. In comparison, later iterations of models like GPT-3, such as Meta’s LLaMA, have expanded their training scope to include additional data sources like Arxiv research papers (92 GB) and StackExchange’s code-related Q&As (78 GB). The authors of the GPT-3 paper did not share the training dataset, but a comparable dataset that is publicly available is Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research by Soldaini et al. 2024 (https://arxiv.org/abs/2402.00159). However, the collection may contain copyrighted works, and the exact usage terms may depend on the intended use case and country. A closer look at the GPT architecture GPT was originally introduced in the paper “Improving Language Understanding by Generative Pre- Training” (https://mng.bz/x2qg) by Radford et al.',\n",
       "  'sentence_chunk_size': 1342,\n",
       "  'sentence_chunk_word_count': 214,\n",
       "  'sentence_chunk_tokens': 335.5},\n",
       " {'page_number': 4,\n",
       "  'sentence_chunk': 'from OpenAI. GPT-3 is a scaled-up version of this model that has more parameters and was trained on a larger dataset. In addition, the original model offered in ChatGPT was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI’s InstructGPT paper (https://arxiv.org/abs/2203.02155). As figure 1.6 shows, these models are competent text completion models and can carry out other tasks such as spelling correction, classification, or language translation. This is actually very remarkable given that GPT models are pretrained on a relatively simple next-word prediction task',\n",
       "  'sentence_chunk_size': 604,\n",
       "  'sentence_chunk_word_count': 90,\n",
       "  'sentence_chunk_tokens': 151.0},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling. This means that we don’t need to collect labels for the training data explicitly but can use the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict. Since this next-word prediction task allows us to create labels “on the fly,” it is possible to use massive unlabeled text datasets to train LLMs. Compared to the original transformer architecture, the general GPT architecture is relatively simple. Essentially, it’s just the decoder part without the encoder. Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of autoregressive model. Autoregressive models incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves the coherence of the resulting text. Architectures such as GPT-3 are also significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times.',\n",
       "  'sentence_chunk_size': 1197,\n",
       "  'sentence_chunk_word_count': 190,\n",
       "  'sentence_chunk_tokens': 299.25},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'GPT-3 has 96 transformer layers and 175 billion parameters in total. GPT-3 was introduced in 2020, which, by the standards of deep learning and large language model development, is considered a long time ago. However, more recent architectures, such as Meta’s Llama models, are still based on the same underlying concepts, introducing only minor modifications. Hence, understanding GPT remains as relevant as ever, so I focus on implementing the prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs. Although the original transformer model, consisting of encoder and decoder blocks, was explicitly designed for language translation, GPT models—despite their larger yet simpler decoder-only architecture aimed at next-word prediction—are also capable of performing translation tasks. This capability was initially unexpected to researchers, as it emerged from a model primarily trained on a next-word prediction task, which is a task that did not specifically target translation. The ability to perform tasks that the model wasn’t explicitly trained to perform is called an emergent behavior. This capability isn’t explicitly taught during training but emerges as a natural consequence of the model’s exposure to vast quantities of multilingual data in diverse contexts. The fact that GPT models can “learn” the translation patterns between languages and perform translation tasks even though they weren’t specifically trained for it demonstrates the benefits and capabilities of these large-scale, generative language models. We can perform diverse tasks without using diverse models for each.',\n",
       "  'sentence_chunk_size': 1647,\n",
       "  'sentence_chunk_word_count': 238,\n",
       "  'sentence_chunk_tokens': 411.75},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'Working with text data During the pretraining stage, LLMs process text one word at a time. Training LLMs with millions to billions of parameters using a next-word prediction task yields models with impressive capabilities. These models can then be further finetuned to follow general instructions or perform specific target tasks. But before we can implement and train LLMs, we need to prepare the training dataset. This involves splitting text into individual word and subword tokens, which can then be encoded into vector representations for the LLM. You’ll also learn about advanced tokenization schemes like byte pair encoding, which is utilized in popular LLMs like GPT. Lastly, we’ll implement a sampling and data-loading strategy to produce the input-output pairs necessary for training LLMs. Understanding word embeddings Deep neural network models, including LLMs, cannot process raw text directly. Since text is categorical, it isn’t compatible with the mathematical operations used to implement and train neural networks. Therefore, we need a way to represent words as continuous-valued vectors.',\n",
       "  'sentence_chunk_size': 1106,\n",
       "  'sentence_chunk_word_count': 165,\n",
       "  'sentence_chunk_tokens': 276.5},\n",
       " {'page_number': 5,\n",
       "  'sentence_chunk': 'The concept of converting data into a vector format is often referred to as embedding. Using a specific neural network layer or another pretrained neural network model, we can embed different data types—for example, video, audio, and text. However, it’s important to note that different data formats require distinct embedding models. For example, an embedding model designed for text would not be',\n",
       "  'sentence_chunk_size': 397,\n",
       "  'sentence_chunk_word_count': 62,\n",
       "  'sentence_chunk_tokens': 99.25},\n",
       " {'page_number': 6,\n",
       "  'sentence_chunk': 'suitable for embedding audio or video data. At its core, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space— the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process. While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for retrieval-augmented generation. Retrieval-augmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text. Since our goal is to train GPT- like LLMs, which learn to generate text one word at a time, we will focus on word embeddings. Several algorithms and frameworks have been developed to generate word embeddings. One of the earlier and most popular examples is the Word2Vec approach. Word2Vec trained neural network architecture to generate word embeddings by predicting the context of a word given the target word or vice versa. The main idea behind Word2Vec is that words that appear in similar contexts tend to have similar meanings.',\n",
       "  'sentence_chunk_size': 1250,\n",
       "  'sentence_chunk_word_count': 193,\n",
       "  'sentence_chunk_tokens': 312.5},\n",
       " {'page_number': 6,\n",
       "  'sentence_chunk': 'Consequently, when projected into twodimensional word embeddings for visualization purposes, similar terms are clustered together. Word embeddings can have varying dimensions, from one to thousands. A higher dimensionality might capture more nuanced relationships but at the cost of computational efficiency. While we can use pretrained models such as Word2Vec to generate embeddings for machine learning models, LLMs commonly produce their own embeddings that are part of the input layer and are updated during training. The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand. Unfortunately, high-dimensional embeddings present a challenge for visualization because our sensory perception and common graphical representations are inherently limited to three dimensions or fewer, which is why figure 2.3 shows two-dimensional embeddings in a two-dimensional scatterplot. However, when working with LLMs, we typically use embeddings with a much higher dimensionality. For both GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality of the model’s hidden states) varies based on the specific model variant and size. It is a tradeoff between performance and efficiency. The smallest GPT-2 models (117M and 125M parameters) use an embedding size of 768 dimensions to provide concrete examples.',\n",
       "  'sentence_chunk_size': 1428,\n",
       "  'sentence_chunk_word_count': 209,\n",
       "  'sentence_chunk_tokens': 357.0},\n",
       " {'page_number': 6,\n",
       "  'sentence_chunk': 'The largest GPT-3 model (175B parameters) uses an embedding size of 12,288 dimensions. Tokenizing text Tokenization is a fundamental step in Natural Language Processing (NLP). It involves dividing a Textual input into smaller units known as tokens. These tokens can be in the form of words, characters, sub-words, or sentences. It helps in improving interpretability of text by different models. Let’s understand How Tokenization Works. Depending on the LLM, some researchers also consider additional special tokens such as the following: • [BOS] (beginning of sequence) —This token marks the start of a text. It signifies to the LLM where a piece of content begins. • [ EOS] (end of sequence) —This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one ends and the next begins. • [',\n",
       "  'sentence_chunk_size': 974,\n",
       "  'sentence_chunk_word_count': 155,\n",
       "  'sentence_chunk_tokens': 243.5},\n",
       " {'page_number': 6,\n",
       "  'sentence_chunk': 'PAD] (padding) —When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or “padded” using the [PAD] token, up to the length of the longest text in the batch. The tokenizer used for GPT models does not need any of these tokens; it only uses an <|endoftext|> token for simplicity. <|endoftext|> is analogous to the [EOS] token. <|endoftext|> is also used for padding. When training on batched inputs, we typically use a mask, meaning we don’t attend to',\n",
       "  'sentence_chunk_size': 570,\n",
       "  'sentence_chunk_word_count': 98,\n",
       "  'sentence_chunk_tokens': 142.5},\n",
       " {'page_number': 7,\n",
       "  'sentence_chunk': 'padded tokens. Thus, the specific token chosen for padding becomes inconsequential. Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token for out-of-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks words down into subword units. Attention in LLMs In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by “soft” weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size. Unlike “hard” weights, which are computed during the backwards training pass, “soft” weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme. Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks.',\n",
       "  'sentence_chunk_size': 1361,\n",
       "  'sentence_chunk_word_count': 205,\n",
       "  'sentence_chunk_tokens': 340.25},\n",
       " {'page_number': 7,\n",
       "  'sentence_chunk': 'Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state. Transformers In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets. The modern version of the transformer was proposed in the 2017 paper “Attention Is All You Need” by researchers at Google. Transformers were first developed as an improvement over previous architectures for machine translation,but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,audio,multimodal learning, robotics,[9] and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers). Attention Mechanisms Before we dive into the self-attention mechanism at the heart of LLMs, let’s consider the problem with pre-LLM architectures that do not include attention mechanisms.',\n",
       "  'sentence_chunk_size': 1926,\n",
       "  'sentence_chunk_word_count': 277,\n",
       "  'sentence_chunk_tokens': 481.5},\n",
       " {'page_number': 7,\n",
       "  'sentence_chunk': 'Suppose we want to develop a language translation model that translates text from one language into another. We can’t simply translate a text word by word due to the grammatical structures in the source and target language. To address this problem, it is common to use a deep neural network with two submodules, an encoder and a decoder. The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text.',\n",
       "  'sentence_chunk_size': 460,\n",
       "  'sentence_chunk_word_count': 81,\n",
       "  'sentence_chunk_tokens': 115.0},\n",
       " {'page_number': 8,\n",
       "  'sentence_chunk': 'Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder–decoder architecture for language translation. An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text. If you are unfamiliar with RNNs, don’t worry—you don’t need to know the detailed workings of RNNs to follow this discussion; our focus here is more on the general concept of the encoder–decoder setup. In an encoder–decoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence in the final hidden state. The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction. While we don’t need to know the inner workings of these encoder–decoder RNNs, the key idea here is that the encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in this hidden state to produce the output. You can think of this hidden state as an embedding vector.',\n",
       "  'sentence_chunk_size': 1340,\n",
       "  'sentence_chunk_word_count': 224,\n",
       "  'sentence_chunk_tokens': 335.0},\n",
       " {'page_number': 8,\n",
       "  'sentence_chunk': 'The big limitation of encoder–decoder RNNs is that the RNN can’t directly access earlier hidden states from the encoder during the decoding phase. Consequently, it relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances. Capturing data dependencies with attention mechanisms Although RNNs work fine for translating short sentences, they don’t work well for longer texts as they don’t have direct access to previous words in the input. One major shortcoming in this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder. Hence, researchers developed the Bahdanau attention mechanism for RNNs in 2014 (named after the first author of the respective paper), which modifies the encoder– decoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step. Interestingly, only three years later, researchers found that RNN architectures are not required for building deep neural networks for natural language processing and proposed the original transformer architecture including a self-attention mechanism inspired by the Bahdanau attention mechanism.',\n",
       "  'sentence_chunk_size': 1310,\n",
       "  'sentence_chunk_word_count': 196,\n",
       "  'sentence_chunk_tokens': 327.5}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d46f3",
   "metadata": {},
   "source": [
    "#### Pretty printer for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e899aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def pretty_printer(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67d48513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Attention mechanism\n",
      "Results:\n",
      "\n",
      "Score: 0.5038601756095886\n",
      "\n",
      "Text: \n",
      "\n",
      "padded tokens. Thus, the specific token chosen for padding becomes\n",
      "inconsequential. Moreover, the tokenizer used for GPT models also doesn’t use an\n",
      "<|unk|> token for out-of-vocabulary words. Instead, GPT models use a byte pair\n",
      "encoding tokenizer, which breaks words down into subword units. Attention in\n",
      "LLMs In machine learning, attention is a method that determines the importance\n",
      "of each component in a sequence relative to the other components in that\n",
      "sequence. In natural language processing, importance is represented by “soft”\n",
      "weights assigned to each word in a sentence. More generally, attention encodes\n",
      "vectors called token embeddings across a fixed-width sequence that can range\n",
      "from tens to millions of tokens in size. Unlike “hard” weights, which are\n",
      "computed during the backwards training pass, “soft” weights exist only in the\n",
      "forward pass and therefore change with every step of the input. Earlier designs\n",
      "implemented the attention mechanism in a serial recurrent neural network (RNN)\n",
      "language translation system, but a more recent design, namely the transformer,\n",
      "removed the slower sequential RNN and relied more heavily on the faster parallel\n",
      "attention scheme. Inspired by ideas about attention in humans, the attention\n",
      "mechanism was developed to address the weaknesses of using information from the\n",
      "hidden layers of recurrent neural networks.\n",
      "Page Number: 7\n",
      "\n",
      "\n",
      "Score: 0.44822824001312256\n",
      "\n",
      "Text: \n",
      "\n",
      "Recurrent neural networks favor more recent information contained in words at\n",
      "the end of a sentence, while information earlier in the sentence tends to be\n",
      "attenuated. Attention allows a token equal access to any part of a sentence\n",
      "directly, rather than only through the previous state. Transformers In deep\n",
      "learning, transformer is an architecture based on the multi-head attention\n",
      "mechanism, in which text is converted to numerical representations called\n",
      "tokens, and each token is converted into a vector via lookup from a word\n",
      "embedding table. At each layer, each token is then contextualized within the\n",
      "scope of the context window with other (unmasked) tokens via a parallel multi-\n",
      "head attention mechanism, allowing the signal for key tokens to be amplified and\n",
      "less important tokens to be diminished. Transformers have the advantage of\n",
      "having no recurrent units, therefore requiring less training time than earlier\n",
      "recurrent neural architectures (RNNs) such as long short-term memory (LSTM).\n",
      "Later variations have been widely adopted for training large language models\n",
      "(LLMs) on large (language) datasets. The modern version of the transformer was\n",
      "proposed in the 2017 paper “Attention Is All You Need” by researchers at Google.\n",
      "Transformers were first developed as an improvement over previous architectures\n",
      "for machine translation,but have found many applications since. They are used in\n",
      "large-scale natural language processing, computer vision (vision transformers),\n",
      "reinforcement learning,audio,multimodal learning, robotics,[9] and even playing\n",
      "chess. It has also led to the development of pre-trained systems, such as\n",
      "generative pre-trained transformers (GPTs) and BERT (bidirectional encoder\n",
      "representations from transformers). Attention Mechanisms Before we dive into the\n",
      "self-attention mechanism at the heart of LLMs, let’s consider the problem with\n",
      "pre-LLM architectures that do not include attention mechanisms.\n",
      "Page Number: 7\n",
      "\n",
      "\n",
      "Score: 0.43473654985427856\n",
      "\n",
      "Text: \n",
      "\n",
      "output text. In a translation task, for example, the encoder would encode the\n",
      "text from the source language into vectors, and the decoder would decode these\n",
      "vectors to generate text in the target language. Both the encoder and decoder\n",
      "consist of many layers connected by a so-called self- attention mechanism. You\n",
      "may have many questions regarding how the inputs are preprocessed and encoded.\n",
      "These will be addressed in a step-by-step implementation in subsequent chapters.\n",
      "A key component of transformers and LLMs is the selfattention mechanism (not\n",
      "shown), which allows the model to weigh the importance of different words or\n",
      "tokens in a sequence relative to each other. This mechanism enables the model to\n",
      "capture long-range dependencies and contextual relationships within the input\n",
      "data, enhancing its ability to generate coherent and contextually relevant\n",
      "output. Later variants of the transformer architecture, such as BERT (short for\n",
      "bidirectional encoder representations from transformers) and the various GPT\n",
      "models (short for generative pretrained transformers), built on this concept to\n",
      "adapt this architecture for different tasks. BERT, which is built upon the\n",
      "original transformer’s encoder submodule, differs in its training approach from\n",
      "GPT. While GPT is designed for generative tasks, BERT and its variants\n",
      "specialize in masked word prediction, where the model predicts masked or hidden\n",
      "words in a given sentence.\n",
      "Page Number: 3\n",
      "\n",
      "\n",
      "Score: 0.3841928541660309\n",
      "\n",
      "Text: \n",
      "\n",
      "The big limitation of encoder–decoder RNNs is that the RNN can’t directly access\n",
      "earlier hidden states from the encoder during the decoding phase. Consequently,\n",
      "it relies solely on the current hidden state, which encapsulates all relevant\n",
      "information. This can lead to a loss of context, especially in complex sentences\n",
      "where dependencies might span long distances. Capturing data dependencies with\n",
      "attention mechanisms Although RNNs work fine for translating short sentences,\n",
      "they don’t work well for longer texts as they don’t have direct access to\n",
      "previous words in the input. One major shortcoming in this approach is that the\n",
      "RNN must remember the entire encoded input in a single hidden state before\n",
      "passing it to the decoder. Hence, researchers developed the Bahdanau attention\n",
      "mechanism for RNNs in 2014 (named after the first author of the respective\n",
      "paper), which modifies the encoder– decoder RNN such that the decoder can\n",
      "selectively access different parts of the input sequence at each decoding step.\n",
      "Interestingly, only three years later, researchers found that RNN architectures\n",
      "are not required for building deep neural networks for natural language\n",
      "processing and proposed the original transformer architecture including a self-\n",
      "attention mechanism inspired by the Bahdanau attention mechanism.\n",
      "Page Number: 8\n",
      "\n",
      "\n",
      "Score: 0.37318670749664307\n",
      "\n",
      "Text: \n",
      "\n",
      "Before the advent of transformers, recurrent neural networks (RNNs) were the\n",
      "most popular encoder–decoder architecture for language translation. An RNN is a\n",
      "type of neural network where outputs from previous steps are fed as inputs to\n",
      "the current step, making them well-suited for sequential data like text. If you\n",
      "are unfamiliar with RNNs, don’t worry—you don’t need to know the detailed\n",
      "workings of RNNs to follow this discussion; our focus here is more on the\n",
      "general concept of the encoder–decoder setup. In an encoder–decoder RNN, the\n",
      "input text is fed into the encoder, which processes it sequentially. The encoder\n",
      "updates its hidden state (the internal values at the hidden layers) at each\n",
      "step, trying to capture the entire meaning of the input sentence in the final\n",
      "hidden state. The decoder then takes this final hidden state to start generating\n",
      "the translated sentence, one word at a time. It also updates its hidden state at\n",
      "each step, which is supposed to carry the context necessary for the next-word\n",
      "prediction. While we don’t need to know the inner workings of these\n",
      "encoder–decoder RNNs, the key idea here is that the encoder part processes the\n",
      "entire input text into a hidden state (memory cell). The decoder then takes in\n",
      "this hidden state to produce the output. You can think of this hidden state as\n",
      "an embedding vector.\n",
      "Page Number: 8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\")\n",
    "print(\"Results:\\n\")\n",
    "for score, idx in zip(top_results[0],top_results[1]):\n",
    "    print(f\"Score: {score}\\n\")\n",
    "    print(\"Text: \\n\")\n",
    "    text_def = page_chunk[idx][\"sentence_chunk\"]\n",
    "    pretty_printer(text_def)\n",
    "    print(f\"Page Number: {page_chunk[idx][\"page_number\"]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bcb0b",
   "metadata": {},
   "source": [
    "#### Functioning our semantic pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a234901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main goal is to just integrate everything into one singular pipeline\n",
    "def get_semantic_result(query: str, embeddings: torch.Tensor, EMmodel: SentenceTransformer=embedding_model, return_resources: int = 1, time_bool: bool=True)-> tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    if not query.strip():\n",
    "        print(\"[WARN] Empty query given, returning no results.\")\n",
    "        return None, None\n",
    "    \n",
    "    query_embeddings = EMmodel.encode(query, convert_to_tensor=True).to(device=device)\n",
    "\n",
    "    start_time = timer()\n",
    "    dort_prod = util.dot_score(query_embeddings, embeddings_chunk)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if time_bool:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} = {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dort_prod, k=return_resources)\n",
    "\n",
    "    return scores, indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea3fe3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for printing top result\n",
    "def pretty_printer_top(query: str, embeddings: torch.Tensor, page_chunk_dict: list[dict]=page_chunk):\n",
    "    scores, indices = get_semantic_result(query,  embeddings=embed_query, return_resources= 5)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"Results:\\n\")\n",
    "    for score, idx in zip(scores, indices):\n",
    "        print(f\"Score: {score:0.4f}\\n\")\n",
    "        print(\"Text: \\n\")\n",
    "        text_def = page_chunk[idx][\"sentence_chunk\"]\n",
    "        pretty_printer(text_def)\n",
    "        print(f\"Page Number: {page_chunk[idx][\"page_number\"]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a53ff5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 768 = 0.00018 seconds.\n",
      "\n",
      "\n",
      "Query: Formula 1\n",
      "Results:\n",
      "\n",
      "Score: 0.1147\n",
      "\n",
      "Text: \n",
      "\n",
      "This local implementation can significantly decrease latency and reduce server-\n",
      "related costs. Furthermore, custom LLMs grant developers complete autonomy,\n",
      "allowing them to control updates and modifications to the model as needed. The\n",
      "general process of creating an LLM includes pretraining and fine-tuning. The\n",
      "“pre” in “pretraining” refers to the initial phase where a model like an LLM is\n",
      "trained on a large, diverse dataset to develop a broad understanding of\n",
      "language. This pretrained model then serves as a foundational resource that can\n",
      "be further refined through fine-tuning, a process where the model is\n",
      "specifically trained on a narrower dataset that is more specific to particular\n",
      "tasks or domains. The first step in creating an LLM is to train it on a large\n",
      "corpus of text data, sometimes referred to as raw text. Here, “raw” refers to\n",
      "the fact that this data is just regular text without any labeling information. (\n",
      "Filtering may be applied, such as removing formatting characters or documents in\n",
      "unknown languages.) This first training stage of an LLM is also known as\n",
      "pretraining, creating an initial pretrained LLM, often called a base or\n",
      "foundation model. A typical example of such a model is the GPT-3 model (the\n",
      "precursor of the original model offered in ChatGPT).\n",
      "Page Number: 2\n",
      "\n",
      "\n",
      "Score: 0.1131\n",
      "\n",
      "Text: \n",
      "\n",
      "The encoder module processes the input text and encodes it into a series of\n",
      "numerical representations or vectors that capture the contextual information of\n",
      "the input. Then, the decoder module takes these encoded vectors and generates\n",
      "the\n",
      "Page Number: 2\n",
      "\n",
      "\n",
      "Score: 0.1071\n",
      "\n",
      "Text: \n",
      "\n",
      "Another important distinction between contemporary LLMs and earlier NLP models\n",
      "is that earlier NLP models were typically designed for specific tasks, such as\n",
      "text categorization, language translation, etc. While those earlier NLP models\n",
      "excelled in their narrow applications, LLMs demonstrate a broader proficiency\n",
      "across a wide range of NLP tasks. The success behind LLMs can be attributed to\n",
      "the transformer architecture that underpins many LLMs and the vast amounts of\n",
      "data on which LLMs are trained, allowing them to capture a wide variety of\n",
      "linguistic nuances, contexts, and patterns that would be challenging to encode\n",
      "manually. This shift toward implementing models based on the transformer\n",
      "architecture and using large training datasets to train LLMs has fundamentally\n",
      "transformed NLP, providing more capable tools for understanding and interacting\n",
      "with human language. The following discussion sets a foundation to accomplish\n",
      "the primary objective of this book: understanding LLMs by implementing a\n",
      "ChatGPT-like LLM based on the transformer architecture step by step in code.\n",
      "What is an LLM ? An LLM is a neural network designed to understand, generate,\n",
      "and respond to human-like text. These models are deep neural networks trained on\n",
      "massive amounts of text data, sometimes encompassing large portions of the\n",
      "entire publicly available text on the internet. The “large” in “large language\n",
      "model” refers to both the model’s size in terms of parameters and the immense\n",
      "dataset on which it’s trained. Models like this often have tens or even hundreds\n",
      "of billions of parameters, which are the adjustable weights in the network that\n",
      "are optimized during training to predict the next word in a sequence.\n",
      "Page Number: 0\n",
      "\n",
      "\n",
      "Score: 0.1043\n",
      "\n",
      "Text: \n",
      "\n",
      "This model is capable of text completion—that is, finishing a half-written\n",
      "sentence provided by a user. It also has limited few-shot capabilities, which\n",
      "means it can learn to perform new tasks based on only a few examples instead of\n",
      "needing extensive training data. After obtaining a pretrained LLM from training\n",
      "on large text datasets, where the LLM is trained to predict the next word in the\n",
      "text, we can further train the LLM on labeled data, also known as fine-tuning.\n",
      "The two most popular categories of fine-tuning LLMs are instruction fine-tuning\n",
      "and classification fine-tuning. In instruction fine-tuning, the labeled dataset\n",
      "consists of instruction and answer pairs, such as a query to translate a text\n",
      "accompanied by the correctly translated text. In classification fine-tuning, the\n",
      "labeled dataset consists of texts and associated class labels—for example,\n",
      "emails associated with “spam” and “not spam” labels. Introduction to\n",
      "Transformers Most modern LLMs rely on the transformer architecture, which is a\n",
      "deep neural network architecture introduced in the 2017 paper “Attention Is All\n",
      "You Need” (https://arxiv.org/abs/1706. 03762). To understand LLMs, we must\n",
      "understand the original transformer, which was developed for machine\n",
      "translation, translating English texts to German and French. The transformer\n",
      "architecture consists of two submodules: an encoder and a decoder.\n",
      "Page Number: 2\n",
      "\n",
      "\n",
      "Score: 0.0996\n",
      "\n",
      "Text: \n",
      "\n",
      "Suppose we want to develop a language translation model that translates text\n",
      "from one language into another. We can’t simply translate a text word by word\n",
      "due to the grammatical structures in the source and target language. To address\n",
      "this problem, it is common to use a deep neural network with two submodules, an\n",
      "encoder and a decoder. The job of the encoder is to first read in and process\n",
      "the entire text, and the decoder then produces the translated text.\n",
      "Page Number: 7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pretty_printer_top(\"Formula 1\", embeddings=embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "942a9588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU Memory: 4 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes/(2**30))\n",
    "\n",
    "print(f\"Available GPU Memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a366da",
   "metadata": {},
   "source": [
    "#### Loading an LLM locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebd0dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"  # must be set before importing torch\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "572a43e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    }
   ],
   "source": [
    "attn_implementation = \"sdpa\"  \n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# quant_Config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "print(f\"[INFO] Using model: {model_name}\")\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4a492",
   "metadata": {},
   "source": [
    "##### Loading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bfca962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1582: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name, \n",
    "                                           torch_dtype=torch.bfloat16, \n",
    "                                           device_map=\"auto\" \n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdcaa2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "097a4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_model_param(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "def get_model_memory(model: torch.nn.Module):\n",
    "   mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])\n",
    "   mem_buffers = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "   total_bytes = mem_buffers+mem_params\n",
    "   total_bytes_MB = total_bytes/(1024**2)\n",
    "\n",
    "   return total_bytes_MB/(1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2db4ed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1100048384\n",
      "Total memory size: 2.049 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Parameters: {get_model_param(llm)}\")\n",
    "print(f\"Total memory size: {get_model_memory(llm):0.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b9cbe1",
   "metadata": {},
   "source": [
    "#### Generating text from LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d815e3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Generate a haiku about programming\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Generate a haiku about programming\"\n",
    "print(f\"Input text: {input_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2612a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": input_text,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5857e839",
   "metadata": {},
   "source": [
    "#### Using LLMs for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "312bd813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  529, 29989,  1792, 29989, 29958,    13,  5631,   403,   263,   447,\n",
      "         18282,  1048,  8720,     2, 29871,    13, 29966, 29989,   465, 22137,\n",
      "         29989, 29958,    13,   797, 18925, 24496, 29892,    13, 29909,   775,\n",
      "           304,   443,   908, 29892,    13, 29909,  4086,   304,  4653, 29892,\n",
      "            13,  9283,  4056, 29915, 29879,  1095,  2222,  4972, 29889,     2]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokeniser.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ")\n",
    "                                                \n",
    "\n",
    "outputs = llm.generate(**inputs, max_new_tokens=50).to(device)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e2cd946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = tokeniser.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c94bef26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      ": <|user|>\n",
      "Generate a haiku about programming \n",
      "<|assistant|>\n",
      "Infinite possibilities,\n",
      "A code to unlock,\n",
      "A language to express,\n",
      "Programming's endless flow.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Text\\n: {output_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
